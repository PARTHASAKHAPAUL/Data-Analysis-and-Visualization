{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**NAME: PARTHA SAKHA PAUL**\n",
        "\n",
        "#**ROLL: MA23M016**\n",
        "\n",
        "**PROGRAMMING ASSIGNMENT 4**"
      ],
      "metadata": {
        "id": "TPexkuHNCd6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Building a Neural Network from Scratch. In this exercise we build the digit recognizer neural network from scratch, using numpy, pandas and pyplot**"
      ],
      "metadata": {
        "id": "o-PCs2npmNn8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing necessary libraries**"
      ],
      "metadata": {
        "id": "IMtRUHFOpsOn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9ozQ7M2vRoF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# importing dataset\n",
        "from keras.datasets import mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Step 1:**\n",
        "\n",
        "    In google colab you can get the MNIST files from the keras datasets and you have to make some appropriate changes to the code to load the data from there. In any case, make sure that the gray scale values for the images are scaled in the interval [0, 1]."
      ],
      "metadata": {
        "id": "krXYO5IwoNam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spliting data for validation and training"
      ],
      "metadata": {
        "id": "ERva1zdbq_qn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owlMbf7vJB3W"
      },
      "outputs": [],
      "source": [
        "# loading the data\n",
        "(x_train_mnist, y_train_mnist), (x_test_mnist, y_test_mnist) = mnist.load_data()\n",
        "\n",
        "# spliting data for validation\n",
        "x_val_mnist = x_train_mnist[54000:]\n",
        "y_val_mnist = y_train_mnist[54000:]\n",
        "\n",
        "# spliting data for training\n",
        "x_train_mnist = x_train_mnist[:54000]\n",
        "y_train_mnist = y_train_mnist[:54000]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting the dataset"
      ],
      "metadata": {
        "id": "g0NWnwgurGSn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "7XIM6CXOJB3W",
        "outputId": "3a7b3259-3af6-4230-9d55-42bc70cfcee4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# labels\n",
        "class_names = ['0', '1', '2', '3', '4',\n",
        "               '5', '6', '7', '8', '9']\n",
        "image = []\n",
        "label = []\n",
        "for i in range(54000):\n",
        "  if len(label) >= 10:\n",
        "    break;\n",
        "  if class_names[y_train_mnist[i]] not in label:\n",
        "      image.append(x_train_mnist[i])\n",
        "      label.append(class_names[y_train_mnist[i]])\n",
        "\n",
        "#the dimensions of the plot grid\n",
        "plt.figure(figsize=(10,10))\n",
        "\n",
        "for i in range(len(image)):\n",
        "  plt.subplot(2, 5, i+1)  #plotting in a 2x5 grid\n",
        "  plt.xticks([])  #remove x-ticks\n",
        "  plt.yticks([])  #remove y-ticks\n",
        "  plt.grid(False)  #no grid\n",
        "  plt.imshow(image[i], cmap=plt.cm.binary)  #plotting the image in binary colormap\n",
        "  plt.xlabel(label[i])  #x-axis label\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reshaping, Normalizing of the training and validation input dataset and One hot encoding of the corresponding output labels"
      ],
      "metadata": {
        "id": "pVa66NDhrRME"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePM8OYjgJB3X",
        "outputId": "dcb94c4c-ed95-4817-d4f2-2e8d6f23e731"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n",
            "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "(54000, 10)\n"
          ]
        }
      ],
      "source": [
        "# vectorise the data\n",
        "x_train_mnist = x_train_mnist.reshape(x_train_mnist.shape[0], 784)\n",
        "# print(x_train.shape)\n",
        "x_val_mnist  = x_val_mnist.reshape(x_val_mnist.shape[0], 784)\n",
        "x_test_mnist = x_test_mnist.reshape(x_test_mnist.shape[0], 784)\n",
        "\n",
        "# normalize\n",
        "x_train_mnist = x_train_mnist / 255.0\n",
        "x_test_mnist = x_test_mnist / 255.0\n",
        "x_val_mnist  = x_val_mnist / 255.0\n",
        "# print(x_train)\n",
        "\n",
        "# One hot encoding for labels\n",
        "def one_hot_enc(labels, dimension=10):\n",
        "    # Creating an array of zeros of shape (number of labels, number of categories)\n",
        "    one_hot_labels = np.zeros((len(labels), dimension))\n",
        "\n",
        "    one_hot_labels[np.arange(len(labels)), labels] = 1\n",
        "    return one_hot_labels\n",
        "\n",
        "\n",
        "\n",
        "print(y_train_mnist[1023])\n",
        "# one-hot encoding to the datasets\n",
        "y_train_encode_mnist = one_hot_enc(y_train_mnist)\n",
        "y_val_encode_mnist = one_hot_enc(y_val_mnist)\n",
        "y_test_encode_mnist = one_hot_enc(y_test_mnist)\n",
        "\n",
        "print(y_train_encode_mnist[1023])  # for verifying\n",
        "print(y_train_encode_mnist.shape) #verifying"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# All necessary functions(Activation,Loss,Output)::"
      ],
      "metadata": {
        "id": "gEzt4WDxrzFF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtemDZIevP-9"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "def sigmoid_deriv(x):\n",
        "    sig = sigmoid(x)\n",
        "    return sig * (1 - sig)\n",
        "\n",
        "def cross_entropy(y_hat, y):\n",
        "    m = y.shape[0]\n",
        "    logyhat = np.log(y_hat)\n",
        "    loss = -np.sum(y * logyhat) / m\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activation and activation derivatives dictionaries"
      ],
      "metadata": {
        "id": "M6cYnwVosIcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "activation_functions = {\n",
        "    \"sigmoid\": sigmoid,\n",
        "}\n",
        "\n",
        "activation_derivatives = {\n",
        "    \"sigmoid\": sigmoid_deriv,\n",
        "}"
      ],
      "metadata": {
        "id": "l6pfJrjKsNqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network with feedforward propagation::"
      ],
      "metadata": {
        "id": "qVaWW3l7s0Qs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwDGRvDNQl5h"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork:\n",
        "    #initialize the network with construtor\n",
        "    def __init__(self, layer_sizes, activation_func=\"sigmoid\", weight_init=\"random\"):\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.activation_func = activation_func\n",
        "        self.weights, self.biases = self.init_network(layer_sizes, weight_init)\n",
        "\n",
        "    #initializing function for weights and biases\n",
        "    def init_network(self, layer_sizes, weight_init):\n",
        "        weights = []\n",
        "        biases = []\n",
        "        # i = 0 to L-1; here len(self.layer_sizes) = L+1(input_layer+hidden_layers+output_layer)\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            stddev = 0.1  # standard deviation for random initialization\n",
        "            weight = np.random.randn(layer_sizes[i], layer_sizes[i + 1]) * stddev\n",
        "            bias = np.zeros((1, layer_sizes[i + 1]))\n",
        "            weights.append(weight)\n",
        "            biases.append(bias)\n",
        "        return weights, biases\n",
        "\n",
        "    #forward propagation\n",
        "    def feedforward(self, X):\n",
        "        a = {}\n",
        "        h = {\"h0\": X}\n",
        "        # i = 1 to L; here len(self.layer_sizes) = L+1(input_layer+hidden_layers+output_layer)\n",
        "        for i in range(1, len(self.layer_sizes)):\n",
        "            h_previous = h[\"h\" + str(i-1)]   #input layer(data point)\n",
        "            a_current = np.dot(h_previous, self.weights[i-1]) + self.biases[i-1]\n",
        "            a[\"a\" + str(i)] = a_current\n",
        "            if i == len(self.layer_sizes) - 1:  # for output layer\n",
        "                h_current = softmax(a_current)\n",
        "            else:\n",
        "                h_current = activation_functions[self.activation_func](a_current)   #sigmoid(a_current)\n",
        "            h[\"h\" + str(i)] = h_current\n",
        "        y_hat = h[\"h\" + str(len(self.layer_sizes) - 1)]\n",
        "        return h, a, y_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One example of probability distribution(bad distribution) over the 10 classes by feedforward NN before training the model::\n"
      ],
      "metadata": {
        "id": "EQcvWo5EXrRb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uzjq1QE9d1tW",
        "outputId": "2b730842-cc39-4fcf-935a-b54e01cc16dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A probability distribution over the 10 classes by feedforward NN before train:: \n",
            " [0.09995716 0.11253809 0.09442142 0.09669404 0.10337414 0.09979037\n",
            " 0.08663367 0.12065914 0.07798648 0.10794549]\n",
            "Corresponding labelled output:: \n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "layer_sizes = [784, 10, 10] # means input size: 28*28=784, one hidden layer of size 10 neurons, and the output class labels 10\n",
        "\n",
        "# initializing the model\n",
        "model_forward = NeuralNetwork(layer_sizes, activation_func=\"sigmoid\", weight_init=\"random\")\n",
        "_,_,y_hat = model_forward.feedforward(x_train_mnist)\n",
        "print('A probability distribution over the 10 classes by feedforward NN before train:: \\n',y_hat[5180])\n",
        "# print(sum(y_hat[51]))\n",
        "print('Corresponding labelled output:: \\n',y_train_encode_mnist[5180])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For backward propagation and the gradient descent\n"
      ],
      "metadata": {
        "id": "2xttRzToZspW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFsvDRsVcvzC"
      },
      "outputs": [],
      "source": [
        "# for backward propagation and the gradient descent\n",
        "class backward_optimizer(NeuralNetwork):\n",
        "    #backward propagation\n",
        "    def backward(self, h, a, y, y_hat):\n",
        "        number_hidden = len(self.layer_sizes) - 2  #excluding input and output layers\n",
        "        grad = self.init_grad()\n",
        "\n",
        "        #derivative of cross-entropy w.r.t softmax function\n",
        "        da = y_hat - y\n",
        "\n",
        "        # i = L to 1; here len(self.layer_sizes) = L+1(input_layer+hidden_layers+output_layer)\n",
        "        for i in reversed(range(1, len(self.layer_sizes))):\n",
        "            grad[\"dW\" + str(i)] = np.dot(h[\"h\" + str(i-1)].T, da) / y.shape[0]   #for normalizing, dividing by y.shape[0]\n",
        "            grad[\"db\" + str(i)] = np.sum(da, axis=0, keepdims=True) / y.shape[0]\n",
        "            if i > 1:  #no need to calculate derivatives after the first hidden layer\n",
        "                dh_previous = np.dot(da, self.weights[i-1].T)\n",
        "                da = dh_previous * activation_derivatives[self.activation_func](a[\"a\" + str(i-1)])\n",
        "        return grad\n",
        "\n",
        "\n",
        "    #gradient initializing\n",
        "    def init_grad(self):\n",
        "        grad = {}\n",
        "        # i = 1 to L; here len(self.layer_sizes) = L+1(input_layer+hidden_layers+output_layer)\n",
        "        for i in range(1, len(self.layer_sizes)):\n",
        "            grad[\"dW\" + str(i)] = np.zeros_like(self.weights[i-1])   #initializing dw1 with W1 having all zeros\n",
        "            grad[\"db\" + str(i)] = np.zeros_like(self.biases[i-1])    #initializing db1 with b1 having all zeros\n",
        "        return grad\n",
        "\n",
        "    #Gradient Descent Method\n",
        "    def grad_descent(self, X_train, Y_train,X_val, Y_val_encode, max_epochs, eta, batch_size):\n",
        "        train_datapoints = X_train.shape[0]  #train_datapoints = 54000\n",
        "\n",
        "        for epoch in range(max_epochs):\n",
        "            for start in range(0, train_datapoints, batch_size):\n",
        "                end = min(start + batch_size, train_datapoints)\n",
        "                X_batch = X_train[start:end]\n",
        "                Y_batch = Y_train[start:end]\n",
        "\n",
        "                # Forward prop\n",
        "                h, a, y_hat = self.feedforward(X_batch)\n",
        "                # print(h['h0'])\n",
        "\n",
        "                # Backward prop\n",
        "                grad = self.backward(h, a, Y_batch, y_hat)\n",
        "\n",
        "                # Update weights and biases with grad_descent\n",
        "                for i in range(1, len(self.layer_sizes)):\n",
        "                    self.weights[i-1] -= eta * (grad[\"dW\" + str(i)])\n",
        "                    self.biases[i-1] -= eta * grad[\"db\" + str(i)]\n",
        "\n",
        "            # printing loss and accuracy at the end of each epoch\n",
        "            loss, accuracy = self.loss_accuracy(X_train, Y_train)\n",
        "            print(f\"End of Epoch {epoch}, Training Loss: {loss}, Training Accuracy: {accuracy * 100}\")\n",
        "\n",
        "            # printing loss and accuracy on the validation dataset\n",
        "            val_loss, val_accuracy = self.loss_accuracy(X_val, Y_val_encode)\n",
        "            print(f\"End of Epoch {epoch}, Validation Loss: {val_loss}, Validation Accuracy: {val_accuracy * 100}\")\n",
        "\n",
        "    #loss_accuracy function\n",
        "    def loss_accuracy(self, X, y):\n",
        "        h, a, y_hat = self.feedforward(X)\n",
        "        loss = cross_entropy(y_hat, y)\n",
        "\n",
        "        predictions = np.argmax(y_hat, axis=1)   #finding indices of max element in each row of y_hat\n",
        "        labels = np.argmax(y, axis=1)            #finding indices of max element in each row of y\n",
        "        accuracy = np.mean(predictions == labels) #mean of the above two\n",
        "\n",
        "        return loss, accuracy\n",
        "\n",
        "    #for training the model by the optimizers\n",
        "    def train(self,optimizer,max_epochs, eta,batch_size):\n",
        "        if optimizer == 'grad_descent':\n",
        "          self.grad_descent(x_train_mnist, y_train_encode_mnist,x_val_mnist, y_val_encode_mnist, max_epochs, eta,batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Here 1st we have taken a small network of one hidden layer with size 10 neurons::\n",
        "    input size: 28*28=784, one hidden layer of size 10 neurons, and the output class labels 10"
      ],
      "metadata": {
        "id": "21KueAOeZBKF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKsUsmixRlpj"
      },
      "outputs": [],
      "source": [
        "layer_sizes = [784, 10, 10]\n",
        "\n",
        "# initializing the model\n",
        "model = backward_optimizer(layer_sizes, activation_func=\"sigmoid\", weight_init=\"random\")\n",
        "\n",
        "#for verifying the results\n",
        "\n",
        "# print(model.weights[0].shape)\n",
        "# print(model.weights[1].shape)\n",
        "# print(model.weights[0][5])\n",
        "# print(model.weights[1][99])\n",
        "# print()\n",
        "# print(model.biases[0].shape)\n",
        "# print(model.biases[1].shape)\n",
        "# print(model.biases[0][0])\n",
        "# print(model.biases[1][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **First you should verify that the code runs in a very similar manner as seen in the video. That is, with 500 forward/backward propagation steps and learning rate Î± = 0.1, the accuracy of the trained network should be about 82 to 85 percent.**"
      ],
      "metadata": {
        "id": "_OrF4gOgqBw0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3bevr0k0c2b",
        "outputId": "30b3e6cd-e0a0-432c-ef8d-f6c6bc698dd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 0, Training Loss: 2.30139155749906, Training Accuracy: 10.287037037037038\n",
            "End of Epoch 0, Validation Loss: 2.3018805051237217, Validation Accuracy: 10.533333333333333\n",
            "End of Epoch 1, Training Loss: 2.300212914282717, Training Accuracy: 10.318518518518518\n",
            "End of Epoch 1, Validation Loss: 2.3006839401392147, Validation Accuracy: 10.6\n",
            "End of Epoch 2, Training Loss: 2.299051200883306, Training Accuracy: 10.362962962962962\n",
            "End of Epoch 2, Validation Loss: 2.299503336248518, Validation Accuracy: 10.633333333333333\n",
            "End of Epoch 3, Training Loss: 2.297904951480522, Training Accuracy: 10.411111111111111\n",
            "End of Epoch 3, Validation Loss: 2.2983372556071404, Validation Accuracy: 10.7\n",
            "End of Epoch 4, Training Loss: 2.2967727904067625, Training Accuracy: 10.49074074074074\n",
            "End of Epoch 4, Validation Loss: 2.2971843492983797, Validation Accuracy: 10.733333333333334\n",
            "End of Epoch 5, Training Loss: 2.2956534246292097, Training Accuracy: 10.57037037037037\n",
            "End of Epoch 5, Validation Loss: 2.296043349892338, Validation Accuracy: 10.816666666666666\n",
            "End of Epoch 6, Training Loss: 2.294545636964705, Training Accuracy: 10.672222222222222\n",
            "End of Epoch 6, Validation Loss: 2.294913064729115, Validation Accuracy: 10.866666666666665\n",
            "End of Epoch 7, Training Loss: 2.2934482799426603, Training Accuracy: 10.785185185185185\n",
            "End of Epoch 7, Validation Loss: 2.2937923698425386, Validation Accuracy: 11.0\n",
            "End of Epoch 8, Training Loss: 2.2923602702426082, Training Accuracy: 10.883333333333333\n",
            "End of Epoch 8, Validation Loss: 2.2926802044519956, Validation Accuracy: 11.083333333333334\n",
            "End of Epoch 9, Training Loss: 2.291280583642663, Training Accuracy: 11.016666666666666\n",
            "End of Epoch 9, Validation Loss: 2.291575565959521, Validation Accuracy: 11.166666666666666\n",
            "End of Epoch 10, Training Loss: 2.2902082504234578, Training Accuracy: 11.174074074074074\n",
            "End of Epoch 10, Validation Loss: 2.290477505397403, Validation Accuracy: 11.333333333333332\n",
            "End of Epoch 11, Training Loss: 2.289142351179129, Training Accuracy: 11.342592592592593\n",
            "End of Epoch 11, Validation Loss: 2.2893851232785436, Validation Accuracy: 11.35\n",
            "End of Epoch 12, Training Loss: 2.2880820129930184, Training Accuracy: 11.501851851851852\n",
            "End of Epoch 12, Validation Loss: 2.288297565807772, Validation Accuracy: 11.5\n",
            "End of Epoch 13, Training Loss: 2.2870264059409005, Training Accuracy: 11.683333333333334\n",
            "End of Epoch 13, Validation Loss: 2.2872140214174506, Validation Accuracy: 11.683333333333334\n",
            "End of Epoch 14, Training Loss: 2.2859747398891033, Training Accuracy: 11.901851851851852\n",
            "End of Epoch 14, Validation Loss: 2.286133717595096, Validation Accuracy: 11.866666666666667\n",
            "End of Epoch 15, Training Loss: 2.2849262615586623, Training Accuracy: 12.114814814814814\n",
            "End of Epoch 15, Validation Loss: 2.2850559179745886, Validation Accuracy: 12.133333333333333\n",
            "End of Epoch 16, Training Loss: 2.283880251830082, Training Accuracy: 12.322222222222221\n",
            "End of Epoch 16, Validation Loss: 2.283979919665807, Validation Accuracy: 12.316666666666666\n",
            "End of Epoch 17, Training Loss: 2.282836023266124, Training Accuracy: 12.548148148148147\n",
            "End of Epoch 17, Validation Loss: 2.2829050508004274, Validation Accuracy: 12.533333333333333\n",
            "End of Epoch 18, Training Loss: 2.2817929178326013, Training Accuracy: 12.762962962962963\n",
            "End of Epoch 18, Validation Loss: 2.281830668274078, Validation Accuracy: 12.8\n",
            "End of Epoch 19, Training Loss: 2.280750304799341, Training Accuracy: 12.972222222222221\n",
            "End of Epoch 19, Validation Loss: 2.2807561556672353, Validation Accuracy: 13.116666666666665\n",
            "End of Epoch 20, Training Loss: 2.2797075788054157, Training Accuracy: 13.170370370370371\n",
            "End of Epoch 20, Validation Loss: 2.2796809213291422, Validation Accuracy: 13.450000000000001\n",
            "End of Epoch 21, Training Loss: 2.278664158074413, Training Accuracy: 13.43148148148148\n",
            "End of Epoch 21, Validation Loss: 2.2786043966106746, Validation Accuracy: 13.65\n",
            "End of Epoch 22, Training Loss: 2.2776194827670233, Training Accuracy: 13.694444444444445\n",
            "End of Epoch 22, Validation Loss: 2.2775260342335657, Validation Accuracy: 13.916666666666666\n",
            "End of Epoch 23, Training Loss: 2.276573013459469, Training Accuracy: 13.962962962962964\n",
            "End of Epoch 23, Validation Loss: 2.276445306784645, Validation Accuracy: 14.166666666666666\n",
            "End of Epoch 24, Training Loss: 2.275524229737524, Training Accuracy: 14.33888888888889\n",
            "End of Epoch 24, Validation Loss: 2.2753617053249267, Validation Accuracy: 14.399999999999999\n",
            "End of Epoch 25, Training Loss: 2.274472628896799, Training Accuracy: 14.762962962962964\n",
            "End of Epoch 25, Validation Loss: 2.2742747381043253, Validation Accuracy: 14.899999999999999\n",
            "End of Epoch 26, Training Loss: 2.2734177247409506, Training Accuracy: 15.303703703703702\n",
            "End of Epoch 26, Validation Loss: 2.2731839293737144, Validation Accuracy: 15.45\n",
            "End of Epoch 27, Training Loss: 2.2723590464701844, Training Accuracy: 15.935185185185185\n",
            "End of Epoch 27, Validation Loss: 2.2720888182867887, Validation Accuracy: 16.1\n",
            "End of Epoch 28, Training Loss: 2.2712961376532075, Training Accuracy: 16.674074074074074\n",
            "End of Epoch 28, Validation Loss: 2.2709889578849185, Validation Accuracy: 16.966666666666665\n",
            "End of Epoch 29, Training Loss: 2.27022855527636, Training Accuracy: 17.58888888888889\n",
            "End of Epoch 29, Validation Loss: 2.2698839141587803, Validation Accuracy: 17.816666666666666\n",
            "End of Epoch 30, Training Loss: 2.2691558688642393, Training Accuracy: 18.431481481481484\n",
            "End of Epoch 30, Validation Loss: 2.268773265181131, Validation Accuracy: 18.53333333333333\n",
            "End of Epoch 31, Training Loss: 2.268077659666649, Training Accuracy: 19.392592592592592\n",
            "End of Epoch 31, Validation Loss: 2.2676566003055694, Validation Accuracy: 19.45\n",
            "End of Epoch 32, Training Loss: 2.2669935199071127, Training Accuracy: 20.405555555555555\n",
            "End of Epoch 32, Validation Loss: 2.2665335194265803, Validation Accuracy: 20.599999999999998\n",
            "End of Epoch 33, Training Loss: 2.2659030520886563, Training Accuracy: 21.56111111111111\n",
            "End of Epoch 33, Validation Loss: 2.2654036322965716, Validation Accuracy: 21.766666666666666\n",
            "End of Epoch 34, Training Loss: 2.264805868352876, Training Accuracy: 22.577777777777776\n",
            "End of Epoch 34, Validation Loss: 2.2642665578959433, Validation Accuracy: 22.55\n",
            "End of Epoch 35, Training Loss: 2.263701589888662, Training Accuracy: 23.692592592592593\n",
            "End of Epoch 35, Validation Loss: 2.2631219238525957, Validation Accuracy: 23.35\n",
            "End of Epoch 36, Training Loss: 2.262589846387244, Training Accuracy: 24.648148148148145\n",
            "End of Epoch 36, Validation Loss: 2.2619693659075377, Validation Accuracy: 24.433333333333334\n",
            "End of Epoch 37, Training Loss: 2.2614702755404776, Training Accuracy: 25.657407407407405\n",
            "End of Epoch 37, Validation Loss: 2.260808527423539, Validation Accuracy: 25.383333333333336\n",
            "End of Epoch 38, Training Loss: 2.260342522579568, Training Accuracy: 26.48333333333333\n",
            "End of Epoch 38, Validation Loss: 2.259639058934024, Validation Accuracy: 26.38333333333333\n",
            "End of Epoch 39, Training Loss: 2.259206239851583, Training Accuracy: 27.314814814814813\n",
            "End of Epoch 39, Validation Loss: 2.2584606177295874, Validation Accuracy: 27.133333333333333\n",
            "End of Epoch 40, Training Loss: 2.2580610864313915, Training Accuracy: 28.085185185185185\n",
            "End of Epoch 40, Validation Loss: 2.257272867479751, Validation Accuracy: 28.050000000000004\n",
            "End of Epoch 41, Training Loss: 2.2569067277667534, Training Accuracy: 28.78333333333333\n",
            "End of Epoch 41, Validation Loss: 2.256075477887727, Validation Accuracy: 28.76666666666667\n",
            "End of Epoch 42, Training Loss: 2.255742835354546, Training Accuracy: 29.442592592592593\n",
            "End of Epoch 42, Validation Loss: 2.2548681243761353, Validation Accuracy: 29.51666666666667\n",
            "End of Epoch 43, Training Loss: 2.2545690864461574, Training Accuracy: 30.038888888888888\n",
            "End of Epoch 43, Validation Loss: 2.253650487801766, Validation Accuracy: 30.233333333333334\n",
            "End of Epoch 44, Training Loss: 2.2533851637803175, Training Accuracy: 30.61111111111111\n",
            "End of Epoch 44, Validation Loss: 2.2524222541976155, Validation Accuracy: 30.85\n",
            "End of Epoch 45, Training Loss: 2.252190755341685, Training Accuracy: 31.224074074074075\n",
            "End of Epoch 45, Validation Loss: 2.251183114540542, Validation Accuracy: 31.2\n",
            "End of Epoch 46, Training Loss: 2.250985554143652, Training Accuracy: 31.72037037037037\n",
            "End of Epoch 46, Validation Loss: 2.2499327645430065, Validation Accuracy: 31.616666666666664\n",
            "End of Epoch 47, Training Loss: 2.24976925803393, Training Accuracy: 32.272222222222226\n",
            "End of Epoch 47, Validation Loss: 2.248670904467468, Validation Accuracy: 32.15\n",
            "End of Epoch 48, Training Loss: 2.248541569521576, Training Accuracy: 32.74444444444444\n",
            "End of Epoch 48, Validation Loss: 2.247397238962085, Validation Accuracy: 32.483333333333334\n",
            "End of Epoch 49, Training Loss: 2.2473021956241874, Training Accuracy: 33.13333333333333\n",
            "End of Epoch 49, Validation Loss: 2.24611147691647, Validation Accuracy: 32.800000000000004\n",
            "End of Epoch 50, Training Loss: 2.246050847734094, Training Accuracy: 33.51851851851852\n",
            "End of Epoch 50, Validation Loss: 2.244813331336323, Validation Accuracy: 33.25\n",
            "End of Epoch 51, Training Loss: 2.2447872415024315, Training Accuracy: 33.894444444444446\n",
            "End of Epoch 51, Validation Loss: 2.2435025192358373, Validation Accuracy: 33.416666666666664\n",
            "End of Epoch 52, Training Loss: 2.2435110967400447, Training Accuracy: 34.275925925925925\n",
            "End of Epoch 52, Validation Loss: 2.2421787615468216, Validation Accuracy: 33.85\n",
            "End of Epoch 53, Training Loss: 2.2422221373342364, Training Accuracy: 34.57592592592593\n",
            "End of Epoch 53, Validation Loss: 2.2408417830435763, Validation Accuracy: 34.38333333333333\n",
            "End of Epoch 54, Training Loss: 2.2409200911804295, Training Accuracy: 34.87777777777778\n",
            "End of Epoch 54, Validation Loss: 2.2394913122825733, Validation Accuracy: 34.88333333333333\n",
            "End of Epoch 55, Training Loss: 2.239604690127847, Training Accuracy: 35.2037037037037\n",
            "End of Epoch 55, Validation Loss: 2.2381270815560685, Validation Accuracy: 35.15\n",
            "End of Epoch 56, Training Loss: 2.2382756699383743, Training Accuracy: 35.455555555555556\n",
            "End of Epoch 56, Validation Loss: 2.2367488268588125, Validation Accuracy: 35.43333333333333\n",
            "End of Epoch 57, Training Loss: 2.2369327702577935, Training Accuracy: 35.73148148148148\n",
            "End of Epoch 57, Validation Loss: 2.235356287867044, Validation Accuracy: 35.6\n",
            "End of Epoch 58, Training Loss: 2.2355757345986302, Training Accuracy: 35.9962962962963\n",
            "End of Epoch 58, Validation Loss: 2.233949207929023, Validation Accuracy: 35.96666666666667\n",
            "End of Epoch 59, Training Loss: 2.2342043103338796, Training Accuracy: 36.214814814814815\n",
            "End of Epoch 59, Validation Loss: 2.2325273340663707, Validation Accuracy: 36.35\n",
            "End of Epoch 60, Training Loss: 2.2328182487009, Training Accuracy: 36.422222222222224\n",
            "End of Epoch 60, Validation Loss: 2.2310904169855093, Validation Accuracy: 36.53333333333333\n",
            "End of Epoch 61, Training Loss: 2.2314173048148005, Training Accuracy: 36.629629629629626\n",
            "End of Epoch 61, Validation Loss: 2.22963821109854, Validation Accuracy: 36.733333333333334\n",
            "End of Epoch 62, Training Loss: 2.230001237690682, Training Accuracy: 36.861111111111114\n",
            "End of Epoch 62, Validation Loss: 2.228170474552908, Validation Accuracy: 37.2\n",
            "End of Epoch 63, Training Loss: 2.2285698102740867, Training Accuracy: 37.053703703703704\n",
            "End of Epoch 63, Validation Loss: 2.2266869692692324, Validation Accuracy: 37.516666666666666\n",
            "End of Epoch 64, Training Loss: 2.227122789479059, Training Accuracy: 37.24074074074074\n",
            "End of Epoch 64, Validation Loss: 2.2251874609866964, Validation Accuracy: 37.733333333333334\n",
            "End of Epoch 65, Training Loss: 2.225659946233223, Training Accuracy: 37.42962962962963\n",
            "End of Epoch 65, Validation Loss: 2.2236717193154147, Validation Accuracy: 37.983333333333334\n",
            "End of Epoch 66, Training Loss: 2.2241810555293116, Training Accuracy: 37.59074074074074\n",
            "End of Epoch 66, Validation Loss: 2.222139517795208, Validation Accuracy: 38.11666666666667\n",
            "End of Epoch 67, Training Loss: 2.2226858964825906, Training Accuracy: 37.785185185185185\n",
            "End of Epoch 67, Validation Loss: 2.2205906339602386, Validation Accuracy: 38.35\n",
            "End of Epoch 68, Training Loss: 2.2211742523936264, Training Accuracy: 37.99444444444444\n",
            "End of Epoch 68, Validation Loss: 2.219024849408968, Validation Accuracy: 38.550000000000004\n",
            "End of Epoch 69, Training Loss: 2.2196459108159035, Training Accuracy: 38.135185185185186\n",
            "End of Epoch 69, Validation Loss: 2.2174419498789106, Validation Accuracy: 38.71666666666667\n",
            "End of Epoch 70, Training Loss: 2.218100663627735, Training Accuracy: 38.33888888888889\n",
            "End of Epoch 70, Validation Loss: 2.215841725325686, Validation Accuracy: 38.9\n",
            "End of Epoch 71, Training Loss: 2.216538307108018, Training Accuracy: 38.507407407407406\n",
            "End of Epoch 71, Validation Loss: 2.2142239700058575, Validation Accuracy: 38.95\n",
            "End of Epoch 72, Training Loss: 2.2149586420153082, Training Accuracy: 38.65555555555555\n",
            "End of Epoch 72, Validation Loss: 2.2125884825630933, Validation Accuracy: 39.06666666666666\n",
            "End of Epoch 73, Training Loss: 2.2133614736697695, Training Accuracy: 38.81481481481481\n",
            "End of Epoch 73, Validation Loss: 2.2109350661171594, Validation Accuracy: 39.15\n",
            "End of Epoch 74, Training Loss: 2.211746612037521, Training Accuracy: 38.977777777777774\n",
            "End of Epoch 74, Validation Loss: 2.2092635283552995, Validation Accuracy: 39.28333333333333\n",
            "End of Epoch 75, Training Loss: 2.210113871816946, Training Accuracy: 39.13148148148149\n",
            "End of Epoch 75, Validation Loss: 2.207573681625542, Validation Accuracy: 39.36666666666667\n",
            "End of Epoch 76, Training Loss: 2.20846307252652, Training Accuracy: 39.30740740740741\n",
            "End of Epoch 76, Validation Loss: 2.205865343031511, Validation Accuracy: 39.75\n",
            "End of Epoch 77, Training Loss: 2.206794038593751, Training Accuracy: 39.41481481481481\n",
            "End of Epoch 77, Validation Loss: 2.2041383345283054, Validation Accuracy: 39.93333333333333\n",
            "End of Epoch 78, Training Loss: 2.205106599444797, Training Accuracy: 39.58148148148148\n",
            "End of Epoch 78, Validation Loss: 2.2023924830190453, Validation Accuracy: 40.08333333333333\n",
            "End of Epoch 79, Training Loss: 2.2034005895943913, Training Accuracy: 39.75925925925926\n",
            "End of Epoch 79, Validation Loss: 2.200627620451678, Validation Accuracy: 40.28333333333333\n",
            "End of Epoch 80, Training Loss: 2.2016758487356904, Training Accuracy: 39.93518518518518\n",
            "End of Epoch 80, Validation Loss: 2.1988435839156617, Validation Accuracy: 40.33333333333333\n",
            "End of Epoch 81, Training Loss: 2.1999322218296578, Training Accuracy: 40.09259259259259\n",
            "End of Epoch 81, Validation Loss: 2.197040215738161, Validation Accuracy: 40.516666666666666\n",
            "End of Epoch 82, Training Loss: 2.1981695591936523, Training Accuracy: 40.27407407407407\n",
            "End of Epoch 82, Validation Loss: 2.195217363579381, Validation Accuracy: 40.699999999999996\n",
            "End of Epoch 83, Training Loss: 2.196387716588876, Training Accuracy: 40.40555555555556\n",
            "End of Epoch 83, Validation Loss: 2.1933748805267115, Validation Accuracy: 40.9\n",
            "End of Epoch 84, Training Loss: 2.1945865553063553, Training Accuracy: 40.55740740740741\n",
            "End of Epoch 84, Validation Loss: 2.1915126251873427, Validation Accuracy: 41.05\n",
            "End of Epoch 85, Training Loss: 2.192765942251155, Training Accuracy: 40.696296296296296\n",
            "End of Epoch 85, Validation Loss: 2.189630461779041, Validation Accuracy: 41.266666666666666\n",
            "End of Epoch 86, Training Loss: 2.1909257500245296, Training Accuracy: 40.82222222222222\n",
            "End of Epoch 86, Validation Loss: 2.187728260218784, Validation Accuracy: 41.41666666666667\n",
            "End of Epoch 87, Training Loss: 2.1890658570037425, Training Accuracy: 40.992592592592594\n",
            "End of Epoch 87, Validation Loss: 2.1858058962089775, Validation Accuracy: 41.53333333333333\n",
            "End of Epoch 88, Training Loss: 2.187186147419289, Training Accuracy: 41.129629629629626\n",
            "End of Epoch 88, Validation Loss: 2.1838632513209815, Validation Accuracy: 41.71666666666667\n",
            "End of Epoch 89, Training Loss: 2.185286511429294, Training Accuracy: 41.285185185185185\n",
            "End of Epoch 89, Validation Loss: 2.181900213075704, Validation Accuracy: 41.86666666666667\n",
            "End of Epoch 90, Training Loss: 2.183366845190848, Training Accuracy: 41.44444444444444\n",
            "End of Epoch 90, Validation Loss: 2.179916675021021, Validation Accuracy: 41.983333333333334\n",
            "End of Epoch 91, Training Loss: 2.1814270509280895, Training Accuracy: 41.57777777777778\n",
            "End of Epoch 91, Validation Loss: 2.177912536805812, Validation Accuracy: 42.25\n",
            "End of Epoch 92, Training Loss: 2.1794670369968303, Training Accuracy: 41.70185185185185\n",
            "End of Epoch 92, Validation Loss: 2.1758877042504166, Validation Accuracy: 42.31666666666667\n",
            "End of Epoch 93, Training Loss: 2.177486717945573, Training Accuracy: 41.83888888888889\n",
            "End of Epoch 93, Validation Loss: 2.1738420894133212, Validation Accuracy: 42.46666666666667\n",
            "End of Epoch 94, Training Loss: 2.1754860145727384, Training Accuracy: 41.97777777777778\n",
            "End of Epoch 94, Validation Loss: 2.171775610653925, Validation Accuracy: 42.66666666666667\n",
            "End of Epoch 95, Training Loss: 2.1734648539800077, Training Accuracy: 42.12407407407407\n",
            "End of Epoch 95, Validation Loss: 2.1696881926912237, Validation Accuracy: 42.766666666666666\n",
            "End of Epoch 96, Training Loss: 2.1714231696216104, Training Accuracy: 42.224074074074075\n",
            "End of Epoch 96, Validation Loss: 2.1675797666582968, Validation Accuracy: 42.9\n",
            "End of Epoch 97, Training Loss: 2.1693609013494903, Training Accuracy: 42.37777777777777\n",
            "End of Epoch 97, Validation Loss: 2.1654502701524763, Validation Accuracy: 43.13333333333333\n",
            "End of Epoch 98, Training Loss: 2.1672779954542554, Training Accuracy: 42.51481481481482\n",
            "End of Epoch 98, Validation Loss: 2.163299647281103, Validation Accuracy: 43.166666666666664\n",
            "End of Epoch 99, Training Loss: 2.1651744047018284, Training Accuracy: 42.63703703703703\n",
            "End of Epoch 99, Validation Loss: 2.161127848702784, Validation Accuracy: 43.416666666666664\n",
            "End of Epoch 100, Training Loss: 2.163050088365745, Training Accuracy: 42.788888888888884\n",
            "End of Epoch 100, Validation Loss: 2.158934831664103, Validation Accuracy: 43.55\n",
            "End of Epoch 101, Training Loss: 2.1609050122550717, Training Accuracy: 42.93703703703704\n",
            "End of Epoch 101, Validation Loss: 2.1567205600317094, Validation Accuracy: 43.71666666666666\n",
            "End of Epoch 102, Training Loss: 2.1587391487378866, Training Accuracy: 43.068518518518516\n",
            "End of Epoch 102, Validation Loss: 2.154485004319771, Validation Accuracy: 43.86666666666667\n",
            "End of Epoch 103, Training Loss: 2.1565524767603383, Training Accuracy: 43.17037037037037\n",
            "End of Epoch 103, Validation Loss: 2.1522281417127553, Validation Accuracy: 44.1\n",
            "End of Epoch 104, Training Loss: 2.154344981861243, Training Accuracy: 43.272222222222226\n",
            "End of Epoch 104, Validation Loss: 2.1499499560835362, Validation Accuracy: 44.166666666666664\n",
            "End of Epoch 105, Training Loss: 2.152116656182259, Training Accuracy: 43.44444444444445\n",
            "End of Epoch 105, Validation Loss: 2.1476504380068184, Validation Accuracy: 44.2\n",
            "End of Epoch 106, Training Loss: 2.1498674984736406, Training Accuracy: 43.58148148148148\n",
            "End of Epoch 106, Validation Loss: 2.1453295847679126, Validation Accuracy: 44.35\n",
            "End of Epoch 107, Training Loss: 2.1475975140955885, Training Accuracy: 43.72592592592592\n",
            "End of Epoch 107, Validation Loss: 2.142987400366858, Validation Accuracy: 44.516666666666666\n",
            "End of Epoch 108, Training Loss: 2.1453067150152476, Training Accuracy: 43.82962962962963\n",
            "End of Epoch 108, Validation Loss: 2.140623895517954, Validation Accuracy: 44.6\n",
            "End of Epoch 109, Training Loss: 2.142995119799391, Training Accuracy: 43.98518518518518\n",
            "End of Epoch 109, Validation Loss: 2.1382390876447155, Validation Accuracy: 44.78333333333334\n",
            "End of Epoch 110, Training Loss: 2.1406627536028298, Training Accuracy: 44.10370370370371\n",
            "End of Epoch 110, Validation Loss: 2.1358330008703272, Validation Accuracy: 44.983333333333334\n",
            "End of Epoch 111, Training Loss: 2.1383096481526134, Training Accuracy: 44.27037037037037\n",
            "End of Epoch 111, Validation Loss: 2.133405666003632, Validation Accuracy: 45.1\n",
            "End of Epoch 112, Training Loss: 2.1359358417280783, Training Accuracy: 44.39074074074074\n",
            "End of Epoch 112, Validation Loss: 2.1309571205207263, Validation Accuracy: 45.18333333333333\n",
            "End of Epoch 113, Training Loss: 2.1335413791367994, Training Accuracy: 44.52407407407407\n",
            "End of Epoch 113, Validation Loss: 2.1284874085422354, Validation Accuracy: 45.33333333333333\n",
            "End of Epoch 114, Training Loss: 2.1311263116865278, Training Accuracy: 44.67962962962963\n",
            "End of Epoch 114, Validation Loss: 2.1259965808063224, Validation Accuracy: 45.516666666666666\n",
            "End of Epoch 115, Training Loss: 2.128690697153177, Training Accuracy: 44.81296296296296\n",
            "End of Epoch 115, Validation Loss: 2.1234846946375296, Validation Accuracy: 45.71666666666667\n",
            "End of Epoch 116, Training Loss: 2.1262345997449206, Training Accuracy: 44.9537037037037\n",
            "End of Epoch 116, Validation Loss: 2.120951813911513, Validation Accuracy: 45.81666666666667\n",
            "End of Epoch 117, Training Loss: 2.1237580900624913, Training Accuracy: 45.07592592592592\n",
            "End of Epoch 117, Validation Loss: 2.118398009015756, Validation Accuracy: 45.9\n",
            "End of Epoch 118, Training Loss: 2.12126124505575, Training Accuracy: 45.22592592592593\n",
            "End of Epoch 118, Validation Loss: 2.115823356806353, Validation Accuracy: 46.06666666666667\n",
            "End of Epoch 119, Training Loss: 2.1187441479765843, Training Accuracy: 45.33333333333333\n",
            "End of Epoch 119, Validation Loss: 2.1132279405609355, Validation Accuracy: 46.166666666666664\n",
            "End of Epoch 120, Training Loss: 2.1162068883282394, Training Accuracy: 45.487037037037034\n",
            "End of Epoch 120, Validation Loss: 2.110611849927827, Validation Accuracy: 46.266666666666666\n",
            "End of Epoch 121, Training Loss: 2.1136495618111297, Training Accuracy: 45.57962962962963\n",
            "End of Epoch 121, Validation Loss: 2.1079751808715184, Validation Accuracy: 46.38333333333333\n",
            "End of Epoch 122, Training Loss: 2.1110722702652174, Training Accuracy: 45.711111111111116\n",
            "End of Epoch 122, Validation Loss: 2.1053180356145424, Validation Accuracy: 46.5\n",
            "End of Epoch 123, Training Loss: 2.1084751216090236, Training Accuracy: 45.85185185185185\n",
            "End of Epoch 123, Validation Loss: 2.1026405225758267, Validation Accuracy: 46.56666666666667\n",
            "End of Epoch 124, Training Loss: 2.1058582297753548, Training Accuracy: 45.97777777777778\n",
            "End of Epoch 124, Validation Loss: 2.099942756305619, Validation Accuracy: 46.733333333333334\n",
            "End of Epoch 125, Training Loss: 2.103221714643791, Training Accuracy: 46.105555555555554\n",
            "End of Epoch 125, Validation Loss: 2.0972248574170527, Validation Accuracy: 46.9\n",
            "End of Epoch 126, Training Loss: 2.100565701970029, Training Accuracy: 46.20740740740741\n",
            "End of Epoch 126, Validation Loss: 2.0944869525144503, Validation Accuracy: 47.03333333333333\n",
            "End of Epoch 127, Training Loss: 2.0978903233121335, Training Accuracy: 46.34814814814815\n",
            "End of Epoch 127, Validation Loss: 2.091729174118424, Validation Accuracy: 47.13333333333333\n",
            "End of Epoch 128, Training Loss: 2.095195715953759, Training Accuracy: 46.4537037037037\n",
            "End of Epoch 128, Validation Loss: 2.0889516605878673, Validation Accuracy: 47.28333333333333\n",
            "End of Epoch 129, Training Loss: 2.092482022824415, Training Accuracy: 46.58888888888889\n",
            "End of Epoch 129, Validation Loss: 2.08615455603891, Validation Accuracy: 47.41666666666667\n",
            "End of Epoch 130, Training Loss: 2.0897493924168353, Training Accuracy: 46.705555555555556\n",
            "End of Epoch 130, Validation Loss: 2.083338010260905, Validation Accuracy: 47.516666666666666\n",
            "End of Epoch 131, Training Loss: 2.0869979787015063, Training Accuracy: 46.837037037037035\n",
            "End of Epoch 131, Validation Loss: 2.08050217862953, Validation Accuracy: 47.66666666666667\n",
            "End of Epoch 132, Training Loss: 2.084227941038425, Training Accuracy: 46.95740740740741\n",
            "End of Epoch 132, Validation Loss: 2.077647222017076, Validation Accuracy: 47.75\n",
            "End of Epoch 133, Training Loss: 2.081439444086137, Training Accuracy: 47.09814814814815\n",
            "End of Epoch 133, Validation Loss: 2.074773306699989, Validation Accuracy: 47.91666666666667\n",
            "End of Epoch 134, Training Loss: 2.0786326577081384, Training Accuracy: 47.21111111111111\n",
            "End of Epoch 134, Validation Loss: 2.071880604263744, Validation Accuracy: 48.03333333333333\n",
            "End of Epoch 135, Training Loss: 2.075807756876661, Training Accuracy: 47.31666666666667\n",
            "End of Epoch 135, Validation Loss: 2.0689692915051197, Validation Accuracy: 48.199999999999996\n",
            "End of Epoch 136, Training Loss: 2.072964921573945, Training Accuracy: 47.446296296296296\n",
            "End of Epoch 136, Validation Loss: 2.0660395503319426, Validation Accuracy: 48.43333333333334\n",
            "End of Epoch 137, Training Loss: 2.0701043366910366, Training Accuracy: 47.55\n",
            "End of Epoch 137, Validation Loss: 2.0630915676603796, Validation Accuracy: 48.68333333333334\n",
            "End of Epoch 138, Training Loss: 2.0672261919241826, Training Accuracy: 47.70185185185185\n",
            "End of Epoch 138, Validation Loss: 2.0601255353098424, Validation Accuracy: 48.78333333333333\n",
            "End of Epoch 139, Training Loss: 2.0643306816688822, Training Accuracy: 47.85\n",
            "End of Epoch 139, Validation Loss: 2.0571416498955792, Validation Accuracy: 48.88333333333333\n",
            "End of Epoch 140, Training Loss: 2.0614180049116566, Training Accuracy: 47.98703703703704\n",
            "End of Epoch 140, Validation Loss: 2.054140112719034, Validation Accuracy: 48.983333333333334\n",
            "End of Epoch 141, Training Loss: 2.058488365119632, Training Accuracy: 48.13148148148148\n",
            "End of Epoch 141, Validation Loss: 2.0511211296560354, Validation Accuracy: 49.1\n",
            "End of Epoch 142, Training Loss: 2.055541970127964, Training Accuracy: 48.25185185185185\n",
            "End of Epoch 142, Validation Loss: 2.0480849110428982, Validation Accuracy: 49.31666666666666\n",
            "End of Epoch 143, Training Loss: 2.052579032025216, Training Accuracy: 48.379629629629626\n",
            "End of Epoch 143, Validation Loss: 2.045031671560522, Validation Accuracy: 49.4\n",
            "End of Epoch 144, Training Loss: 2.0495997670367405, Training Accuracy: 48.50370370370371\n",
            "End of Epoch 144, Validation Loss: 2.0419616301165493, Validation Accuracy: 49.55\n",
            "End of Epoch 145, Training Loss: 2.0466043954061535, Training Accuracy: 48.644444444444446\n",
            "End of Epoch 145, Validation Loss: 2.03887500972568, Validation Accuracy: 49.766666666666666\n",
            "End of Epoch 146, Training Loss: 2.043593141274984, Training Accuracy: 48.73888888888889\n",
            "End of Epoch 146, Validation Loss: 2.035772037388221, Validation Accuracy: 49.916666666666664\n",
            "End of Epoch 147, Training Loss: 2.0405662325605873, Training Accuracy: 48.864814814814814\n",
            "End of Epoch 147, Validation Loss: 2.0326529439669567, Validation Accuracy: 49.983333333333334\n",
            "End of Epoch 148, Training Loss: 2.0375239008323875, Training Accuracy: 48.977777777777774\n",
            "End of Epoch 148, Validation Loss: 2.0295179640624257, Validation Accuracy: 50.18333333333334\n",
            "End of Epoch 149, Training Loss: 2.0344663811865766, Training Accuracy: 49.12037037037037\n",
            "End of Epoch 149, Validation Loss: 2.0263673358867056, Validation Accuracy: 50.33333333333333\n",
            "End of Epoch 150, Training Loss: 2.031393912119341, Training Accuracy: 49.2074074074074\n",
            "End of Epoch 150, Validation Loss: 2.023201301135785, Validation Accuracy: 50.449999999999996\n",
            "End of Epoch 151, Training Loss: 2.0283067353987083, Training Accuracy: 49.324074074074076\n",
            "End of Epoch 151, Validation Loss: 2.020020104860634, Validation Accuracy: 50.56666666666667\n",
            "End of Epoch 152, Training Loss: 2.025205095935148, Training Accuracy: 49.43703703703704\n",
            "End of Epoch 152, Validation Loss: 2.0168239953370635, Validation Accuracy: 50.66666666666667\n",
            "End of Epoch 153, Training Loss: 2.0220892416509924, Training Accuracy: 49.55555555555556\n",
            "End of Epoch 153, Validation Loss: 2.013613223934477, Validation Accuracy: 50.78333333333334\n",
            "End of Epoch 154, Training Loss: 2.018959423348813, Training Accuracy: 49.666666666666664\n",
            "End of Epoch 154, Validation Loss: 2.0103880449836242, Validation Accuracy: 51.06666666666667\n",
            "End of Epoch 155, Training Loss: 2.015815894578864, Training Accuracy: 49.75370370370371\n",
            "End of Epoch 155, Validation Loss: 2.0071487156434555, Validation Accuracy: 51.11666666666667\n",
            "End of Epoch 156, Training Loss: 2.0126589115056657, Training Accuracy: 49.87222222222222\n",
            "End of Epoch 156, Validation Loss: 2.0038954957671913, Validation Accuracy: 51.16666666666667\n",
            "End of Epoch 157, Training Loss: 2.0094887327739235, Training Accuracy: 50.00185185185185\n",
            "End of Epoch 157, Validation Loss: 2.0006286477677233, Validation Accuracy: 51.24999999999999\n",
            "End of Epoch 158, Training Loss: 2.0063056193738134, Training Accuracy: 50.11666666666667\n",
            "End of Epoch 158, Validation Loss: 1.9973484364824488, Validation Accuracy: 51.366666666666674\n",
            "End of Epoch 159, Training Loss: 2.0031098345058163, Training Accuracy: 50.242592592592594\n",
            "End of Epoch 159, Validation Loss: 1.994055129037664, Validation Accuracy: 51.483333333333334\n",
            "End of Epoch 160, Training Loss: 1.9999016434452006, Training Accuracy: 50.349999999999994\n",
            "End of Epoch 160, Validation Loss: 1.99074899471263, Validation Accuracy: 51.63333333333333\n",
            "End of Epoch 161, Training Loss: 1.9966813134062806, Training Accuracy: 50.411111111111104\n",
            "End of Epoch 161, Validation Loss: 1.9874303048034287, Validation Accuracy: 51.83333333333333\n",
            "End of Epoch 162, Training Loss: 1.9934491134065764, Training Accuracy: 50.522222222222226\n",
            "End of Epoch 162, Validation Loss: 1.984099332486731, Validation Accuracy: 52.03333333333333\n",
            "End of Epoch 163, Training Loss: 1.990205314130991, Training Accuracy: 50.64074074074074\n",
            "End of Epoch 163, Validation Loss: 1.9807563526835936, Validation Accuracy: 52.233333333333334\n",
            "End of Epoch 164, Training Loss: 1.9869501877961677, Training Accuracy: 50.770370370370365\n",
            "End of Epoch 164, Validation Loss: 1.9774016419234064, Validation Accuracy: 52.33333333333333\n",
            "End of Epoch 165, Training Loss: 1.9836840080150975, Training Accuracy: 50.87962962962963\n",
            "End of Epoch 165, Validation Loss: 1.9740354782081189, Validation Accuracy: 52.55\n",
            "End of Epoch 166, Training Loss: 1.9804070496621555, Training Accuracy: 50.99259259259259\n",
            "End of Epoch 166, Validation Loss: 1.9706581408768509, Validation Accuracy: 52.78333333333334\n",
            "End of Epoch 167, Training Loss: 1.9771195887386648, Training Accuracy: 51.14814814814815\n",
            "End of Epoch 167, Validation Loss: 1.967269910471025, Validation Accuracy: 52.93333333333333\n",
            "End of Epoch 168, Training Loss: 1.9738219022391201, Training Accuracy: 51.300000000000004\n",
            "End of Epoch 168, Validation Loss: 1.9638710686001262, Validation Accuracy: 53.13333333333333\n",
            "End of Epoch 169, Training Loss: 1.970514268018195, Training Accuracy: 51.416666666666664\n",
            "End of Epoch 169, Validation Loss: 1.9604618978082209, Validation Accuracy: 53.233333333333334\n",
            "End of Epoch 170, Training Loss: 1.9671969646586493, Training Accuracy: 51.53888888888889\n",
            "End of Epoch 170, Validation Loss: 1.9570426814413373, Validation Accuracy: 53.400000000000006\n",
            "End of Epoch 171, Training Loss: 1.963870271340274, Training Accuracy: 51.67222222222222\n",
            "End of Epoch 171, Validation Loss: 1.953613703515838, Validation Accuracy: 53.5\n",
            "End of Epoch 172, Training Loss: 1.9605344677099694, Training Accuracy: 51.818518518518516\n",
            "End of Epoch 172, Validation Loss: 1.9501752485878894, Validation Accuracy: 53.583333333333336\n",
            "End of Epoch 173, Training Loss: 1.9571898337530884, Training Accuracy: 51.92962962962962\n",
            "End of Epoch 173, Validation Loss: 1.9467276016241388, Validation Accuracy: 53.75\n",
            "End of Epoch 174, Training Loss: 1.9538366496661455, Training Accuracy: 52.00925925925925\n",
            "End of Epoch 174, Validation Loss: 1.9432710478737172, Validation Accuracy: 53.86666666666666\n",
            "End of Epoch 175, Training Loss: 1.9504751957310207, Training Accuracy: 52.11851851851852\n",
            "End of Epoch 175, Validation Loss: 1.9398058727416623, Validation Accuracy: 54.03333333333333\n",
            "End of Epoch 176, Training Loss: 1.9471057521907353, Training Accuracy: 52.23888888888889\n",
            "End of Epoch 176, Validation Loss: 1.9363323616638728, Validation Accuracy: 54.1\n",
            "End of Epoch 177, Training Loss: 1.9437285991269306, Training Accuracy: 52.35740740740741\n",
            "End of Epoch 177, Validation Loss: 1.932850799983695, Validation Accuracy: 54.266666666666666\n",
            "End of Epoch 178, Training Loss: 1.9403440163391303, Training Accuracy: 52.51296296296296\n",
            "End of Epoch 178, Validation Loss: 1.9293614728302324, Validation Accuracy: 54.400000000000006\n",
            "End of Epoch 179, Training Loss: 1.936952283225887, Training Accuracy: 52.653703703703705\n",
            "End of Epoch 179, Validation Loss: 1.9258646649984745, Validation Accuracy: 54.53333333333333\n",
            "End of Epoch 180, Training Loss: 1.9335536786679057, Training Accuracy: 52.7537037037037\n",
            "End of Epoch 180, Validation Loss: 1.9223606608313373, Validation Accuracy: 54.61666666666667\n",
            "End of Epoch 181, Training Loss: 1.9301484809132357, Training Accuracy: 52.912962962962965\n",
            "End of Epoch 181, Validation Loss: 1.918849744103695, Validation Accuracy: 54.733333333333334\n",
            "End of Epoch 182, Training Loss: 1.9267369674645876, Training Accuracy: 53.02037037037037\n",
            "End of Epoch 182, Validation Loss: 1.9153321979084874, Validation Accuracy: 54.766666666666666\n",
            "End of Epoch 183, Training Loss: 1.9233194149688875, Training Accuracy: 53.153703703703705\n",
            "End of Epoch 183, Validation Loss: 1.9118083045449852, Validation Accuracy: 54.93333333333334\n",
            "End of Epoch 184, Training Loss: 1.9198960991091256, Training Accuracy: 53.26851851851851\n",
            "End of Epoch 184, Validation Loss: 1.9082783454092713, Validation Accuracy: 55.016666666666666\n",
            "End of Epoch 185, Training Loss: 1.916467294498552, Training Accuracy: 53.37777777777778\n",
            "End of Epoch 185, Validation Loss: 1.904742600887026, Validation Accuracy: 55.13333333333333\n",
            "End of Epoch 186, Training Loss: 1.9130332745773146, Training Accuracy: 53.49814814814815\n",
            "End of Epoch 186, Validation Loss: 1.9012013502486647, Validation Accuracy: 55.233333333333334\n",
            "End of Epoch 187, Training Loss: 1.909594311511568, Training Accuracy: 53.60370370370371\n",
            "End of Epoch 187, Validation Loss: 1.8976548715468964, Validation Accuracy: 55.31666666666667\n",
            "End of Epoch 188, Training Loss: 1.9061506760951341, Training Accuracy: 53.72222222222223\n",
            "End of Epoch 188, Validation Loss: 1.8941034415167555, Validation Accuracy: 55.55\n",
            "End of Epoch 189, Training Loss: 1.9027026376537364, Training Accuracy: 53.84444444444444\n",
            "End of Epoch 189, Validation Loss: 1.8905473354781546, Validation Accuracy: 55.666666666666664\n",
            "End of Epoch 190, Training Loss: 1.8992504639518735, Training Accuracy: 53.93888888888889\n",
            "End of Epoch 190, Validation Loss: 1.8869868272410122, Validation Accuracy: 55.833333333333336\n",
            "End of Epoch 191, Training Loss: 1.8957944211023698, Training Accuracy: 54.08888888888889\n",
            "End of Epoch 191, Validation Loss: 1.883422189012989, Validation Accuracy: 56.05\n",
            "End of Epoch 192, Training Loss: 1.892334773478614, Training Accuracy: 54.22222222222223\n",
            "End of Epoch 192, Validation Loss: 1.8798536913098753, Validation Accuracy: 56.18333333333333\n",
            "End of Epoch 193, Training Loss: 1.8888717836295608, Training Accuracy: 54.36296296296297\n",
            "End of Epoch 193, Validation Loss: 1.876281602868659, Validation Accuracy: 56.18333333333333\n",
            "End of Epoch 194, Training Loss: 1.8854057121974728, Training Accuracy: 54.47037037037037\n",
            "End of Epoch 194, Validation Loss: 1.8727061905633042, Validation Accuracy: 56.38333333333333\n",
            "End of Epoch 195, Training Loss: 1.8819368178384601, Training Accuracy: 54.57777777777778\n",
            "End of Epoch 195, Validation Loss: 1.869127719323267, Validation Accuracy: 56.49999999999999\n",
            "End of Epoch 196, Training Loss: 1.878465357145819, Training Accuracy: 54.71296296296296\n",
            "End of Epoch 196, Validation Loss: 1.8655464520547609, Validation Accuracy: 56.616666666666674\n",
            "End of Epoch 197, Training Loss: 1.8749915845761873, Training Accuracy: 54.80555555555555\n",
            "End of Epoch 197, Validation Loss: 1.8619626495647974, Validation Accuracy: 56.8\n",
            "End of Epoch 198, Training Loss: 1.8715157523785126, Training Accuracy: 54.90925925925926\n",
            "End of Epoch 198, Validation Loss: 1.8583765704880058, Validation Accuracy: 56.91666666666667\n",
            "End of Epoch 199, Training Loss: 1.86803811052587, Training Accuracy: 55.016666666666666\n",
            "End of Epoch 199, Validation Loss: 1.8547884712162421, Validation Accuracy: 57.05\n",
            "End of Epoch 200, Training Loss: 1.8645589066500858, Training Accuracy: 55.148148148148145\n",
            "End of Epoch 200, Validation Loss: 1.8511986058309944, Validation Accuracy: 57.18333333333333\n",
            "End of Epoch 201, Training Loss: 1.8610783859792015, Training Accuracy: 55.25370370370371\n",
            "End of Epoch 201, Validation Loss: 1.847607226038574, Validation Accuracy: 57.31666666666667\n",
            "End of Epoch 202, Training Loss: 1.8575967912777571, Training Accuracy: 55.35740740740741\n",
            "End of Epoch 202, Validation Loss: 1.8440145811081106, Validation Accuracy: 57.45\n",
            "End of Epoch 203, Training Loss: 1.8541143627898733, Training Accuracy: 55.45\n",
            "End of Epoch 203, Validation Loss: 1.8404209178123079, Validation Accuracy: 57.616666666666674\n",
            "End of Epoch 204, Training Loss: 1.8506313381851414, Training Accuracy: 55.56481481481481\n",
            "End of Epoch 204, Validation Loss: 1.8368264803709953, Validation Accuracy: 57.63333333333334\n",
            "End of Epoch 205, Training Loss: 1.8471479525072758, Training Accuracy: 55.68518518518518\n",
            "End of Epoch 205, Validation Loss: 1.8332315103974166, Validation Accuracy: 57.8\n",
            "End of Epoch 206, Training Loss: 1.8436644381255491, Training Accuracy: 55.80555555555555\n",
            "End of Epoch 206, Validation Loss: 1.8296362468472727, Validation Accuracy: 57.81666666666667\n",
            "End of Epoch 207, Training Loss: 1.8401810246889296, Training Accuracy: 55.96296296296296\n",
            "End of Epoch 207, Validation Loss: 1.826040925970475, Validation Accuracy: 57.983333333333334\n",
            "End of Epoch 208, Training Loss: 1.8366979390829545, Training Accuracy: 56.09074074074074\n",
            "End of Epoch 208, Validation Loss: 1.8224457812656036, Validation Accuracy: 58.15\n",
            "End of Epoch 209, Training Loss: 1.83321540538927, Training Accuracy: 56.20185185185185\n",
            "End of Epoch 209, Validation Loss: 1.8188510434370306, Validation Accuracy: 58.36666666666667\n",
            "End of Epoch 210, Training Loss: 1.8297336448478152, Training Accuracy: 56.30925925925926\n",
            "End of Epoch 210, Validation Loss: 1.815256940354685, Validation Accuracy: 58.45\n",
            "End of Epoch 211, Training Loss: 1.8262528758216325, Training Accuracy: 56.3962962962963\n",
            "End of Epoch 211, Validation Loss: 1.8116636970164333, Validation Accuracy: 58.46666666666667\n",
            "End of Epoch 212, Training Loss: 1.8227733137642412, Training Accuracy: 56.512962962962966\n",
            "End of Epoch 212, Validation Loss: 1.8080715355130337, Validation Accuracy: 58.650000000000006\n",
            "End of Epoch 213, Training Loss: 1.8192951711895624, Training Accuracy: 56.5962962962963\n",
            "End of Epoch 213, Validation Loss: 1.8044806749956273, Validation Accuracy: 58.78333333333333\n",
            "End of Epoch 214, Training Loss: 1.8158186576443247, Training Accuracy: 56.70740740740741\n",
            "End of Epoch 214, Validation Loss: 1.8008913316457396, Validation Accuracy: 58.9\n",
            "End of Epoch 215, Training Loss: 1.812343979682946, Training Accuracy: 56.79814814814815\n",
            "End of Epoch 215, Validation Loss: 1.7973037186477405, Validation Accuracy: 59.0\n",
            "End of Epoch 216, Training Loss: 1.808871340844801, Training Accuracy: 56.940740740740736\n",
            "End of Epoch 216, Validation Loss: 1.7937180461637208, Validation Accuracy: 59.099999999999994\n",
            "End of Epoch 217, Training Loss: 1.805400941633877, Training Accuracy: 57.03518518518519\n",
            "End of Epoch 217, Validation Loss: 1.7901345213107525, Validation Accuracy: 59.25\n",
            "End of Epoch 218, Training Loss: 1.8019329795007242, Training Accuracy: 57.12222222222222\n",
            "End of Epoch 218, Validation Loss: 1.7865533481404716, Validation Accuracy: 59.36666666666667\n",
            "End of Epoch 219, Training Loss: 1.7984676488266824, Training Accuracy: 57.23703703703704\n",
            "End of Epoch 219, Validation Loss: 1.7829747276209507, Validation Accuracy: 59.416666666666664\n",
            "End of Epoch 220, Training Loss: 1.7950051409103225, Training Accuracy: 57.33518518518519\n",
            "End of Epoch 220, Validation Loss: 1.779398857620805, Validation Accuracy: 59.56666666666667\n",
            "End of Epoch 221, Training Loss: 1.791545643956056, Training Accuracy: 57.43888888888888\n",
            "End of Epoch 221, Validation Loss: 1.775825932895477, Validation Accuracy: 59.8\n",
            "End of Epoch 222, Training Loss: 1.7880893430648392, Training Accuracy: 57.53333333333334\n",
            "End of Epoch 222, Validation Loss: 1.7722561450756669, Validation Accuracy: 59.85\n",
            "End of Epoch 223, Training Loss: 1.7846364202269536, Training Accuracy: 57.63148148148149\n",
            "End of Epoch 223, Validation Loss: 1.768689682657834, Validation Accuracy: 60.03333333333334\n",
            "End of Epoch 224, Training Loss: 1.7811870543167752, Training Accuracy: 57.74444444444444\n",
            "End of Epoch 224, Validation Loss: 1.7651267309967331, Validation Accuracy: 60.13333333333334\n",
            "End of Epoch 225, Training Loss: 1.7777414210894966, Training Accuracy: 57.85\n",
            "End of Epoch 225, Validation Loss: 1.7615674722999244, Validation Accuracy: 60.25\n",
            "End of Epoch 226, Training Loss: 1.7742996931797357, Training Accuracy: 57.959259259259255\n",
            "End of Epoch 226, Validation Loss: 1.7580120856242083, Validation Accuracy: 60.5\n",
            "End of Epoch 227, Training Loss: 1.7708620401019768, Training Accuracy: 58.07222222222222\n",
            "End of Epoch 227, Validation Loss: 1.754460746873916, Validation Accuracy: 60.633333333333326\n",
            "End of Epoch 228, Training Loss: 1.7674286282527962, Training Accuracy: 58.19074074074074\n",
            "End of Epoch 228, Validation Loss: 1.7509136288010225, Validation Accuracy: 60.68333333333334\n",
            "End of Epoch 229, Training Loss: 1.7639996209147966, Training Accuracy: 58.333333333333336\n",
            "End of Epoch 229, Validation Loss: 1.7473709010070058, Validation Accuracy: 60.83333333333333\n",
            "End of Epoch 230, Training Loss: 1.7605751782622139, Training Accuracy: 58.47407407407408\n",
            "End of Epoch 230, Validation Loss: 1.7438327299464094, Validation Accuracy: 61.016666666666666\n",
            "End of Epoch 231, Training Loss: 1.757155457368116, Training Accuracy: 58.577777777777776\n",
            "End of Epoch 231, Validation Loss: 1.7402992789320433, Validation Accuracy: 61.16666666666667\n",
            "End of Epoch 232, Training Loss: 1.7537406122131622, Training Accuracy: 58.67407407407408\n",
            "End of Epoch 232, Validation Loss: 1.7367707081417743, Validation Accuracy: 61.31666666666666\n",
            "End of Epoch 233, Training Loss: 1.75033079369584, Training Accuracy: 58.77592592592593\n",
            "End of Epoch 233, Validation Loss: 1.7332471746268399, Validation Accuracy: 61.46666666666667\n",
            "End of Epoch 234, Training Loss: 1.7469261496441397, Training Accuracy: 58.90925925925926\n",
            "End of Epoch 234, Validation Loss: 1.7297288323216462, Validation Accuracy: 61.6\n",
            "End of Epoch 235, Training Loss: 1.7435268248286038, Training Accuracy: 59.00555555555555\n",
            "End of Epoch 235, Validation Loss: 1.7262158320549712, Validation Accuracy: 61.7\n",
            "End of Epoch 236, Training Loss: 1.7401329609766936, Training Accuracy: 59.105555555555554\n",
            "End of Epoch 236, Validation Loss: 1.7227083215625358, Validation Accuracy: 61.78333333333333\n",
            "End of Epoch 237, Training Loss: 1.736744696788417, Training Accuracy: 59.19814814814814\n",
            "End of Epoch 237, Validation Loss: 1.7192064455008829, Validation Accuracy: 61.93333333333333\n",
            "End of Epoch 238, Training Loss: 1.7333621679531637, Training Accuracy: 59.29629629629629\n",
            "End of Epoch 238, Validation Loss: 1.7157103454625025, Validation Accuracy: 62.03333333333333\n",
            "End of Epoch 239, Training Loss: 1.7299855071676922, Training Accuracy: 59.398148148148145\n",
            "End of Epoch 239, Validation Loss: 1.7122201599921563, Validation Accuracy: 62.133333333333326\n",
            "End of Epoch 240, Training Loss: 1.7266148441552152, Training Accuracy: 59.483333333333334\n",
            "End of Epoch 240, Validation Loss: 1.7087360246043484, Validation Accuracy: 62.21666666666666\n",
            "End of Epoch 241, Training Loss: 1.7232503056855206, Training Accuracy: 59.59814814814814\n",
            "End of Epoch 241, Validation Loss: 1.7052580718018804, Validation Accuracy: 62.28333333333333\n",
            "End of Epoch 242, Training Loss: 1.7198920155960953, Training Accuracy: 59.699999999999996\n",
            "End of Epoch 242, Validation Loss: 1.7017864310954482, Validation Accuracy: 62.36666666666667\n",
            "End of Epoch 243, Training Loss: 1.7165400948141676, Training Accuracy: 59.798148148148144\n",
            "End of Epoch 243, Validation Loss: 1.698321229024222, Validation Accuracy: 62.416666666666664\n",
            "End of Epoch 244, Training Loss: 1.7131946613796623, Training Accuracy: 59.8925925925926\n",
            "End of Epoch 244, Validation Loss: 1.6948625891773657, Validation Accuracy: 62.6\n",
            "End of Epoch 245, Training Loss: 1.7098558304689664, Training Accuracy: 59.99444444444444\n",
            "End of Epoch 245, Validation Loss: 1.691410632216434, Validation Accuracy: 62.71666666666667\n",
            "End of Epoch 246, Training Loss: 1.706523714419499, Training Accuracy: 60.06111111111111\n",
            "End of Epoch 246, Validation Loss: 1.687965475898614, Validation Accuracy: 62.93333333333333\n",
            "End of Epoch 247, Training Loss: 1.7031984227550105, Training Accuracy: 60.15185185185186\n",
            "End of Epoch 247, Validation Loss: 1.6845272351007479, Validation Accuracy: 63.13333333333333\n",
            "End of Epoch 248, Training Loss: 1.6998800622115768, Training Accuracy: 60.23333333333334\n",
            "End of Epoch 248, Validation Loss: 1.6810960218440993, Validation Accuracy: 63.24999999999999\n",
            "End of Epoch 249, Training Loss: 1.6965687367642401, Training Accuracy: 60.351851851851855\n",
            "End of Epoch 249, Validation Loss: 1.677671945319815, Validation Accuracy: 63.31666666666666\n",
            "End of Epoch 250, Training Loss: 1.6932645476542418, Training Accuracy: 60.4574074074074\n",
            "End of Epoch 250, Validation Loss: 1.6742551119150355, Validation Accuracy: 63.416666666666664\n",
            "End of Epoch 251, Training Loss: 1.6899675934168221, Training Accuracy: 60.56111111111111\n",
            "End of Epoch 251, Validation Loss: 1.6708456252396096, Validation Accuracy: 63.483333333333334\n",
            "End of Epoch 252, Training Loss: 1.6866779699095247, Training Accuracy: 60.6537037037037\n",
            "End of Epoch 252, Validation Loss: 1.6674435861533745, Validation Accuracy: 63.6\n",
            "End of Epoch 253, Training Loss: 1.6833957703409757, Training Accuracy: 60.7537037037037\n",
            "End of Epoch 253, Validation Loss: 1.6640490927939584, Validation Accuracy: 63.71666666666667\n",
            "End of Epoch 254, Training Loss: 1.6801210853000939, Training Accuracy: 60.870370370370374\n",
            "End of Epoch 254, Validation Loss: 1.6606622406050573, Validation Accuracy: 63.9\n",
            "End of Epoch 255, Training Loss: 1.676854002785693, Training Accuracy: 60.97407407407407\n",
            "End of Epoch 255, Validation Loss: 1.657283122365164, Validation Accuracy: 63.96666666666667\n",
            "End of Epoch 256, Training Loss: 1.673594608236437, Training Accuracy: 61.06666666666667\n",
            "End of Epoch 256, Validation Loss: 1.6539118282166876, Validation Accuracy: 64.08333333333334\n",
            "End of Epoch 257, Training Loss: 1.6703429845611117, Training Accuracy: 61.181481481481484\n",
            "End of Epoch 257, Validation Loss: 1.6505484456954527, Validation Accuracy: 64.18333333333334\n",
            "End of Epoch 258, Training Loss: 1.6670992121691826, Training Accuracy: 61.30185185185185\n",
            "End of Epoch 258, Validation Loss: 1.6471930597605176, Validation Accuracy: 64.35\n",
            "End of Epoch 259, Training Loss: 1.6638633690015974, Training Accuracy: 61.385185185185186\n",
            "End of Epoch 259, Validation Loss: 1.6438457528242971, Validation Accuracy: 64.43333333333334\n",
            "End of Epoch 260, Training Loss: 1.6606355305617984, Training Accuracy: 61.46666666666667\n",
            "End of Epoch 260, Validation Loss: 1.640506604782945, Validation Accuracy: 64.53333333333333\n",
            "End of Epoch 261, Training Loss: 1.6574157699469276, Training Accuracy: 61.57962962962963\n",
            "End of Epoch 261, Validation Loss: 1.637175693046965, Validation Accuracy: 64.68333333333334\n",
            "End of Epoch 262, Training Loss: 1.6542041578791802, Training Accuracy: 61.675925925925924\n",
            "End of Epoch 262, Validation Loss: 1.6338530925720252, Validation Accuracy: 64.8\n",
            "End of Epoch 263, Training Loss: 1.6510007627372727, Training Accuracy: 61.76851851851852\n",
            "End of Epoch 263, Validation Loss: 1.630538875889941, Validation Accuracy: 64.88333333333334\n",
            "End of Epoch 264, Training Loss: 1.6478056505880225, Training Accuracy: 61.84074074074074\n",
            "End of Epoch 264, Validation Loss: 1.6272331131398, Validation Accuracy: 64.95\n",
            "End of Epoch 265, Training Loss: 1.64461888521798, Training Accuracy: 61.93703703703704\n",
            "End of Epoch 265, Validation Loss: 1.6239358720991983, Validation Accuracy: 65.10000000000001\n",
            "End of Epoch 266, Training Loss: 1.6414405281651132, Training Accuracy: 62.01851851851852\n",
            "End of Epoch 266, Validation Loss: 1.6206472182155693, Validation Accuracy: 65.21666666666667\n",
            "End of Epoch 267, Training Loss: 1.638270638750505, Training Accuracy: 62.10555555555556\n",
            "End of Epoch 267, Validation Loss: 1.617367214637573, Validation Accuracy: 65.31666666666666\n",
            "End of Epoch 268, Training Loss: 1.635109274110052, Training Accuracy: 62.237037037037034\n",
            "End of Epoch 268, Validation Loss: 1.6140959222465217, Validation Accuracy: 65.38333333333334\n",
            "End of Epoch 269, Training Loss: 1.6319564892261258, Training Accuracy: 62.324074074074076\n",
            "End of Epoch 269, Validation Loss: 1.6108333996878264, Validation Accuracy: 65.5\n",
            "End of Epoch 270, Training Loss: 1.6288123369591938, Training Accuracy: 62.43703703703704\n",
            "End of Epoch 270, Validation Loss: 1.6075797034024335, Validation Accuracy: 65.53333333333333\n",
            "End of Epoch 271, Training Loss: 1.6256768680793687, Training Accuracy: 62.51296296296296\n",
            "End of Epoch 271, Validation Loss: 1.6043348876582393, Validation Accuracy: 65.63333333333333\n",
            "End of Epoch 272, Training Loss: 1.6225501312978692, Training Accuracy: 62.62222222222222\n",
            "End of Epoch 272, Validation Loss: 1.6010990045814522, Validation Accuracy: 65.83333333333333\n",
            "End of Epoch 273, Training Loss: 1.619432173298382, Training Accuracy: 62.71296296296296\n",
            "End of Epoch 273, Validation Loss: 1.5978721041878987, Validation Accuracy: 66.0\n",
            "End of Epoch 274, Training Loss: 1.616323038768292, Training Accuracy: 62.80555555555556\n",
            "End of Epoch 274, Validation Loss: 1.594654234414238, Validation Accuracy: 66.05\n",
            "End of Epoch 275, Training Loss: 1.6132227704297808, Training Accuracy: 62.903703703703705\n",
            "End of Epoch 275, Validation Loss: 1.5914454411490873, Validation Accuracy: 66.28333333333333\n",
            "End of Epoch 276, Training Loss: 1.6101314090707752, Training Accuracy: 63.0\n",
            "End of Epoch 276, Validation Loss: 1.5882457682640239, Validation Accuracy: 66.4\n",
            "End of Epoch 277, Training Loss: 1.6070489935757182, Training Accuracy: 63.08148148148148\n",
            "End of Epoch 277, Validation Loss: 1.585055257644465, Validation Accuracy: 66.51666666666667\n",
            "End of Epoch 278, Training Loss: 1.6039755609561743, Training Accuracy: 63.14814814814815\n",
            "End of Epoch 278, Validation Loss: 1.581873949220402, Validation Accuracy: 66.66666666666666\n",
            "End of Epoch 279, Training Loss: 1.600911146381234, Training Accuracy: 63.242592592592594\n",
            "End of Epoch 279, Validation Loss: 1.5787018809969764, Validation Accuracy: 66.83333333333333\n",
            "End of Epoch 280, Training Loss: 1.597855783207717, Training Accuracy: 63.33888888888889\n",
            "End of Epoch 280, Validation Loss: 1.575539089084895, Validation Accuracy: 66.86666666666666\n",
            "End of Epoch 281, Training Loss: 1.5948095030101652, Training Accuracy: 63.422222222222224\n",
            "End of Epoch 281, Validation Loss: 1.5723856077306573, Validation Accuracy: 66.93333333333334\n",
            "End of Epoch 282, Training Loss: 1.5917723356106095, Training Accuracy: 63.50370370370371\n",
            "End of Epoch 282, Validation Loss: 1.5692414693465992, Validation Accuracy: 67.03333333333333\n",
            "End of Epoch 283, Training Loss: 1.5887443091081064, Training Accuracy: 63.58518518518519\n",
            "End of Epoch 283, Validation Loss: 1.566106704540737, Validation Accuracy: 67.03333333333333\n",
            "End of Epoch 284, Training Loss: 1.585725449908039, Training Accuracy: 63.651851851851845\n",
            "End of Epoch 284, Validation Loss: 1.5629813421464016, Validation Accuracy: 67.2\n",
            "End of Epoch 285, Training Loss: 1.5827157827511622, Training Accuracy: 63.74074074074074\n",
            "End of Epoch 285, Validation Loss: 1.5598654092516566, Validation Accuracy: 67.33333333333333\n",
            "End of Epoch 286, Training Loss: 1.5797153307424006, Training Accuracy: 63.82592592592593\n",
            "End of Epoch 286, Validation Loss: 1.5567589312284926, Validation Accuracy: 67.4\n",
            "End of Epoch 287, Training Loss: 1.5767241153793878, Training Accuracy: 63.90555555555556\n",
            "End of Epoch 287, Validation Loss: 1.5536619317617923, Validation Accuracy: 67.56666666666666\n",
            "End of Epoch 288, Training Loss: 1.573742156580735, Training Accuracy: 63.98888888888889\n",
            "End of Epoch 288, Validation Loss: 1.5505744328780575, Validation Accuracy: 67.63333333333334\n",
            "End of Epoch 289, Training Loss: 1.5707694727140395, Training Accuracy: 64.09814814814816\n",
            "End of Epoch 289, Validation Loss: 1.547496454973892, Validation Accuracy: 67.71666666666667\n",
            "End of Epoch 290, Training Loss: 1.5678060806236078, Training Accuracy: 64.18148148148148\n",
            "End of Epoch 290, Validation Loss: 1.5444280168442401, Validation Accuracy: 67.83333333333333\n",
            "End of Epoch 291, Training Loss: 1.5648519956579074, Training Accuracy: 64.26111111111112\n",
            "End of Epoch 291, Validation Loss: 1.54136913571037, Validation Accuracy: 67.96666666666667\n",
            "End of Epoch 292, Training Loss: 1.561907231696744, Training Accuracy: 64.31666666666666\n",
            "End of Epoch 292, Validation Loss: 1.5383198272476033, Validation Accuracy: 67.98333333333333\n",
            "End of Epoch 293, Training Loss: 1.55897180117814, Training Accuracy: 64.40370370370371\n",
            "End of Epoch 293, Validation Loss: 1.5352801056127823, Validation Accuracy: 68.13333333333334\n",
            "End of Epoch 294, Training Loss: 1.5560457151249456, Training Accuracy: 64.4962962962963\n",
            "End of Epoch 294, Validation Loss: 1.5322499834714776, Validation Accuracy: 68.31666666666666\n",
            "End of Epoch 295, Training Loss: 1.5531289831711503, Training Accuracy: 64.56666666666668\n",
            "End of Epoch 295, Validation Loss: 1.5292294720249295, Validation Accuracy: 68.38333333333333\n",
            "End of Epoch 296, Training Loss: 1.550221613587911, Training Accuracy: 64.62777777777777\n",
            "End of Epoch 296, Validation Loss: 1.5262185810367213, Validation Accuracy: 68.48333333333333\n",
            "End of Epoch 297, Training Loss: 1.5473236133092938, Training Accuracy: 64.71111111111111\n",
            "End of Epoch 297, Validation Loss: 1.5232173188591829, Validation Accuracy: 68.60000000000001\n",
            "End of Epoch 298, Training Loss: 1.5444349879577168, Training Accuracy: 64.79259259259258\n",
            "End of Epoch 298, Validation Loss: 1.5202256924595263, Validation Accuracy: 68.7\n",
            "End of Epoch 299, Training Loss: 1.5415557418691197, Training Accuracy: 64.87222222222222\n",
            "End of Epoch 299, Validation Loss: 1.5172437074457108, Validation Accuracy: 68.71666666666667\n",
            "End of Epoch 300, Training Loss: 1.538685878117823, Training Accuracy: 64.96666666666667\n",
            "End of Epoch 300, Validation Loss: 1.5142713680920306, Validation Accuracy: 68.86666666666666\n",
            "End of Epoch 301, Training Loss: 1.5358253985411205, Training Accuracy: 65.04814814814814\n",
            "End of Epoch 301, Validation Loss: 1.511308677364435, Validation Accuracy: 68.91666666666667\n",
            "End of Epoch 302, Training Loss: 1.5329743037635644, Training Accuracy: 65.12037037037037\n",
            "End of Epoch 302, Validation Loss: 1.5083556369455773, Validation Accuracy: 68.91666666666667\n",
            "End of Epoch 303, Training Loss: 1.530132593220974, Training Accuracy: 65.23148148148148\n",
            "End of Epoch 303, Validation Loss: 1.5054122472595848, Validation Accuracy: 69.0\n",
            "End of Epoch 304, Training Loss: 1.527300265184159, Training Accuracy: 65.29629629629629\n",
            "End of Epoch 304, Validation Loss: 1.5024785074965656, Validation Accuracy: 69.06666666666666\n",
            "End of Epoch 305, Training Loss: 1.5244773167823518, Training Accuracy: 65.37037037037037\n",
            "End of Epoch 305, Validation Loss: 1.4995544156368392, Validation Accuracy: 69.13333333333334\n",
            "End of Epoch 306, Training Loss: 1.5216637440263636, Training Accuracy: 65.45185185185186\n",
            "End of Epoch 306, Validation Loss: 1.4966399684748968, Validation Accuracy: 69.28333333333333\n",
            "End of Epoch 307, Training Loss: 1.5188595418314588, Training Accuracy: 65.54074074074074\n",
            "End of Epoch 307, Validation Loss: 1.493735161643097, Validation Accuracy: 69.38333333333333\n",
            "End of Epoch 308, Training Loss: 1.5160647040399469, Training Accuracy: 65.66296296296296\n",
            "End of Epoch 308, Validation Loss: 1.4908399896350906, Validation Accuracy: 69.45\n",
            "End of Epoch 309, Training Loss: 1.513279223443498, Training Accuracy: 65.72592592592592\n",
            "End of Epoch 309, Validation Loss: 1.4879544458289793, Validation Accuracy: 69.53333333333333\n",
            "End of Epoch 310, Training Loss: 1.5105030918051963, Training Accuracy: 65.8\n",
            "End of Epoch 310, Validation Loss: 1.4850785225102148, Validation Accuracy: 69.6\n",
            "End of Epoch 311, Training Loss: 1.507736299881304, Training Accuracy: 65.87407407407407\n",
            "End of Epoch 311, Validation Loss: 1.4822122108942266, Validation Accuracy: 69.65\n",
            "End of Epoch 312, Training Loss: 1.5049788374427686, Training Accuracy: 65.95370370370371\n",
            "End of Epoch 312, Validation Loss: 1.4793555011488, Validation Accuracy: 69.73333333333333\n",
            "End of Epoch 313, Training Loss: 1.5022306932964629, Training Accuracy: 66.0074074074074\n",
            "End of Epoch 313, Validation Loss: 1.476508382416186, Validation Accuracy: 69.76666666666667\n",
            "End of Epoch 314, Training Loss: 1.4994918553061567, Training Accuracy: 66.08518518518518\n",
            "End of Epoch 314, Validation Loss: 1.4736708428349614, Validation Accuracy: 69.8\n",
            "End of Epoch 315, Training Loss: 1.4967623104132342, Training Accuracy: 66.16296296296296\n",
            "End of Epoch 315, Validation Loss: 1.4708428695616318, Validation Accuracy: 69.89999999999999\n",
            "End of Epoch 316, Training Loss: 1.4940420446571465, Training Accuracy: 66.2388888888889\n",
            "End of Epoch 316, Validation Loss: 1.468024448791983, Validation Accuracy: 69.93333333333334\n",
            "End of Epoch 317, Training Loss: 1.4913310431956186, Training Accuracy: 66.31666666666666\n",
            "End of Epoch 317, Validation Loss: 1.4652155657821846, Validation Accuracy: 70.0\n",
            "End of Epoch 318, Training Loss: 1.4886292903245966, Training Accuracy: 66.39259259259259\n",
            "End of Epoch 318, Validation Loss: 1.4624162048696454, Validation Accuracy: 70.1\n",
            "End of Epoch 319, Training Loss: 1.4859367694979568, Training Accuracy: 66.45555555555556\n",
            "End of Epoch 319, Validation Loss: 1.4596263494936217, Validation Accuracy: 70.1\n",
            "End of Epoch 320, Training Loss: 1.4832534633469547, Training Accuracy: 66.55\n",
            "End of Epoch 320, Validation Loss: 1.456845982215592, Validation Accuracy: 70.23333333333333\n",
            "End of Epoch 321, Training Loss: 1.480579353699447, Training Accuracy: 66.61481481481482\n",
            "End of Epoch 321, Validation Loss: 1.4540750847393804, Validation Accuracy: 70.36666666666666\n",
            "End of Epoch 322, Training Loss: 1.4779144215988638, Training Accuracy: 66.68888888888888\n",
            "End of Epoch 322, Validation Loss: 1.4513136379310543, Validation Accuracy: 70.41666666666667\n",
            "End of Epoch 323, Training Loss: 1.4752586473229528, Training Accuracy: 66.78888888888889\n",
            "End of Epoch 323, Validation Loss: 1.4485616218385822, Validation Accuracy: 70.43333333333334\n",
            "End of Epoch 324, Training Loss: 1.472612010402285, Training Accuracy: 66.87037037037037\n",
            "End of Epoch 324, Validation Loss: 1.4458190157112594, Validation Accuracy: 70.5\n",
            "End of Epoch 325, Training Loss: 1.4699744896385332, Training Accuracy: 66.94814814814815\n",
            "End of Epoch 325, Validation Loss: 1.4430857980189078, Validation Accuracy: 70.58333333333333\n",
            "End of Epoch 326, Training Loss: 1.4673460631225215, Training Accuracy: 67.03518518518518\n",
            "End of Epoch 326, Validation Loss: 1.440361946470848, Validation Accuracy: 70.63333333333334\n",
            "End of Epoch 327, Training Loss: 1.464726708252059, Training Accuracy: 67.10000000000001\n",
            "End of Epoch 327, Validation Loss: 1.4376474380346462, Validation Accuracy: 70.7\n",
            "End of Epoch 328, Training Loss: 1.4621164017495416, Training Accuracy: 67.15\n",
            "End of Epoch 328, Validation Loss: 1.4349422489546442, Validation Accuracy: 70.75\n",
            "End of Epoch 329, Training Loss: 1.459515119679347, Training Accuracy: 67.2037037037037\n",
            "End of Epoch 329, Validation Loss: 1.4322463547702653, Validation Accuracy: 70.81666666666668\n",
            "End of Epoch 330, Training Loss: 1.4569228374650036, Training Accuracy: 67.27407407407408\n",
            "End of Epoch 330, Validation Loss: 1.4295597303341077, Validation Accuracy: 70.86666666666666\n",
            "End of Epoch 331, Training Loss: 1.4543395299061654, Training Accuracy: 67.31851851851852\n",
            "End of Epoch 331, Validation Loss: 1.4268823498298253, Validation Accuracy: 70.89999999999999\n",
            "End of Epoch 332, Training Loss: 1.4517651711953548, Training Accuracy: 67.38148148148149\n",
            "End of Epoch 332, Validation Loss: 1.4242141867897953, Validation Accuracy: 71.01666666666667\n",
            "End of Epoch 333, Training Loss: 1.4491997349345214, Training Accuracy: 67.44814814814815\n",
            "End of Epoch 333, Validation Loss: 1.4215552141125771, Validation Accuracy: 71.06666666666666\n",
            "End of Epoch 334, Training Loss: 1.4466431941513869, Training Accuracy: 67.52777777777777\n",
            "End of Epoch 334, Validation Loss: 1.4189054040801685, Validation Accuracy: 71.1\n",
            "End of Epoch 335, Training Loss: 1.4440955213155873, Training Accuracy: 67.60185185185185\n",
            "End of Epoch 335, Validation Loss: 1.4162647283750527, Validation Accuracy: 71.18333333333334\n",
            "End of Epoch 336, Training Loss: 1.4415566883546262, Training Accuracy: 67.68888888888888\n",
            "End of Epoch 336, Validation Loss: 1.4136331580970518, Validation Accuracy: 71.25\n",
            "End of Epoch 337, Training Loss: 1.439026666669628, Training Accuracy: 67.75925925925927\n",
            "End of Epoch 337, Validation Loss: 1.4110106637799726, Validation Accuracy: 71.3\n",
            "End of Epoch 338, Training Loss: 1.4365054271508915, Training Accuracy: 67.83518518518518\n",
            "End of Epoch 338, Validation Loss: 1.408397215408064, Validation Accuracy: 71.39999999999999\n",
            "End of Epoch 339, Training Loss: 1.4339929401932745, Training Accuracy: 67.92222222222222\n",
            "End of Epoch 339, Validation Loss: 1.405792782432273, Validation Accuracy: 71.46666666666667\n",
            "End of Epoch 340, Training Loss: 1.4314891757113664, Training Accuracy: 67.98148148148148\n",
            "End of Epoch 340, Validation Loss: 1.4031973337863153, Validation Accuracy: 71.53333333333333\n",
            "End of Epoch 341, Training Loss: 1.4289941031544895, Training Accuracy: 68.05185185185185\n",
            "End of Epoch 341, Validation Loss: 1.4006108379025484, Validation Accuracy: 71.65\n",
            "End of Epoch 342, Training Loss: 1.4265076915215171, Training Accuracy: 68.13703703703705\n",
            "End of Epoch 342, Validation Loss: 1.398033262727664, Validation Accuracy: 71.75\n",
            "End of Epoch 343, Training Loss: 1.4240299093755087, Training Accuracy: 68.21296296296296\n",
            "End of Epoch 343, Validation Loss: 1.3954645757381898, Validation Accuracy: 71.78333333333333\n",
            "End of Epoch 344, Training Loss: 1.4215607248581656, Training Accuracy: 68.27777777777779\n",
            "End of Epoch 344, Validation Loss: 1.3929047439558095, Validation Accuracy: 71.89999999999999\n",
            "End of Epoch 345, Training Loss: 1.4191001057041104, Training Accuracy: 68.34074074074074\n",
            "End of Epoch 345, Validation Loss: 1.3903537339624992, Validation Accuracy: 71.91666666666666\n",
            "End of Epoch 346, Training Loss: 1.4166480192549986, Training Accuracy: 68.38703703703703\n",
            "End of Epoch 346, Validation Loss: 1.3878115119154861, Validation Accuracy: 71.98333333333333\n",
            "End of Epoch 347, Training Loss: 1.4142044324734473, Training Accuracy: 68.45\n",
            "End of Epoch 347, Validation Loss: 1.385278043562028, Validation Accuracy: 72.08333333333333\n",
            "End of Epoch 348, Training Loss: 1.4117693119568053, Training Accuracy: 68.53148148148148\n",
            "End of Epoch 348, Validation Loss: 1.382753294254016, Validation Accuracy: 72.18333333333334\n",
            "End of Epoch 349, Training Loss: 1.4093426239507474, Training Accuracy: 68.60185185185185\n",
            "End of Epoch 349, Validation Loss: 1.3802372289624036, Validation Accuracy: 72.26666666666667\n",
            "End of Epoch 350, Training Loss: 1.4069243343627058, Training Accuracy: 68.66111111111111\n",
            "End of Epoch 350, Validation Loss: 1.3777298122914623, Validation Accuracy: 72.35000000000001\n",
            "End of Epoch 351, Training Loss: 1.4045144087751344, Training Accuracy: 68.73518518518519\n",
            "End of Epoch 351, Validation Loss: 1.3752310084928665, Validation Accuracy: 72.46666666666667\n",
            "End of Epoch 352, Training Loss: 1.402112812458611, Training Accuracy: 68.80185185185185\n",
            "End of Epoch 352, Validation Loss: 1.3727407814796082, Validation Accuracy: 72.51666666666667\n",
            "End of Epoch 353, Training Loss: 1.3997195103847833, Training Accuracy: 68.8537037037037\n",
            "End of Epoch 353, Validation Loss: 1.3702590948397462, Validation Accuracy: 72.61666666666666\n",
            "End of Epoch 354, Training Loss: 1.397334467239146, Training Accuracy: 68.91666666666667\n",
            "End of Epoch 354, Validation Loss: 1.3677859118499855, Validation Accuracy: 72.65\n",
            "End of Epoch 355, Training Loss: 1.3949576474336678, Training Accuracy: 68.98333333333333\n",
            "End of Epoch 355, Validation Loss: 1.3653211954890931, Validation Accuracy: 72.66666666666667\n",
            "End of Epoch 356, Training Loss: 1.3925890151192601, Training Accuracy: 69.05\n",
            "End of Epoch 356, Validation Loss: 1.3628649084511526, Validation Accuracy: 72.76666666666667\n",
            "End of Epoch 357, Training Loss: 1.3902285341980871, Training Accuracy: 69.10185185185185\n",
            "End of Epoch 357, Validation Loss: 1.360417013158654, Validation Accuracy: 72.81666666666666\n",
            "End of Epoch 358, Training Loss: 1.387876168335727, Training Accuracy: 69.17962962962963\n",
            "End of Epoch 358, Validation Loss: 1.3579774717754225, Validation Accuracy: 72.96666666666667\n",
            "End of Epoch 359, Training Loss: 1.3855318809731743, Training Accuracy: 69.25925925925925\n",
            "End of Epoch 359, Validation Loss: 1.3555462462193903, Validation Accuracy: 73.08333333333333\n",
            "End of Epoch 360, Training Loss: 1.3831956353387025, Training Accuracy: 69.32407407407408\n",
            "End of Epoch 360, Validation Loss: 1.3531232981752104, Validation Accuracy: 73.18333333333334\n",
            "End of Epoch 361, Training Loss: 1.3808673944595637, Training Accuracy: 69.39259259259259\n",
            "End of Epoch 361, Validation Loss: 1.3507085891067092, Validation Accuracy: 73.21666666666667\n",
            "End of Epoch 362, Training Loss: 1.378547121173546, Training Accuracy: 69.46481481481482\n",
            "End of Epoch 362, Validation Loss: 1.3483020802691883, Validation Accuracy: 73.25\n",
            "End of Epoch 363, Training Loss: 1.3762347781403892, Training Accuracy: 69.53703703703704\n",
            "End of Epoch 363, Validation Loss: 1.3459037327215704, Validation Accuracy: 73.26666666666667\n",
            "End of Epoch 364, Training Loss: 1.3739303278530344, Training Accuracy: 69.59074074074074\n",
            "End of Epoch 364, Validation Loss: 1.3435135073383886, Validation Accuracy: 73.35000000000001\n",
            "End of Epoch 365, Training Loss: 1.371633732648757, Training Accuracy: 69.64814814814815\n",
            "End of Epoch 365, Validation Loss: 1.3411313648216294, Validation Accuracy: 73.45\n",
            "End of Epoch 366, Training Loss: 1.3693449547201273, Training Accuracy: 69.68518518518519\n",
            "End of Epoch 366, Validation Loss: 1.3387572657124176, Validation Accuracy: 73.53333333333333\n",
            "End of Epoch 367, Training Loss: 1.3670639561258453, Training Accuracy: 69.74814814814815\n",
            "End of Epoch 367, Validation Loss: 1.336391170402555, Validation Accuracy: 73.55000000000001\n",
            "End of Epoch 368, Training Loss: 1.3647906988014251, Training Accuracy: 69.80185185185185\n",
            "End of Epoch 368, Validation Loss: 1.3340330391459085, Validation Accuracy: 73.63333333333333\n",
            "End of Epoch 369, Training Loss: 1.362525144569741, Training Accuracy: 69.8462962962963\n",
            "End of Epoch 369, Validation Loss: 1.33168283206965, Validation Accuracy: 73.66666666666667\n",
            "End of Epoch 370, Training Loss: 1.3602672551514272, Training Accuracy: 69.89814814814815\n",
            "End of Epoch 370, Validation Loss: 1.3293405091853472, Validation Accuracy: 73.7\n",
            "End of Epoch 371, Training Loss: 1.358016992175145, Training Accuracy: 69.95370370370371\n",
            "End of Epoch 371, Validation Loss: 1.3270060303999083, Validation Accuracy: 73.71666666666667\n",
            "End of Epoch 372, Training Loss: 1.3557743171877044, Training Accuracy: 70.01851851851852\n",
            "End of Epoch 372, Validation Loss: 1.324679355526382, Validation Accuracy: 73.83333333333333\n",
            "End of Epoch 373, Training Loss: 1.3535391916640438, Training Accuracy: 70.07962962962962\n",
            "End of Epoch 373, Validation Loss: 1.3223604442946082, Validation Accuracy: 73.91666666666666\n",
            "End of Epoch 374, Training Loss: 1.3513115770170838, Training Accuracy: 70.12222222222222\n",
            "End of Epoch 374, Validation Loss: 1.320049256361728, Validation Accuracy: 73.98333333333333\n",
            "End of Epoch 375, Training Loss: 1.3490914346074283, Training Accuracy: 70.16851851851852\n",
            "End of Epoch 375, Validation Loss: 1.3177457513225483, Validation Accuracy: 74.0\n",
            "End of Epoch 376, Training Loss: 1.3468787257529384, Training Accuracy: 70.25555555555556\n",
            "End of Epoch 376, Validation Loss: 1.3154498887197623, Validation Accuracy: 74.06666666666666\n",
            "End of Epoch 377, Training Loss: 1.3446734117381627, Training Accuracy: 70.31666666666668\n",
            "End of Epoch 377, Validation Loss: 1.313161628054026, Validation Accuracy: 74.08333333333333\n",
            "End of Epoch 378, Training Loss: 1.3424754538236383, Training Accuracy: 70.36851851851851\n",
            "End of Epoch 378, Validation Loss: 1.310880928793896, Validation Accuracy: 74.13333333333333\n",
            "End of Epoch 379, Training Loss: 1.3402848132550496, Training Accuracy: 70.41296296296296\n",
            "End of Epoch 379, Validation Loss: 1.3086077503856226, Validation Accuracy: 74.21666666666667\n",
            "End of Epoch 380, Training Loss: 1.3381014512722529, Training Accuracy: 70.45555555555556\n",
            "End of Epoch 380, Validation Loss: 1.3063420522628022, Validation Accuracy: 74.23333333333333\n",
            "End of Epoch 381, Training Loss: 1.3359253291181694, Training Accuracy: 70.5037037037037\n",
            "End of Epoch 381, Validation Loss: 1.3040837938558891, Validation Accuracy: 74.36666666666667\n",
            "End of Epoch 382, Training Loss: 1.333756408047539, Training Accuracy: 70.56481481481481\n",
            "End of Epoch 382, Validation Loss: 1.3018329346015671, Validation Accuracy: 74.51666666666667\n",
            "End of Epoch 383, Training Loss: 1.3315946493355406, Training Accuracy: 70.61111111111111\n",
            "End of Epoch 383, Validation Loss: 1.2995894339519802, Validation Accuracy: 74.58333333333333\n",
            "End of Epoch 384, Training Loss: 1.329440014286275, Training Accuracy: 70.6888888888889\n",
            "End of Epoch 384, Validation Loss: 1.2973532513838255, Validation Accuracy: 74.65\n",
            "End of Epoch 385, Training Loss: 1.3272924642411246, Training Accuracy: 70.75555555555556\n",
            "End of Epoch 385, Validation Loss: 1.2951243464073017, Validation Accuracy: 74.78333333333333\n",
            "End of Epoch 386, Training Loss: 1.325151960586962, Training Accuracy: 70.83333333333334\n",
            "End of Epoch 386, Validation Loss: 1.2929026785749265, Validation Accuracy: 74.88333333333334\n",
            "End of Epoch 387, Training Loss: 1.3230184647642376, Training Accuracy: 70.89259259259259\n",
            "End of Epoch 387, Validation Loss: 1.290688207490207, Validation Accuracy: 74.91666666666667\n",
            "End of Epoch 388, Training Loss: 1.3208919382749311, Training Accuracy: 70.95185185185186\n",
            "End of Epoch 388, Validation Loss: 1.2884808928161775, Validation Accuracy: 74.93333333333332\n",
            "End of Epoch 389, Training Loss: 1.3187723426903617, Training Accuracy: 70.99814814814815\n",
            "End of Epoch 389, Validation Loss: 1.2862806942837925, Validation Accuracy: 75.0\n",
            "End of Epoch 390, Training Loss: 1.316659639658876, Training Accuracy: 71.0574074074074\n",
            "End of Epoch 390, Validation Loss: 1.2840875717001892, Validation Accuracy: 75.03333333333333\n",
            "End of Epoch 391, Training Loss: 1.3145537909133902, Training Accuracy: 71.12037037037037\n",
            "End of Epoch 391, Validation Loss: 1.2819014849568036, Validation Accuracy: 75.1\n",
            "End of Epoch 392, Training Loss: 1.312454758278809, Training Accuracy: 71.16666666666667\n",
            "End of Epoch 392, Validation Loss: 1.2797223940373548, Validation Accuracy: 75.2\n",
            "End of Epoch 393, Training Loss: 1.3103625036793034, Training Accuracy: 71.23518518518519\n",
            "End of Epoch 393, Validation Loss: 1.2775502590256864, Validation Accuracy: 75.28333333333333\n",
            "End of Epoch 394, Training Loss: 1.3082769891454564, Training Accuracy: 71.29814814814814\n",
            "End of Epoch 394, Validation Loss: 1.2753850401134732, Validation Accuracy: 75.38333333333334\n",
            "End of Epoch 395, Training Loss: 1.3061981768212787, Training Accuracy: 71.35185185185186\n",
            "End of Epoch 395, Validation Loss: 1.273226697607786, Validation Accuracy: 75.46666666666667\n",
            "End of Epoch 396, Training Loss: 1.3041260289710814, Training Accuracy: 71.41481481481482\n",
            "End of Epoch 396, Validation Loss: 1.271075191938524, Validation Accuracy: 75.51666666666667\n",
            "End of Epoch 397, Training Loss: 1.3020605079862257, Training Accuracy: 71.47407407407408\n",
            "End of Epoch 397, Validation Loss: 1.2689304836657007, Validation Accuracy: 75.58333333333334\n",
            "End of Epoch 398, Training Loss: 1.300001576391727, Training Accuracy: 71.5425925925926\n",
            "End of Epoch 398, Validation Loss: 1.2667925334865988, Validation Accuracy: 75.61666666666666\n",
            "End of Epoch 399, Training Loss: 1.2979491968527337, Training Accuracy: 71.59629629629629\n",
            "End of Epoch 399, Validation Loss: 1.2646613022427833, Validation Accuracy: 75.68333333333334\n",
            "End of Epoch 400, Training Loss: 1.295903332180862, Training Accuracy: 71.67037037037038\n",
            "End of Epoch 400, Validation Loss: 1.2625367509269751, Validation Accuracy: 75.8\n",
            "End of Epoch 401, Training Loss: 1.293863945340407, Training Accuracy: 71.71666666666667\n",
            "End of Epoch 401, Validation Loss: 1.2604188406897874, Validation Accuracy: 75.86666666666667\n",
            "End of Epoch 402, Training Loss: 1.2918309994544077, Training Accuracy: 71.76296296296296\n",
            "End of Epoch 402, Validation Loss: 1.258307532846323, Validation Accuracy: 75.9\n",
            "End of Epoch 403, Training Loss: 1.2898044578105796, Training Accuracy: 71.8037037037037\n",
            "End of Epoch 403, Validation Loss: 1.2562027888826308, Validation Accuracy: 76.0\n",
            "End of Epoch 404, Training Loss: 1.2877842838671176, Training Accuracy: 71.85185185185186\n",
            "End of Epoch 404, Validation Loss: 1.2541045704620242, Validation Accuracy: 76.0\n",
            "End of Epoch 405, Training Loss: 1.285770441258354, Training Accuracy: 71.89814814814814\n",
            "End of Epoch 405, Validation Loss: 1.2520128394312606, Validation Accuracy: 76.01666666666667\n",
            "End of Epoch 406, Training Loss: 1.2837628938002852, Training Accuracy: 71.94259259259259\n",
            "End of Epoch 406, Validation Loss: 1.2499275578265776, Validation Accuracy: 76.01666666666667\n",
            "End of Epoch 407, Training Loss: 1.2817616054959624, Training Accuracy: 71.98518518518519\n",
            "End of Epoch 407, Validation Loss: 1.2478486878795922, Validation Accuracy: 76.05\n",
            "End of Epoch 408, Training Loss: 1.27976654054074, Training Accuracy: 72.02962962962964\n",
            "End of Epoch 408, Validation Loss: 1.2457761920230586, Validation Accuracy: 76.1\n",
            "End of Epoch 409, Training Loss: 1.2777776633273943, Training Accuracy: 72.07037037037037\n",
            "End of Epoch 409, Validation Loss: 1.2437100328964836, Validation Accuracy: 76.14999999999999\n",
            "End of Epoch 410, Training Loss: 1.2757949384510976, Training Accuracy: 72.12407407407407\n",
            "End of Epoch 410, Validation Loss: 1.241650173351603, Validation Accuracy: 76.2\n",
            "End of Epoch 411, Training Loss: 1.2738183307142632, Training Accuracy: 72.16851851851852\n",
            "End of Epoch 411, Validation Loss: 1.2395965764577155, Validation Accuracy: 76.23333333333333\n",
            "End of Epoch 412, Training Loss: 1.27184780513124, Training Accuracy: 72.22037037037038\n",
            "End of Epoch 412, Validation Loss: 1.237549205506875, Validation Accuracy: 76.28333333333333\n",
            "End of Epoch 413, Training Loss: 1.2698833269328822, Training Accuracy: 72.28148148148148\n",
            "End of Epoch 413, Validation Loss: 1.2355080240189393, Validation Accuracy: 76.31666666666666\n",
            "End of Epoch 414, Training Loss: 1.2679248615709704, Training Accuracy: 72.33518518518518\n",
            "End of Epoch 414, Validation Loss: 1.23347299574648, Validation Accuracy: 76.35\n",
            "End of Epoch 415, Training Loss: 1.2659723747224987, Training Accuracy: 72.38333333333333\n",
            "End of Epoch 415, Validation Loss: 1.2314440846795462, Validation Accuracy: 76.36666666666667\n",
            "End of Epoch 416, Training Loss: 1.264025832293824, Training Accuracy: 72.44444444444444\n",
            "End of Epoch 416, Validation Loss: 1.229421255050286, Validation Accuracy: 76.41666666666667\n",
            "End of Epoch 417, Training Loss: 1.26208520042467, Training Accuracy: 72.47592592592592\n",
            "End of Epoch 417, Validation Loss: 1.227404471337426, Validation Accuracy: 76.46666666666667\n",
            "End of Epoch 418, Training Loss: 1.260150445492001, Training Accuracy: 72.53518518518518\n",
            "End of Epoch 418, Validation Loss: 1.2253936982706068, Validation Accuracy: 76.48333333333333\n",
            "End of Epoch 419, Training Loss: 1.2582215341137428, Training Accuracy: 72.58703703703704\n",
            "End of Epoch 419, Validation Loss: 1.2233889008345733, Validation Accuracy: 76.58333333333334\n",
            "End of Epoch 420, Training Loss: 1.2562984331523805, Training Accuracy: 72.62222222222222\n",
            "End of Epoch 420, Validation Loss: 1.2213900442732208, Validation Accuracy: 76.68333333333334\n",
            "End of Epoch 421, Training Loss: 1.2543811097184028, Training Accuracy: 72.65925925925926\n",
            "End of Epoch 421, Validation Loss: 1.219397094093501, Validation Accuracy: 76.64999999999999\n",
            "End of Epoch 422, Training Loss: 1.2524695311736136, Training Accuracy: 72.71296296296296\n",
            "End of Epoch 422, Validation Loss: 1.2174100160691765, Validation Accuracy: 76.73333333333333\n",
            "End of Epoch 423, Training Loss: 1.250563665134298, Training Accuracy: 72.77037037037037\n",
            "End of Epoch 423, Validation Loss: 1.2154287762444362, Validation Accuracy: 76.78333333333333\n",
            "End of Epoch 424, Training Loss: 1.248663479474255, Training Accuracy: 72.84074074074074\n",
            "End of Epoch 424, Validation Loss: 1.2134533409373636, Validation Accuracy: 76.9\n",
            "End of Epoch 425, Training Loss: 1.246768942327687, Training Accuracy: 72.86111111111111\n",
            "End of Epoch 425, Validation Loss: 1.2114836767432593, Validation Accuracy: 76.95\n",
            "End of Epoch 426, Training Loss: 1.2448800220919465, Training Accuracy: 72.91296296296296\n",
            "End of Epoch 426, Validation Loss: 1.2095197505378206, Validation Accuracy: 76.98333333333333\n",
            "End of Epoch 427, Training Loss: 1.242996687430147, Training Accuracy: 72.97962962962963\n",
            "End of Epoch 427, Validation Loss: 1.207561529480174, Validation Accuracy: 77.11666666666666\n",
            "End of Epoch 428, Training Loss: 1.241118907273632, Training Accuracy: 73.04074074074074\n",
            "End of Epoch 428, Validation Loss: 1.2056089810157624, Validation Accuracy: 77.21666666666667\n",
            "End of Epoch 429, Training Loss: 1.239246650824306, Training Accuracy: 73.1\n",
            "End of Epoch 429, Validation Loss: 1.2036620728790892, Validation Accuracy: 77.28333333333333\n",
            "End of Epoch 430, Training Loss: 1.2373798875568258, Training Accuracy: 73.1574074074074\n",
            "End of Epoch 430, Validation Loss: 1.2017207730963138, Validation Accuracy: 77.3\n",
            "End of Epoch 431, Training Loss: 1.2355185872206493, Training Accuracy: 73.20555555555556\n",
            "End of Epoch 431, Validation Loss: 1.1997850499877063, Validation Accuracy: 77.35\n",
            "End of Epoch 432, Training Loss: 1.233662719841952, Training Accuracy: 73.2462962962963\n",
            "End of Epoch 432, Validation Loss: 1.1978548721699536, Validation Accuracy: 77.45\n",
            "End of Epoch 433, Training Loss: 1.2318122557254005, Training Accuracy: 73.3\n",
            "End of Epoch 433, Validation Loss: 1.1959302085583219, Validation Accuracy: 77.5\n",
            "End of Epoch 434, Training Loss: 1.2299671654557893, Training Accuracy: 73.34814814814816\n",
            "End of Epoch 434, Validation Loss: 1.1940110283686773, Validation Accuracy: 77.58333333333334\n",
            "End of Epoch 435, Training Loss: 1.22812741989954, Training Accuracy: 73.39259259259259\n",
            "End of Epoch 435, Validation Loss: 1.1920973011193574, Validation Accuracy: 77.58333333333334\n",
            "End of Epoch 436, Training Loss: 1.2262929902060649, Training Accuracy: 73.45555555555555\n",
            "End of Epoch 436, Validation Loss: 1.1901889966329047, Validation Accuracy: 77.58333333333334\n",
            "End of Epoch 437, Training Loss: 1.224463847808994, Training Accuracy: 73.51851851851852\n",
            "End of Epoch 437, Validation Loss: 1.1882860850376515, Validation Accuracy: 77.60000000000001\n",
            "End of Epoch 438, Training Loss: 1.222639964427267, Training Accuracy: 73.57222222222222\n",
            "End of Epoch 438, Validation Loss: 1.1863885367691664, Validation Accuracy: 77.66666666666666\n",
            "End of Epoch 439, Training Loss: 1.2208213120660885, Training Accuracy: 73.62407407407407\n",
            "End of Epoch 439, Validation Loss: 1.1844963225715557, Validation Accuracy: 77.68333333333334\n",
            "End of Epoch 440, Training Loss: 1.2190078630177512, Training Accuracy: 73.66851851851852\n",
            "End of Epoch 440, Validation Loss: 1.1826094134986247, Validation Accuracy: 77.71666666666667\n",
            "End of Epoch 441, Training Loss: 1.2171995898623291, Training Accuracy: 73.72037037037038\n",
            "End of Epoch 441, Validation Loss: 1.180727780914898, Validation Accuracy: 77.76666666666667\n",
            "End of Epoch 442, Training Loss: 1.2153964654682334, Training Accuracy: 73.76296296296296\n",
            "End of Epoch 442, Validation Loss: 1.1788513964964975, Validation Accuracy: 77.8\n",
            "End of Epoch 443, Training Loss: 1.2135984629926415, Training Accuracy: 73.79814814814814\n",
            "End of Epoch 443, Validation Loss: 1.1769802322318856, Validation Accuracy: 77.88333333333334\n",
            "End of Epoch 444, Training Loss: 1.211805555881794, Training Accuracy: 73.8462962962963\n",
            "End of Epoch 444, Validation Loss: 1.1751142604224667, Validation Accuracy: 77.95\n",
            "End of Epoch 445, Training Loss: 1.2100177178711666, Training Accuracy: 73.88703703703705\n",
            "End of Epoch 445, Validation Loss: 1.173253453683051, Validation Accuracy: 77.96666666666667\n",
            "End of Epoch 446, Training Loss: 1.20823492298551, Training Accuracy: 73.92407407407407\n",
            "End of Epoch 446, Validation Loss: 1.171397784942185, Validation Accuracy: 77.98333333333333\n",
            "End of Epoch 447, Training Loss: 1.2064571455387716, Training Accuracy: 73.96296296296296\n",
            "End of Epoch 447, Validation Loss: 1.1695472274423437, Validation Accuracy: 78.01666666666667\n",
            "End of Epoch 448, Training Loss: 1.204684360133881, Training Accuracy: 74.00740740740741\n",
            "End of Epoch 448, Validation Loss: 1.1677017547399913, Validation Accuracy: 78.08333333333334\n",
            "End of Epoch 449, Training Loss: 1.202916541662429, Training Accuracy: 74.05000000000001\n",
            "End of Epoch 449, Validation Loss: 1.1658613407055074, Validation Accuracy: 78.14999999999999\n",
            "End of Epoch 450, Training Loss: 1.2011536653042085, Training Accuracy: 74.09444444444443\n",
            "End of Epoch 450, Validation Loss: 1.1640259595229845, Validation Accuracy: 78.18333333333334\n",
            "End of Epoch 451, Training Loss: 1.1993957065266485, Training Accuracy: 74.11851851851851\n",
            "End of Epoch 451, Validation Loss: 1.1621955856898936, Validation Accuracy: 78.21666666666667\n",
            "End of Epoch 452, Training Loss: 1.1976426410841257, Training Accuracy: 74.16296296296296\n",
            "End of Epoch 452, Validation Loss: 1.1603701940166204, Validation Accuracy: 78.26666666666667\n",
            "End of Epoch 453, Training Loss: 1.1958944450171556, Training Accuracy: 74.19444444444444\n",
            "End of Epoch 453, Validation Loss: 1.1585497596258794, Validation Accuracy: 78.26666666666667\n",
            "End of Epoch 454, Training Loss: 1.194151094651482, Training Accuracy: 74.23333333333333\n",
            "End of Epoch 454, Validation Loss: 1.1567342579519992, Validation Accuracy: 78.3\n",
            "End of Epoch 455, Training Loss: 1.1924125665970395, Training Accuracy: 74.26296296296296\n",
            "End of Epoch 455, Validation Loss: 1.1549236647400862, Validation Accuracy: 78.3\n",
            "End of Epoch 456, Training Loss: 1.1906788377468196, Training Accuracy: 74.30925925925926\n",
            "End of Epoch 456, Validation Loss: 1.153117956045066, Validation Accuracy: 78.33333333333333\n",
            "End of Epoch 457, Training Loss: 1.1889498852756153, Training Accuracy: 74.35925925925926\n",
            "End of Epoch 457, Validation Loss: 1.1513171082306066, Validation Accuracy: 78.36666666666666\n",
            "End of Epoch 458, Training Loss: 1.187225686638674, Training Accuracy: 74.40185185185186\n",
            "End of Epoch 458, Validation Loss: 1.1495210979679245, Validation Accuracy: 78.45\n",
            "End of Epoch 459, Training Loss: 1.1855062195702377, Training Accuracy: 74.44814814814815\n",
            "End of Epoch 459, Validation Loss: 1.1477299022344731, Validation Accuracy: 78.45\n",
            "End of Epoch 460, Training Loss: 1.1837914620819827, Training Accuracy: 74.48333333333333\n",
            "End of Epoch 460, Validation Loss: 1.1459434983125214, Validation Accuracy: 78.48333333333333\n",
            "End of Epoch 461, Training Loss: 1.1820813924613665, Training Accuracy: 74.52777777777779\n",
            "End of Epoch 461, Validation Loss: 1.1441618637876185, Validation Accuracy: 78.56666666666666\n",
            "End of Epoch 462, Training Loss: 1.1803759892698704, Training Accuracy: 74.57222222222222\n",
            "End of Epoch 462, Validation Loss: 1.1423849765469511, Validation Accuracy: 78.64999999999999\n",
            "End of Epoch 463, Training Loss: 1.1786752313411564, Training Accuracy: 74.6037037037037\n",
            "End of Epoch 463, Validation Loss: 1.140612814777596, Validation Accuracy: 78.68333333333334\n",
            "End of Epoch 464, Training Loss: 1.1769790977791232, Training Accuracy: 74.64074074074074\n",
            "End of Epoch 464, Validation Loss: 1.1388453569646644, Validation Accuracy: 78.7\n",
            "End of Epoch 465, Training Loss: 1.1752875679558863, Training Accuracy: 74.66481481481482\n",
            "End of Epoch 465, Validation Loss: 1.1370825818893493, Validation Accuracy: 78.75\n",
            "End of Epoch 466, Training Loss: 1.1736006215096562, Training Accuracy: 74.70370370370371\n",
            "End of Epoch 466, Validation Loss: 1.1353244686268713, Validation Accuracy: 78.83333333333333\n",
            "End of Epoch 467, Training Loss: 1.171918238342551, Training Accuracy: 74.7574074074074\n",
            "End of Epoch 467, Validation Loss: 1.1335709965443292, Validation Accuracy: 78.85\n",
            "End of Epoch 468, Training Loss: 1.1702403986183119, Training Accuracy: 74.80740740740741\n",
            "End of Epoch 468, Validation Loss: 1.131822145298457, Validation Accuracy: 78.9\n",
            "End of Epoch 469, Training Loss: 1.1685670827599517, Training Accuracy: 74.85185185185185\n",
            "End of Epoch 469, Validation Loss: 1.1300778948332868, Validation Accuracy: 78.95\n",
            "End of Epoch 470, Training Loss: 1.1668982714473262, Training Accuracy: 74.88703703703705\n",
            "End of Epoch 470, Validation Loss: 1.1283382253777292, Validation Accuracy: 78.96666666666667\n",
            "End of Epoch 471, Training Loss: 1.1652339456146303, Training Accuracy: 74.91851851851851\n",
            "End of Epoch 471, Validation Loss: 1.1266031174430635, Validation Accuracy: 78.96666666666667\n",
            "End of Epoch 472, Training Loss: 1.163574086447827, Training Accuracy: 74.97777777777777\n",
            "End of Epoch 472, Validation Loss: 1.1248725518203455, Validation Accuracy: 78.98333333333333\n",
            "End of Epoch 473, Training Loss: 1.161918675382011, Training Accuracy: 75.02222222222223\n",
            "End of Epoch 473, Validation Loss: 1.1231465095777393, Validation Accuracy: 79.03333333333333\n",
            "End of Epoch 474, Training Loss: 1.1602676940987073, Training Accuracy: 75.07592592592593\n",
            "End of Epoch 474, Validation Loss: 1.1214249720577663, Validation Accuracy: 79.11666666666667\n",
            "End of Epoch 475, Training Loss: 1.1586211245231122, Training Accuracy: 75.1037037037037\n",
            "End of Epoch 475, Validation Loss: 1.1197079208744858, Validation Accuracy: 79.2\n",
            "End of Epoch 476, Training Loss: 1.1569789488212707, Training Accuracy: 75.15925925925926\n",
            "End of Epoch 476, Validation Loss: 1.1179953379106011, Validation Accuracy: 79.28333333333333\n",
            "End of Epoch 477, Training Loss: 1.155341149397207, Training Accuracy: 75.19074074074074\n",
            "End of Epoch 477, Validation Loss: 1.1162872053144977, Validation Accuracy: 79.3\n",
            "End of Epoch 478, Training Loss: 1.1537077088899996, Training Accuracy: 75.20370370370371\n",
            "End of Epoch 478, Validation Loss: 1.1145835054972193, Validation Accuracy: 79.38333333333333\n",
            "End of Epoch 479, Training Loss: 1.1520786101708067, Training Accuracy: 75.22222222222223\n",
            "End of Epoch 479, Validation Loss: 1.1128842211293783, Validation Accuracy: 79.36666666666666\n",
            "End of Epoch 480, Training Loss: 1.1504538363398498, Training Accuracy: 75.25555555555556\n",
            "End of Epoch 480, Validation Loss: 1.111189335138007, Validation Accuracy: 79.41666666666667\n",
            "End of Epoch 481, Training Loss: 1.1488333707233507, Training Accuracy: 75.28518518518518\n",
            "End of Epoch 481, Validation Loss: 1.1094988307033598, Validation Accuracy: 79.45\n",
            "End of Epoch 482, Training Loss: 1.1472171968704352, Training Accuracy: 75.32777777777778\n",
            "End of Epoch 482, Validation Loss: 1.1078126912556496, Validation Accuracy: 79.5\n",
            "End of Epoch 483, Training Loss: 1.1456052985499914, Training Accuracy: 75.36111111111111\n",
            "End of Epoch 483, Validation Loss: 1.1061309004717486, Validation Accuracy: 79.51666666666667\n",
            "End of Epoch 484, Training Loss: 1.143997659747503, Training Accuracy: 75.39444444444445\n",
            "End of Epoch 484, Validation Loss: 1.1044534422718315, Validation Accuracy: 79.61666666666667\n",
            "End of Epoch 485, Training Loss: 1.1423942646618463, Training Accuracy: 75.41481481481482\n",
            "End of Epoch 485, Validation Loss: 1.1027803008159818, Validation Accuracy: 79.65\n",
            "End of Epoch 486, Training Loss: 1.1407950977020636, Training Accuracy: 75.46111111111111\n",
            "End of Epoch 486, Validation Loss: 1.1011114605007508, Validation Accuracy: 79.66666666666666\n",
            "End of Epoch 487, Training Loss: 1.1392001434841066, Training Accuracy: 75.50185185185185\n",
            "End of Epoch 487, Validation Loss: 1.0994469059556864, Validation Accuracy: 79.68333333333332\n",
            "End of Epoch 488, Training Loss: 1.1376093868275643, Training Accuracy: 75.5425925925926\n",
            "End of Epoch 488, Validation Loss: 1.0977866220398211, Validation Accuracy: 79.7\n",
            "End of Epoch 489, Training Loss: 1.1360228127523673, Training Accuracy: 75.57592592592593\n",
            "End of Epoch 489, Validation Loss: 1.0961305938381325, Validation Accuracy: 79.76666666666667\n",
            "End of Epoch 490, Training Loss: 1.134440406475477, Training Accuracy: 75.61296296296295\n",
            "End of Epoch 490, Validation Loss: 1.094478806657971, Validation Accuracy: 79.76666666666667\n",
            "End of Epoch 491, Training Loss: 1.1328621534075591, Training Accuracy: 75.64074074074074\n",
            "End of Epoch 491, Validation Loss: 1.092831246025465, Validation Accuracy: 79.76666666666667\n",
            "End of Epoch 492, Training Loss: 1.131288039149654, Training Accuracy: 75.65925925925926\n",
            "End of Epoch 492, Validation Loss: 1.091187897681903, Validation Accuracy: 79.78333333333333\n",
            "End of Epoch 493, Training Loss: 1.1297180494898271, Training Accuracy: 75.68333333333334\n",
            "End of Epoch 493, Validation Loss: 1.089548747580094, Validation Accuracy: 79.80000000000001\n",
            "End of Epoch 494, Training Loss: 1.1281521703998236, Training Accuracy: 75.71296296296296\n",
            "End of Epoch 494, Validation Loss: 1.0879137818807119, Validation Accuracy: 79.86666666666666\n",
            "End of Epoch 495, Training Loss: 1.126590388031718, Training Accuracy: 75.74074074074075\n",
            "End of Epoch 495, Validation Loss: 1.086282986948626, Validation Accuracy: 79.88333333333333\n",
            "End of Epoch 496, Training Loss: 1.1250326887145572, Training Accuracy: 75.77407407407406\n",
            "End of Epoch 496, Validation Loss: 1.0846563493492178, Validation Accuracy: 79.93333333333334\n",
            "End of Epoch 497, Training Loss: 1.1234790589510135, Training Accuracy: 75.82222222222222\n",
            "End of Epoch 497, Validation Loss: 1.0830338558446935, Validation Accuracy: 79.96666666666667\n",
            "End of Epoch 498, Training Loss: 1.1219294854140394, Training Accuracy: 75.87037037037037\n",
            "End of Epoch 498, Validation Loss: 1.081415493390385, Validation Accuracy: 79.98333333333333\n",
            "End of Epoch 499, Training Loss: 1.1203839549435248, Training Accuracy: 75.90555555555557\n",
            "End of Epoch 499, Validation Loss: 1.0798012491310525, Validation Accuracy: 80.0\n",
            "End of Epoch 500, Training Loss: 1.1188424545429727, Training Accuracy: 75.93888888888888\n",
            "End of Epoch 500, Validation Loss: 1.0781911103971809, Validation Accuracy: 80.01666666666667\n"
          ]
        }
      ],
      "source": [
        "#gradient descent\n",
        "model.grad_descent(x_train_mnist, y_train_encode_mnist,x_val_mnist,y_val_encode_mnist, eta=0.1, max_epochs=501, batch_size=54000)\n",
        "# batch size means how many datapoints will train in one epoch and\n",
        "# after that it will update the weights and biases"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In the hope to get a higher accuracy, enlarge the neural network, by adding an additional hidden layer and adding more interior nodes. Try 20 nodes in the first and 10 nodes in the second hidden layer. Run propagation with 1000 steps and learning rate Î± = 0.1. How much improvement does the larger network provide? Turn in your code for the larger dataset and some printout that documents the accuracy rate.\n"
      ],
      "metadata": {
        "id": "WTcfrAn4agSg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "    input size: 28*28=784, two hidden layers of size 20 and 10 neurons, and the output class labels 10"
      ],
      "metadata": {
        "id": "7bzt3RGUA2q-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer_sizes = [784,20,10, 10]\n",
        "\n",
        "# initializing the model\n",
        "model = backward_optimizer(layer_sizes, activation_func=\"sigmoid\", weight_init=\"random\")"
      ],
      "metadata": {
        "id": "Y3c8TgWZvhLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.grad_descent(x_train_mnist, y_train_encode_mnist,x_val_mnist,y_val_encode_mnist, eta=0.1, max_epochs=1001, batch_size=5400)\n",
        "# batch size means how many datapoints will train in one epoch and\n",
        "# after that it will update the weights and biases"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yH4BXU3vbKba",
        "outputId": "8018cc9e-b33d-457a-bb67-e6ea37fabad7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "End of Epoch 0, Training Loss: 2.305780214149115, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 0, Validation Loss: 2.3061144364466144, Validation Accuracy: 10.5\n",
            "End of Epoch 1, Training Loss: 2.3032914159901265, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 1, Validation Loss: 2.3037570064233535, Validation Accuracy: 10.5\n",
            "End of Epoch 2, Training Loss: 2.30204878771895, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 2, Validation Loss: 2.3026159230124765, Validation Accuracy: 10.5\n",
            "End of Epoch 3, Training Loss: 2.3013818826461296, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 3, Validation Loss: 2.302024042604937, Validation Accuracy: 10.5\n",
            "End of Epoch 4, Training Loss: 2.3009802465328444, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 4, Validation Loss: 2.3016761177461396, Validation Accuracy: 10.5\n",
            "End of Epoch 5, Training Loss: 2.300700358742288, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 5, Validation Loss: 2.3014337159286486, Validation Accuracy: 10.5\n",
            "End of Epoch 6, Training Loss: 2.3004760351408464, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 6, Validation Loss: 2.301234917475125, Validation Accuracy: 10.5\n",
            "End of Epoch 7, Training Loss: 2.300276776500714, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 7, Validation Loss: 2.301052544526163, Validation Accuracy: 10.5\n",
            "End of Epoch 8, Training Loss: 2.300088480858697, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 8, Validation Loss: 2.3008749783121725, Validation Accuracy: 10.5\n",
            "End of Epoch 9, Training Loss: 2.2999045552790798, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 9, Validation Loss: 2.3006974369241804, Validation Accuracy: 10.5\n",
            "End of Epoch 10, Training Loss: 2.2997218365474144, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 10, Validation Loss: 2.3005180570394894, Validation Accuracy: 10.5\n",
            "End of Epoch 11, Training Loss: 2.299538724201279, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 11, Validation Loss: 2.3003361612363995, Validation Accuracy: 10.5\n",
            "End of Epoch 12, Training Loss: 2.299354327397009, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 12, Validation Loss: 2.3001515091939786, Validation Accuracy: 10.5\n",
            "End of Epoch 13, Training Loss: 2.299168075172261, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 13, Validation Loss: 2.2999639859171674, Validation Accuracy: 10.5\n",
            "End of Epoch 14, Training Loss: 2.298979538173567, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 14, Validation Loss: 2.2997734803627865, Validation Accuracy: 10.5\n",
            "End of Epoch 15, Training Loss: 2.298788346790686, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 15, Validation Loss: 2.299579844471205, Validation Accuracy: 10.5\n",
            "End of Epoch 16, Training Loss: 2.298594153211644, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 16, Validation Loss: 2.299382884314234, Validation Accuracy: 10.5\n",
            "End of Epoch 17, Training Loss: 2.298396613467255, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 17, Validation Loss: 2.2991823626834154, Validation Accuracy: 10.5\n",
            "End of Epoch 18, Training Loss: 2.2981953785531957, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 18, Validation Loss: 2.2989780046405874, Validation Accuracy: 10.5\n",
            "End of Epoch 19, Training Loss: 2.297990089651572, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 19, Validation Loss: 2.298769502831741, Validation Accuracy: 10.5\n",
            "End of Epoch 20, Training Loss: 2.2977803751783115, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 20, Validation Loss: 2.298556521571792, Validation Accuracy: 10.5\n",
            "End of Epoch 21, Training Loss: 2.297565848615168, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 21, Validation Loss: 2.298338699572534, Validation Accuracy: 10.5\n",
            "End of Epoch 22, Training Loss: 2.297346106646678, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 22, Validation Loss: 2.298115651480503, Validation Accuracy: 10.5\n",
            "End of Epoch 23, Training Loss: 2.297120727378114, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 23, Validation Loss: 2.2978869684555665, Validation Accuracy: 10.5\n",
            "End of Epoch 24, Training Loss: 2.296889268526473, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 24, Validation Loss: 2.2976522180009917, Validation Accuracy: 10.5\n",
            "End of Epoch 25, Training Loss: 2.2966512655290603, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 25, Validation Loss: 2.2974109432119323, Validation Accuracy: 10.5\n",
            "End of Epoch 26, Training Loss: 2.2964062295374337, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 26, Validation Loss: 2.297162661565031, Validation Accuracy: 10.5\n",
            "End of Epoch 27, Training Loss: 2.2961536452745324, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 27, Validation Loss: 2.2969068633345184, Validation Accuracy: 10.5\n",
            "End of Epoch 28, Training Loss: 2.295892968736645, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 28, Validation Loss: 2.2966430096910853, Validation Accuracy: 10.5\n",
            "End of Epoch 29, Training Loss: 2.295623624722907, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 29, Validation Loss: 2.296370530517906, Validation Accuracy: 10.5\n",
            "End of Epoch 30, Training Loss: 2.2953450041746395, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 30, Validation Loss: 2.2960888219619195, Validation Accuracy: 10.5\n",
            "End of Epoch 31, Training Loss: 2.2950564613057933, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 31, Validation Loss: 2.295797243726426, Validation Accuracy: 10.5\n",
            "End of Epoch 32, Training Loss: 2.2947573105043197, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 32, Validation Loss: 2.295495116102033, Validation Accuracy: 10.5\n",
            "End of Epoch 33, Training Loss: 2.2944468229824815, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 33, Validation Loss: 2.2951817167259865, Validation Accuracy: 10.5\n",
            "End of Epoch 34, Training Loss: 2.29412422315225, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 34, Validation Loss: 2.2948562770543774, Validation Accuracy: 10.5\n",
            "End of Epoch 35, Training Loss: 2.293788684699633, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 35, Validation Loss: 2.2945179785271237, Validation Accuracy: 10.5\n",
            "End of Epoch 36, Training Loss: 2.293439326329473, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 36, Validation Loss: 2.2941659484014885, Validation Accuracy: 10.5\n",
            "End of Epoch 37, Training Loss: 2.293075207149591, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 37, Validation Loss: 2.293799255226228, Validation Accuracy: 10.5\n",
            "End of Epoch 38, Training Loss: 2.292695321660323, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 38, Validation Loss: 2.2934169039248244, Validation Accuracy: 10.5\n",
            "End of Epoch 39, Training Loss: 2.292298594312424, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 39, Validation Loss: 2.2930178304527025, Validation Accuracy: 10.5\n",
            "End of Epoch 40, Training Loss: 2.291883873593013, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 40, Validation Loss: 2.29260089598974, Validation Accuracy: 10.5\n",
            "End of Epoch 41, Training Loss: 2.291449925595719, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 41, Validation Loss: 2.2921648806256196, Validation Accuracy: 10.5\n",
            "End of Epoch 42, Training Loss: 2.2909954270274624, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 42, Validation Loss: 2.291708476491772, Validation Accuracy: 10.5\n",
            "End of Epoch 43, Training Loss: 2.290518957600384, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 43, Validation Loss: 2.291230280289666, Validation Accuracy: 10.5\n",
            "End of Epoch 44, Training Loss: 2.290018991753424, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 44, Validation Loss: 2.2907287851611478, Validation Accuracy: 10.5\n",
            "End of Epoch 45, Training Loss: 2.289493889643903, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 45, Validation Loss: 2.290202371842401, Validation Accuracy: 10.5\n",
            "End of Epoch 46, Training Loss: 2.288941887345402, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 46, Validation Loss: 2.2896492990389983, Validation Accuracy: 10.5\n",
            "End of Epoch 47, Training Loss: 2.2883610861842407, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 47, Validation Loss: 2.289067692955527, Validation Accuracy: 10.5\n",
            "End of Epoch 48, Training Loss: 2.2877494411431956, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 48, Validation Loss: 2.2884555359095815, Validation Accuracy: 10.5\n",
            "End of Epoch 49, Training Loss: 2.2871047482579576, Training Accuracy: 11.318518518518518\n",
            "End of Epoch 49, Validation Loss: 2.2878106539566505, Validation Accuracy: 10.5\n",
            "End of Epoch 50, Training Loss: 2.2864246309293614, Training Accuracy: 11.335185185185185\n",
            "End of Epoch 50, Validation Loss: 2.2871307034499297, Validation Accuracy: 10.5\n",
            "End of Epoch 51, Training Loss: 2.2857065250731745, Training Accuracy: 11.37962962962963\n",
            "End of Epoch 51, Validation Loss: 2.2864131564575754, Validation Accuracy: 10.549999999999999\n",
            "End of Epoch 52, Training Loss: 2.284947663029438, Training Accuracy: 11.485185185185186\n",
            "End of Epoch 52, Validation Loss: 2.285655284959907, Validation Accuracy: 10.583333333333334\n",
            "End of Epoch 53, Training Loss: 2.2841450561556123, Training Accuracy: 11.716666666666667\n",
            "End of Epoch 53, Validation Loss: 2.284854143750952, Validation Accuracy: 10.766666666666666\n",
            "End of Epoch 54, Training Loss: 2.2832954760327984, Training Accuracy: 12.092592592592592\n",
            "End of Epoch 54, Validation Loss: 2.2840065519732415, Validation Accuracy: 11.083333333333334\n",
            "End of Epoch 55, Training Loss: 2.2823954342227917, Training Accuracy: 12.688888888888888\n",
            "End of Epoch 55, Validation Loss: 2.2831090732226214, Validation Accuracy: 11.683333333333334\n",
            "End of Epoch 56, Training Loss: 2.2814411605267746, Training Accuracy: 13.353703703703705\n",
            "End of Epoch 56, Validation Loss: 2.2821579941720174, Validation Accuracy: 12.333333333333334\n",
            "End of Epoch 57, Training Loss: 2.2804285797151183, Training Accuracy: 14.09074074074074\n",
            "End of Epoch 57, Validation Loss: 2.2811493016807223, Validation Accuracy: 13.066666666666665\n",
            "End of Epoch 58, Training Loss: 2.279353286723631, Training Accuracy: 14.951851851851853\n",
            "End of Epoch 58, Validation Loss: 2.2800786583802473, Validation Accuracy: 14.05\n",
            "End of Epoch 59, Training Loss: 2.2782105203462097, Training Accuracy: 15.842592592592592\n",
            "End of Epoch 59, Validation Loss: 2.2789413767607423, Validation Accuracy: 15.15\n",
            "End of Epoch 60, Training Loss: 2.2769951354993245, Training Accuracy: 16.748148148148147\n",
            "End of Epoch 60, Validation Loss: 2.2777323918253383, Validation Accuracy: 16.03333333333333\n",
            "End of Epoch 61, Training Loss: 2.2757015741922637, Training Accuracy: 17.56111111111111\n",
            "End of Epoch 61, Validation Loss: 2.276446232435737, Validation Accuracy: 16.950000000000003\n",
            "End of Epoch 62, Training Loss: 2.274323835411382, Training Accuracy: 18.37962962962963\n",
            "End of Epoch 62, Validation Loss: 2.2750769915435183, Validation Accuracy: 17.8\n",
            "End of Epoch 63, Training Loss: 2.2728554442193203, Training Accuracy: 19.114814814814814\n",
            "End of Epoch 63, Validation Loss: 2.273618295590691, Validation Accuracy: 18.53333333333333\n",
            "End of Epoch 64, Training Loss: 2.271289420484778, Training Accuracy: 19.8\n",
            "End of Epoch 64, Validation Loss: 2.2720632734730217, Validation Accuracy: 19.21666666666667\n",
            "End of Epoch 65, Training Loss: 2.2696182477977427, Training Accuracy: 20.349999999999998\n",
            "End of Epoch 65, Validation Loss: 2.2704045255937837, Validation Accuracy: 19.833333333333332\n",
            "End of Epoch 66, Training Loss: 2.267833843292425, Training Accuracy: 20.894444444444442\n",
            "End of Epoch 66, Validation Loss: 2.268634093696634, Validation Accuracy: 20.533333333333335\n",
            "End of Epoch 67, Training Loss: 2.265927529298103, Training Accuracy: 21.41111111111111\n",
            "End of Epoch 67, Validation Loss: 2.266743432357106, Validation Accuracy: 21.033333333333335\n",
            "End of Epoch 68, Training Loss: 2.263890007968342, Training Accuracy: 21.946296296296296\n",
            "End of Epoch 68, Validation Loss: 2.264723383234323, Validation Accuracy: 21.55\n",
            "End of Epoch 69, Training Loss: 2.2617113403019427, Training Accuracy: 22.487037037037037\n",
            "End of Epoch 69, Validation Loss: 2.2625641534383374, Validation Accuracy: 22.166666666666668\n",
            "End of Epoch 70, Training Loss: 2.259380931262375, Training Accuracy: 23.014814814814816\n",
            "End of Epoch 70, Validation Loss: 2.2602552996520497, Validation Accuracy: 22.633333333333333\n",
            "End of Epoch 71, Training Loss: 2.2568875230207124, Training Accuracy: 23.418518518518518\n",
            "End of Epoch 71, Validation Loss: 2.257785719954741, Validation Accuracy: 23.266666666666666\n",
            "End of Epoch 72, Training Loss: 2.2542191986801448, Training Accuracy: 23.872222222222224\n",
            "End of Epoch 72, Validation Loss: 2.2551436556171116, Validation Accuracy: 23.666666666666668\n",
            "End of Epoch 73, Training Loss: 2.2513633991711886, Training Accuracy: 24.324074074074073\n",
            "End of Epoch 73, Validation Loss: 2.2523167054596387, Validation Accuracy: 24.183333333333334\n",
            "End of Epoch 74, Training Loss: 2.2483069563114273, Training Accuracy: 24.774074074074072\n",
            "End of Epoch 74, Validation Loss: 2.249291855663763, Validation Accuracy: 24.55\n",
            "End of Epoch 75, Training Loss: 2.2450361452678793, Training Accuracy: 25.2\n",
            "End of Epoch 75, Validation Loss: 2.246055528166382, Validation Accuracy: 25.1\n",
            "End of Epoch 76, Training Loss: 2.241536759798959, Training Accuracy: 25.62962962962963\n",
            "End of Epoch 76, Validation Loss: 2.242593650909643, Validation Accuracy: 25.466666666666665\n",
            "End of Epoch 77, Training Loss: 2.2377942136303273, Training Accuracy: 25.96851851851852\n",
            "End of Epoch 77, Validation Loss: 2.238891753206232, Validation Accuracy: 25.816666666666666\n",
            "End of Epoch 78, Training Loss: 2.2337936710678354, Training Accuracy: 26.335185185185182\n",
            "End of Epoch 78, Validation Loss: 2.2349350892515396, Validation Accuracy: 26.3\n",
            "End of Epoch 79, Training Loss: 2.229520209397199, Training Accuracy: 26.631481481481483\n",
            "End of Epoch 79, Validation Loss: 2.230708792296982, Validation Accuracy: 26.616666666666667\n",
            "End of Epoch 80, Training Loss: 2.224959014688387, Training Accuracy: 26.959259259259262\n",
            "End of Epoch 80, Validation Loss: 2.226198061120701, Validation Accuracy: 27.0\n",
            "End of Epoch 81, Training Loss: 2.2200956112458834, Training Accuracy: 27.23888888888889\n",
            "End of Epoch 81, Validation Loss: 2.221388379127797, Validation Accuracy: 27.116666666666667\n",
            "End of Epoch 82, Training Loss: 2.2149161230794423, Training Accuracy: 27.514814814814812\n",
            "End of Epoch 82, Validation Loss: 2.2162657646381567, Validation Accuracy: 27.35\n",
            "End of Epoch 83, Training Loss: 2.2094075634096964, Training Accuracy: 27.724074074074075\n",
            "End of Epoch 83, Validation Loss: 2.2108170486702177, Validation Accuracy: 27.633333333333333\n",
            "End of Epoch 84, Training Loss: 2.203558145425963, Training Accuracy: 27.89814814814815\n",
            "End of Epoch 84, Validation Loss: 2.2050301738557887, Validation Accuracy: 27.900000000000002\n",
            "End of Epoch 85, Training Loss: 2.197357604417801, Training Accuracy: 28.10185185185185\n",
            "End of Epoch 85, Validation Loss: 2.198894505153968, Validation Accuracy: 28.349999999999998\n",
            "End of Epoch 86, Training Loss: 2.1907975182405077, Training Accuracy: 28.312962962962963\n",
            "End of Epoch 86, Validation Loss: 2.1924011399924312, Validation Accuracy: 28.549999999999997\n",
            "End of Epoch 87, Training Loss: 2.1838716101802254, Training Accuracy: 28.525925925925925\n",
            "End of Epoch 87, Validation Loss: 2.1855432026686183, Validation Accuracy: 28.76666666666667\n",
            "End of Epoch 88, Training Loss: 2.1765760160724605, Training Accuracy: 28.73888888888889\n",
            "End of Epoch 88, Validation Loss: 2.178316105690366, Validation Accuracy: 28.849999999999998\n",
            "End of Epoch 89, Training Loss: 2.1689094964614153, Training Accuracy: 29.005555555555556\n",
            "End of Epoch 89, Validation Loss: 2.170717759670305, Validation Accuracy: 29.01666666666667\n",
            "End of Epoch 90, Training Loss: 2.1608735751126513, Training Accuracy: 29.179629629629627\n",
            "End of Epoch 90, Validation Loss: 2.1627487138424417, Validation Accuracy: 29.299999999999997\n",
            "End of Epoch 91, Training Loss: 2.152472587655807, Training Accuracy: 29.4462962962963\n",
            "End of Epoch 91, Validation Loss: 2.1544122115805893, Validation Accuracy: 29.433333333333334\n",
            "End of Epoch 92, Training Loss: 2.1437136286936003, Training Accuracy: 29.762962962962963\n",
            "End of Epoch 92, Validation Loss: 2.145714149621493, Validation Accuracy: 29.56666666666667\n",
            "End of Epoch 93, Training Loss: 2.134606392250422, Training Accuracy: 30.057407407407403\n",
            "End of Epoch 93, Validation Loss: 2.136662935920735, Validation Accuracy: 29.71666666666667\n",
            "End of Epoch 94, Training Loss: 2.1251629085053163, Training Accuracy: 30.414814814814818\n",
            "End of Epoch 94, Validation Loss: 2.1272692487719307, Validation Accuracy: 30.01666666666667\n",
            "End of Epoch 95, Training Loss: 2.11539718859239, Training Accuracy: 30.777777777777775\n",
            "End of Epoch 95, Validation Loss: 2.1175457082598856, Validation Accuracy: 30.466666666666665\n",
            "End of Epoch 96, Training Loss: 2.105324797833897, Training Accuracy: 31.118518518518517\n",
            "End of Epoch 96, Validation Loss: 2.1075064793093614, Validation Accuracy: 30.766666666666666\n",
            "End of Epoch 97, Training Loss: 2.0949623849546364, Training Accuracy: 31.607407407407408\n",
            "End of Epoch 97, Validation Loss: 2.097166832431166, Validation Accuracy: 31.283333333333335\n",
            "End of Epoch 98, Training Loss: 2.0843271995349224, Training Accuracy: 31.99074074074074\n",
            "End of Epoch 98, Validation Loss: 2.086542692721639, Validation Accuracy: 31.666666666666664\n",
            "End of Epoch 99, Training Loss: 2.0734366313853427, Training Accuracy: 32.42777777777778\n",
            "End of Epoch 99, Validation Loss: 2.075650208968633, Validation Accuracy: 32.18333333333334\n",
            "End of Epoch 100, Training Loss: 2.06230780330321, Training Accuracy: 32.851851851851855\n",
            "End of Epoch 100, Validation Loss: 2.064505372515807, Validation Accuracy: 32.93333333333333\n",
            "End of Epoch 101, Training Loss: 2.050957242972926, Training Accuracy: 33.3537037037037\n",
            "End of Epoch 101, Validation Loss: 2.053123710022492, Validation Accuracy: 33.56666666666667\n",
            "End of Epoch 102, Training Loss: 2.03940065131497, Training Accuracy: 33.733333333333334\n",
            "End of Epoch 102, Validation Loss: 2.0415200661350155, Validation Accuracy: 34.03333333333333\n",
            "End of Epoch 103, Training Loss: 2.0276527745154773, Training Accuracy: 34.10185185185185\n",
            "End of Epoch 103, Validation Loss: 2.029708482475724, Validation Accuracy: 34.516666666666666\n",
            "End of Epoch 104, Training Loss: 2.0157273766588273, Training Accuracy: 34.52777777777778\n",
            "End of Epoch 104, Validation Loss: 2.0177021695953483, Validation Accuracy: 34.86666666666667\n",
            "End of Epoch 105, Training Loss: 2.0036373007110697, Training Accuracy: 34.93148148148148\n",
            "End of Epoch 105, Validation Loss: 2.005513559944341, Validation Accuracy: 35.333333333333336\n",
            "End of Epoch 106, Training Loss: 1.9913945986958719, Training Accuracy: 35.303703703703704\n",
            "End of Epoch 106, Validation Loss: 1.9931544235795884, Validation Accuracy: 35.65\n",
            "End of Epoch 107, Training Loss: 1.979010707985385, Training Accuracy: 35.681481481481484\n",
            "End of Epoch 107, Validation Loss: 1.98063602490273, Validation Accuracy: 35.96666666666667\n",
            "End of Epoch 108, Training Loss: 1.9664966499106387, Training Accuracy: 36.016666666666666\n",
            "End of Epoch 108, Validation Loss: 1.967969298397374, Validation Accuracy: 36.15\n",
            "End of Epoch 109, Training Loss: 1.953863229097716, Training Accuracy: 36.351851851851855\n",
            "End of Epoch 109, Validation Loss: 1.9551650237807352, Validation Accuracy: 36.43333333333334\n",
            "End of Epoch 110, Training Loss: 1.941121216371623, Training Accuracy: 36.71296296296296\n",
            "End of Epoch 110, Validation Loss: 1.942233985505368, Validation Accuracy: 36.766666666666666\n",
            "End of Epoch 111, Training Loss: 1.9282815038019918, Training Accuracy: 37.022222222222226\n",
            "End of Epoch 111, Validation Loss: 1.9291871071941729, Validation Accuracy: 37.03333333333333\n",
            "End of Epoch 112, Training Loss: 1.9153552264799645, Training Accuracy: 37.355555555555554\n",
            "End of Epoch 112, Validation Loss: 1.9160355573594725, Validation Accuracy: 37.333333333333336\n",
            "End of Epoch 113, Training Loss: 1.9023538509917621, Training Accuracy: 37.68888888888889\n",
            "End of Epoch 113, Validation Loss: 1.9027908277410528, Validation Accuracy: 37.7\n",
            "End of Epoch 114, Training Loss: 1.8892892345906471, Training Accuracy: 37.987037037037034\n",
            "End of Epoch 114, Validation Loss: 1.8894647891285021, Validation Accuracy: 37.9\n",
            "End of Epoch 115, Training Loss: 1.8761736613648643, Training Accuracy: 38.351851851851855\n",
            "End of Epoch 115, Validation Loss: 1.876069731249162, Validation Accuracy: 38.2\n",
            "End of Epoch 116, Training Loss: 1.8630198621749816, Training Accuracy: 38.672222222222224\n",
            "End of Epoch 116, Validation Loss: 1.8626183931709286, Validation Accuracy: 38.45\n",
            "End of Epoch 117, Training Loss: 1.8498410240002578, Training Accuracy: 38.97222222222222\n",
            "End of Epoch 117, Validation Loss: 1.8491239889481892, Validation Accuracy: 38.800000000000004\n",
            "End of Epoch 118, Training Loss: 1.8366507920213473, Training Accuracy: 39.318518518518516\n",
            "End of Epoch 118, Validation Loss: 1.8356002304080539, Validation Accuracy: 39.25\n",
            "End of Epoch 119, Training Loss: 1.8234632648369093, Training Accuracy: 39.65\n",
            "End of Epoch 119, Validation Loss: 1.8220613456370809, Validation Accuracy: 39.666666666666664\n",
            "End of Epoch 120, Training Loss: 1.8102929802603243, Training Accuracy: 39.99074074074074\n",
            "End of Epoch 120, Validation Loss: 1.8085220885172342, Validation Accuracy: 40.016666666666666\n",
            "End of Epoch 121, Training Loss: 1.7971548867144012, Training Accuracy: 40.38148148148148\n",
            "End of Epoch 121, Validation Loss: 1.7949977321439967, Validation Accuracy: 40.36666666666667\n",
            "End of Epoch 122, Training Loss: 1.7840642937629414, Training Accuracy: 40.738888888888894\n",
            "End of Epoch 122, Validation Loss: 1.7815040375784061, Validation Accuracy: 40.8\n",
            "End of Epoch 123, Training Loss: 1.7710367950519943, Training Accuracy: 41.114814814814814\n",
            "End of Epoch 123, Validation Loss: 1.7680571893998909, Validation Accuracy: 41.099999999999994\n",
            "End of Epoch 124, Training Loss: 1.7580881579608607, Training Accuracy: 41.51851851851852\n",
            "End of Epoch 124, Validation Loss: 1.7546736909998095, Validation Accuracy: 41.46666666666667\n",
            "End of Epoch 125, Training Loss: 1.745234176480976, Training Accuracy: 41.86851851851851\n",
            "End of Epoch 125, Validation Loss: 1.741370215348663, Validation Accuracy: 42.0\n",
            "End of Epoch 126, Training Loss: 1.7324904869853455, Training Accuracy: 42.25\n",
            "End of Epoch 126, Validation Loss: 1.7281634107670218, Validation Accuracy: 42.516666666666666\n",
            "End of Epoch 127, Training Loss: 1.7198723502318993, Training Accuracy: 42.648148148148145\n",
            "End of Epoch 127, Validation Loss: 1.7150696655777204, Validation Accuracy: 43.0\n",
            "End of Epoch 128, Training Loss: 1.7073944066959226, Training Accuracy: 43.06296296296296\n",
            "End of Epoch 128, Validation Loss: 1.7021048398804324, Validation Accuracy: 43.38333333333333\n",
            "End of Epoch 129, Training Loss: 1.6950704156689684, Training Accuracy: 43.50185185185185\n",
            "End of Epoch 129, Validation Loss: 1.6892839765229244, Validation Accuracy: 43.833333333333336\n",
            "End of Epoch 130, Training Loss: 1.6829129910624205, Training Accuracy: 43.92037037037037\n",
            "End of Epoch 130, Validation Loss: 1.6766210061610118, Validation Accuracy: 44.31666666666666\n",
            "End of Epoch 131, Training Loss: 1.6709333481880504, Training Accuracy: 44.364814814814814\n",
            "End of Epoch 131, Validation Loss: 1.6641284627452266, Validation Accuracy: 44.81666666666666\n",
            "End of Epoch 132, Training Loss: 1.6591410757831309, Training Accuracy: 44.742592592592594\n",
            "End of Epoch 132, Validation Loss: 1.6518172256698693, Validation Accuracy: 45.25\n",
            "End of Epoch 133, Training Loss: 1.6475439462067492, Training Accuracy: 45.16851851851852\n",
            "End of Epoch 133, Validation Loss: 1.639696303195918, Validation Accuracy: 45.766666666666666\n",
            "End of Epoch 134, Training Loss: 1.6361477742306447, Training Accuracy: 45.63888888888889\n",
            "End of Epoch 134, Validation Loss: 1.6277726688338277, Validation Accuracy: 46.33333333333333\n",
            "End of Epoch 135, Training Loss: 1.624956331495513, Training Accuracy: 46.12777777777778\n",
            "End of Epoch 135, Validation Loss: 1.6160511585197403, Validation Accuracy: 46.58333333333333\n",
            "End of Epoch 136, Training Loss: 1.6139713199009504, Training Accuracy: 46.56481481481482\n",
            "End of Epoch 136, Validation Loss: 1.6045344321029456, Validation Accuracy: 46.9\n",
            "End of Epoch 137, Training Loss: 1.6031924033650913, Training Accuracy: 46.979629629629635\n",
            "End of Epoch 137, Validation Loss: 1.5932229983627137, Validation Accuracy: 47.38333333333333\n",
            "End of Epoch 138, Training Loss: 1.592617293908007, Training Accuracy: 47.412962962962965\n",
            "End of Epoch 138, Validation Loss: 1.5821152989139269, Validation Accuracy: 47.91666666666667\n",
            "End of Epoch 139, Training Loss: 1.5822418851710482, Training Accuracy: 47.86111111111111\n",
            "End of Epoch 139, Validation Loss: 1.5712078432613896, Validation Accuracy: 48.46666666666667\n",
            "End of Epoch 140, Training Loss: 1.572060424455484, Training Accuracy: 48.266666666666666\n",
            "End of Epoch 140, Validation Loss: 1.5604953851071726, Validation Accuracy: 49.016666666666666\n",
            "End of Epoch 141, Training Loss: 1.5620657132011848, Training Accuracy: 48.7462962962963\n",
            "End of Epoch 141, Validation Loss: 1.5499711288548, Validation Accuracy: 49.583333333333336\n",
            "End of Epoch 142, Training Loss: 1.5522493254826137, Training Accuracy: 49.2\n",
            "End of Epoch 142, Validation Loss: 1.539626955027638, Validation Accuracy: 50.0\n",
            "End of Epoch 143, Training Loss: 1.5426018344598194, Training Accuracy: 49.64074074074074\n",
            "End of Epoch 143, Validation Loss: 1.5294536538886814, Validation Accuracy: 50.88333333333333\n",
            "End of Epoch 144, Training Loss: 1.5331130376406492, Training Accuracy: 50.011111111111106\n",
            "End of Epoch 144, Validation Loss: 1.5194411577375486, Validation Accuracy: 51.31666666666666\n",
            "End of Epoch 145, Training Loss: 1.5237721731413343, Training Accuracy: 50.529629629629625\n",
            "End of Epoch 145, Validation Loss: 1.5095787639826115, Validation Accuracy: 51.849999999999994\n",
            "End of Epoch 146, Training Loss: 1.5145681207490411, Training Accuracy: 50.97592592592592\n",
            "End of Epoch 146, Validation Loss: 1.4998553429684398, Validation Accuracy: 52.166666666666664\n",
            "End of Epoch 147, Training Loss: 1.505489583386258, Training Accuracy: 51.41481481481481\n",
            "End of Epoch 147, Validation Loss: 1.490259526528007, Validation Accuracy: 52.78333333333334\n",
            "End of Epoch 148, Training Loss: 1.496525246458213, Training Accuracy: 51.855555555555554\n",
            "End of Epoch 148, Validation Loss: 1.4807798751914651, Validation Accuracy: 53.349999999999994\n",
            "End of Epoch 149, Training Loss: 1.487663914430092, Training Accuracy: 52.31481481481482\n",
            "End of Epoch 149, Validation Loss: 1.4714050237986855, Validation Accuracy: 53.98333333333334\n",
            "End of Epoch 150, Training Loss: 1.478894625711096, Training Accuracy: 52.788888888888884\n",
            "End of Epoch 150, Validation Loss: 1.4621238068219224, Validation Accuracy: 54.43333333333334\n",
            "End of Epoch 151, Training Loss: 1.4702067483791093, Training Accuracy: 53.20370370370371\n",
            "End of Epoch 151, Validation Loss: 1.452925365910391, Validation Accuracy: 54.78333333333333\n",
            "End of Epoch 152, Training Loss: 1.4615900603234442, Training Accuracy: 53.629629629629626\n",
            "End of Epoch 152, Validation Loss: 1.44379924294299, Validation Accuracy: 55.21666666666667\n",
            "End of Epoch 153, Training Loss: 1.4530348179044, Training Accuracy: 54.03333333333333\n",
            "End of Epoch 153, Validation Loss: 1.434735462174417, Validation Accuracy: 55.63333333333333\n",
            "End of Epoch 154, Training Loss: 1.4445318171807209, Training Accuracy: 54.50185185185185\n",
            "End of Epoch 154, Validation Loss: 1.4257246048850498, Validation Accuracy: 56.05\n",
            "End of Epoch 155, Training Loss: 1.43607245117762, Training Accuracy: 54.93333333333334\n",
            "End of Epoch 155, Validation Loss: 1.4167578793517999, Validation Accuracy: 56.36666666666667\n",
            "End of Epoch 156, Training Loss: 1.427648765679223, Training Accuracy: 55.34259259259259\n",
            "End of Epoch 156, Validation Loss: 1.4078271880518327, Validation Accuracy: 56.75\n",
            "End of Epoch 157, Training Loss: 1.4192535148076497, Training Accuracy: 55.75370370370371\n",
            "End of Epoch 157, Validation Loss: 1.398925192934897, Validation Accuracy: 57.28333333333333\n",
            "End of Epoch 158, Training Loss: 1.4108802163906948, Training Accuracy: 56.18518518518518\n",
            "End of Epoch 158, Validation Loss: 1.3900453785040048, Validation Accuracy: 57.766666666666666\n",
            "End of Epoch 159, Training Loss: 1.402523205991645, Training Accuracy: 56.58888888888889\n",
            "End of Epoch 159, Validation Loss: 1.381182111462502, Validation Accuracy: 58.38333333333333\n",
            "End of Epoch 160, Training Loss: 1.3941776875975187, Training Accuracy: 56.998148148148154\n",
            "End of Epoch 160, Validation Loss: 1.3723306949157796, Validation Accuracy: 58.666666666666664\n",
            "End of Epoch 161, Training Loss: 1.3858397783957095, Training Accuracy: 57.38703703703704\n",
            "End of Epoch 161, Validation Loss: 1.3634874146119615, Validation Accuracy: 59.01666666666666\n",
            "End of Epoch 162, Training Loss: 1.377506544821959, Training Accuracy: 57.77592592592593\n",
            "End of Epoch 162, Validation Loss: 1.35464957448295, Validation Accuracy: 59.38333333333333\n",
            "End of Epoch 163, Training Loss: 1.3691760271090216, Training Accuracy: 58.12962962962963\n",
            "End of Epoch 163, Validation Loss: 1.3458155187933452, Validation Accuracy: 59.81666666666666\n",
            "End of Epoch 164, Training Loss: 1.3608472498627924, Training Accuracy: 58.48703703703704\n",
            "End of Epoch 164, Validation Loss: 1.3369846384936863, Validation Accuracy: 60.16666666666667\n",
            "End of Epoch 165, Training Loss: 1.3525202166935515, Training Accuracy: 58.81111111111112\n",
            "End of Epoch 165, Validation Loss: 1.3281573598734207, Validation Accuracy: 60.6\n",
            "End of Epoch 166, Training Loss: 1.3441958875866509, Training Accuracy: 59.25\n",
            "End of Epoch 166, Validation Loss: 1.3193351142811571, Validation Accuracy: 60.93333333333333\n",
            "End of Epoch 167, Training Loss: 1.3358761384603821, Training Accuracy: 59.58148148148148\n",
            "End of Epoch 167, Validation Loss: 1.3105202884823401, Validation Accuracy: 61.31666666666666\n",
            "End of Epoch 168, Training Loss: 1.3275637031768375, Training Accuracy: 59.903703703703705\n",
            "End of Epoch 168, Validation Loss: 1.301716156105627, Validation Accuracy: 61.53333333333333\n",
            "End of Epoch 169, Training Loss: 1.3192620990882853, Training Accuracy: 60.19814814814814\n",
            "End of Epoch 169, Validation Loss: 1.292926791527782, Validation Accuracy: 61.88333333333333\n",
            "End of Epoch 170, Training Loss: 1.3109755379598333, Training Accuracy: 60.50555555555556\n",
            "End of Epoch 170, Validation Loss: 1.2841569683949001, Validation Accuracy: 62.3\n",
            "End of Epoch 171, Training Loss: 1.3027088247563634, Training Accuracy: 60.84074074074074\n",
            "End of Epoch 171, Validation Loss: 1.275412045708107, Validation Accuracy: 62.53333333333333\n",
            "End of Epoch 172, Training Loss: 1.2944672472758256, Training Accuracy: 61.15555555555555\n",
            "End of Epoch 172, Validation Loss: 1.266697844956508, Validation Accuracy: 62.93333333333333\n",
            "End of Epoch 173, Training Loss: 1.2862564599255104, Training Accuracy: 61.46666666666667\n",
            "End of Epoch 173, Validation Loss: 1.2580205221187106, Validation Accuracy: 63.38333333333333\n",
            "End of Epoch 174, Training Loss: 1.2780823650637883, Training Accuracy: 61.70925925925926\n",
            "End of Epoch 174, Validation Loss: 1.24938643845931, Validation Accuracy: 63.71666666666667\n",
            "End of Epoch 175, Training Loss: 1.2699509952748977, Training Accuracy: 61.99259259259259\n",
            "End of Epoch 175, Validation Loss: 1.2408020339250434, Validation Accuracy: 64.05\n",
            "End of Epoch 176, Training Loss: 1.2618683997293745, Training Accuracy: 62.35000000000001\n",
            "End of Epoch 176, Validation Loss: 1.2322737066248575, Validation Accuracy: 64.43333333333334\n",
            "End of Epoch 177, Training Loss: 1.2538405374377128, Training Accuracy: 62.588888888888896\n",
            "End of Epoch 177, Validation Loss: 1.2238077014017743, Validation Accuracy: 64.7\n",
            "End of Epoch 178, Training Loss: 1.2458731797638376, Training Accuracy: 62.85740740740741\n",
            "End of Epoch 178, Validation Loss: 1.2154100099226384, Validation Accuracy: 65.03333333333333\n",
            "End of Epoch 179, Training Loss: 1.237971824062734, Training Accuracy: 63.114814814814814\n",
            "End of Epoch 179, Validation Loss: 1.2070862840753147, Validation Accuracy: 65.3\n",
            "End of Epoch 180, Training Loss: 1.2301416197758872, Training Accuracy: 63.37407407407407\n",
            "End of Epoch 180, Validation Loss: 1.1988417638173214, Validation Accuracy: 65.71666666666667\n",
            "End of Epoch 181, Training Loss: 1.2223873077878324, Training Accuracy: 63.62222222222223\n",
            "End of Epoch 181, Validation Loss: 1.1906812200026562, Validation Accuracy: 66.05\n",
            "End of Epoch 182, Training Loss: 1.2147131733417398, Training Accuracy: 63.81296296296296\n",
            "End of Epoch 182, Validation Loss: 1.1826089121526153, Validation Accuracy: 66.35\n",
            "End of Epoch 183, Training Loss: 1.207123012351062, Training Accuracy: 64.01481481481481\n",
            "End of Epoch 183, Validation Loss: 1.1746285606507592, Validation Accuracy: 66.76666666666667\n",
            "End of Epoch 184, Training Loss: 1.1996201105428907, Training Accuracy: 64.25185185185185\n",
            "End of Epoch 184, Validation Loss: 1.1667433324428707, Validation Accuracy: 67.06666666666666\n",
            "End of Epoch 185, Training Loss: 1.1922072345368386, Training Accuracy: 64.46666666666667\n",
            "End of Epoch 185, Validation Loss: 1.1589558390144616, Validation Accuracy: 67.36666666666666\n",
            "End of Epoch 186, Training Loss: 1.1848866337067154, Training Accuracy: 64.67407407407407\n",
            "End of Epoch 186, Validation Loss: 1.151268145200494, Validation Accuracy: 67.60000000000001\n",
            "End of Epoch 187, Training Loss: 1.1776600514923012, Training Accuracy: 64.84814814814814\n",
            "End of Epoch 187, Validation Loss: 1.1436817872498266, Validation Accuracy: 67.85\n",
            "End of Epoch 188, Training Loss: 1.1705287447224695, Training Accuracy: 65.03888888888889\n",
            "End of Epoch 188, Validation Loss: 1.136197798512522, Validation Accuracy: 68.08333333333333\n",
            "End of Epoch 189, Training Loss: 1.1634935094727903, Training Accuracy: 65.26111111111112\n",
            "End of Epoch 189, Validation Loss: 1.1288167411313579, Validation Accuracy: 68.33333333333333\n",
            "End of Epoch 190, Training Loss: 1.1565547120019282, Training Accuracy: 65.46296296296296\n",
            "End of Epoch 190, Validation Loss: 1.1215387421878151, Validation Accuracy: 68.60000000000001\n",
            "End of Epoch 191, Training Loss: 1.1497123233811386, Training Accuracy: 65.67777777777778\n",
            "End of Epoch 191, Validation Loss: 1.1143635328650456, Validation Accuracy: 68.71666666666667\n",
            "End of Epoch 192, Training Loss: 1.142965956538742, Training Accuracy: 65.87037037037037\n",
            "End of Epoch 192, Validation Loss: 1.1072904893333535, Validation Accuracy: 69.11666666666667\n",
            "End of Epoch 193, Training Loss: 1.136314904575241, Training Accuracy: 66.06851851851852\n",
            "End of Epoch 193, Validation Loss: 1.1003186742259474, Validation Accuracy: 69.38333333333333\n",
            "End of Epoch 194, Training Loss: 1.1297581793542553, Training Accuracy: 66.24814814814815\n",
            "End of Epoch 194, Validation Loss: 1.0934468777436928, Validation Accuracy: 69.65\n",
            "End of Epoch 195, Training Loss: 1.1232945495303053, Training Accuracy: 66.4388888888889\n",
            "End of Epoch 195, Validation Loss: 1.086673657598639, Validation Accuracy: 69.95\n",
            "End of Epoch 196, Training Loss: 1.1169225773289793, Training Accuracy: 66.5925925925926\n",
            "End of Epoch 196, Validation Loss: 1.0799973771702533, Validation Accuracy: 70.15\n",
            "End of Epoch 197, Training Loss: 1.1106406535422135, Training Accuracy: 66.75740740740741\n",
            "End of Epoch 197, Validation Loss: 1.0734162414005446, Validation Accuracy: 70.41666666666667\n",
            "End of Epoch 198, Training Loss: 1.1044470303369944, Training Accuracy: 66.91481481481482\n",
            "End of Epoch 198, Validation Loss: 1.0669283300912473, Validation Accuracy: 70.63333333333334\n",
            "End of Epoch 199, Training Loss: 1.0983398515970213, Training Accuracy: 67.07037037037037\n",
            "End of Epoch 199, Validation Loss: 1.0605316283861175, Validation Accuracy: 70.81666666666668\n",
            "End of Epoch 200, Training Loss: 1.0923171806224399, Training Accuracy: 67.23703703703704\n",
            "End of Epoch 200, Validation Loss: 1.0542240543235937, Validation Accuracy: 71.01666666666667\n",
            "End of Epoch 201, Training Loss: 1.0863770251021918, Training Accuracy: 67.4074074074074\n",
            "End of Epoch 201, Validation Loss: 1.0480034834299405, Validation Accuracy: 71.18333333333334\n",
            "End of Epoch 202, Training Loss: 1.0805173593475197, Training Accuracy: 67.57777777777778\n",
            "End of Epoch 202, Validation Loss: 1.0418677703915604, Validation Accuracy: 71.41666666666666\n",
            "End of Epoch 203, Training Loss: 1.0747361438345875, Training Accuracy: 67.71851851851852\n",
            "End of Epoch 203, Validation Loss: 1.0358147678989373, Validation Accuracy: 71.56666666666666\n",
            "End of Epoch 204, Training Loss: 1.0690313421505167, Training Accuracy: 67.89444444444445\n",
            "End of Epoch 204, Validation Loss: 1.0298423427953478, Validation Accuracy: 71.8\n",
            "End of Epoch 205, Training Loss: 1.0634009354719764, Training Accuracy: 68.0611111111111\n",
            "End of Epoch 205, Validation Loss: 1.023948389692862, Validation Accuracy: 71.96666666666667\n",
            "End of Epoch 206, Training Loss: 1.0578429347303622, Training Accuracy: 68.20925925925926\n",
            "End of Epoch 206, Validation Loss: 1.018130842238009, Validation Accuracy: 72.1\n",
            "End of Epoch 207, Training Loss: 1.0523553906342014, Training Accuracy: 68.37777777777778\n",
            "End of Epoch 207, Validation Loss: 1.012387682221459, Validation Accuracy: 72.28333333333333\n",
            "End of Epoch 208, Training Loss: 1.0469364017291287, Training Accuracy: 68.5037037037037\n",
            "End of Epoch 208, Validation Loss: 1.0067169467316204, Validation Accuracy: 72.45\n",
            "End of Epoch 209, Training Loss: 1.0415841206799181, Training Accuracy: 68.66296296296296\n",
            "End of Epoch 209, Validation Loss: 1.0011167335525366, Validation Accuracy: 72.58333333333333\n",
            "End of Epoch 210, Training Loss: 1.0362967589587837, Training Accuracy: 68.87222222222222\n",
            "End of Epoch 210, Validation Loss: 0.995585205002941, Validation Accuracy: 72.75\n",
            "End of Epoch 211, Training Loss: 1.0310725901204507, Training Accuracy: 69.02407407407406\n",
            "End of Epoch 211, Validation Loss: 0.9901205904067714, Validation Accuracy: 72.96666666666667\n",
            "End of Epoch 212, Training Loss: 1.0259099518381447, Training Accuracy: 69.18888888888888\n",
            "End of Epoch 212, Validation Loss: 0.9847211873766359, Validation Accuracy: 73.15\n",
            "End of Epoch 213, Training Loss: 1.0208072468663636, Training Accuracy: 69.32592592592593\n",
            "End of Epoch 213, Validation Loss: 0.9793853620812671, Validation Accuracy: 73.25\n",
            "End of Epoch 214, Training Loss: 1.0157629430866373, Training Accuracy: 69.46666666666667\n",
            "End of Epoch 214, Validation Loss: 0.974111548656459, Validation Accuracy: 73.4\n",
            "End of Epoch 215, Training Loss: 1.0107755727818246, Training Accuracy: 69.60925925925926\n",
            "End of Epoch 215, Validation Loss: 0.9688982479067519, Validation Accuracy: 73.65\n",
            "End of Epoch 216, Training Loss: 1.005843731273349, Training Accuracy: 69.74814814814815\n",
            "End of Epoch 216, Validation Loss: 0.963744025432534, Validation Accuracy: 73.83333333333333\n",
            "End of Epoch 217, Training Loss: 1.0009660750442673, Training Accuracy: 69.85185185185185\n",
            "End of Epoch 217, Validation Loss: 0.958647509304583, Validation Accuracy: 73.91666666666666\n",
            "End of Epoch 218, Training Loss: 0.996141319459534, Training Accuracy: 69.99444444444445\n",
            "End of Epoch 218, Validation Loss: 0.9536073873955395, Validation Accuracy: 74.05000000000001\n",
            "End of Epoch 219, Training Loss: 0.991368236183449, Training Accuracy: 70.13703703703705\n",
            "End of Epoch 219, Validation Loss: 0.9486224044656145, Validation Accuracy: 74.13333333333333\n",
            "End of Epoch 220, Training Loss: 0.986645650383168, Training Accuracy: 70.2962962962963\n",
            "End of Epoch 220, Validation Loss: 0.9436913590880731, Validation Accuracy: 74.21666666666667\n",
            "End of Epoch 221, Training Loss: 0.9819724377964686, Training Accuracy: 70.42592592592592\n",
            "End of Epoch 221, Validation Loss: 0.9388131004888811, Validation Accuracy: 74.31666666666666\n",
            "End of Epoch 222, Training Loss: 0.9773475217318008, Training Accuracy: 70.58518518518518\n",
            "End of Epoch 222, Validation Loss: 0.9339865253643579, Validation Accuracy: 74.38333333333334\n",
            "End of Epoch 223, Training Loss: 0.9727698700590791, Training Accuracy: 70.72777777777777\n",
            "End of Epoch 223, Validation Loss: 0.9292105747308838, Validation Accuracy: 74.55000000000001\n",
            "End of Epoch 224, Training Loss: 0.9682384922407232, Training Accuracy: 70.87037037037037\n",
            "End of Epoch 224, Validation Loss: 0.9244842308516542, Validation Accuracy: 74.85000000000001\n",
            "End of Epoch 225, Training Loss: 0.9637524364442278, Training Accuracy: 70.99074074074075\n",
            "End of Epoch 225, Validation Loss: 0.9198065142771963, Validation Accuracy: 74.95\n",
            "End of Epoch 226, Training Loss: 0.9593107867699819, Training Accuracy: 71.10740740740741\n",
            "End of Epoch 226, Validation Loss: 0.9151764810288738, Validation Accuracy: 75.03333333333333\n",
            "End of Epoch 227, Training Loss: 0.9549126606212242, Training Accuracy: 71.25555555555556\n",
            "End of Epoch 227, Validation Loss: 0.9105932199478823, Validation Accuracy: 75.2\n",
            "End of Epoch 228, Training Loss: 0.9505572062368794, Training Accuracy: 71.37962962962962\n",
            "End of Epoch 228, Validation Loss: 0.9060558502262549, Validation Accuracy: 75.28333333333333\n",
            "End of Epoch 229, Training Loss: 0.9462436004025228, Training Accuracy: 71.50555555555556\n",
            "End of Epoch 229, Validation Loss: 0.9015635191311204, Validation Accuracy: 75.46666666666667\n",
            "End of Epoch 230, Training Loss: 0.9419710463498637, Training Accuracy: 71.6351851851852\n",
            "End of Epoch 230, Validation Loss: 0.8971153999288309, Validation Accuracy: 75.56666666666668\n",
            "End of Epoch 231, Training Loss: 0.9377387718508324, Training Accuracy: 71.78148148148148\n",
            "End of Epoch 231, Validation Loss: 0.8927106900115425, Validation Accuracy: 75.63333333333333\n",
            "End of Epoch 232, Training Loss: 0.9335460275085538, Training Accuracy: 71.90555555555555\n",
            "End of Epoch 232, Validation Loss: 0.8883486092253299, Validation Accuracy: 75.75\n",
            "End of Epoch 233, Training Loss: 0.9293920852441251, Training Accuracy: 72.04814814814814\n",
            "End of Epoch 233, Validation Loss: 0.8840283983958838, Validation Accuracy: 75.85\n",
            "End of Epoch 234, Training Loss: 0.9252762369751302, Training Accuracy: 72.1925925925926\n",
            "End of Epoch 234, Validation Loss: 0.8797493180452027, Validation Accuracy: 75.96666666666667\n",
            "End of Epoch 235, Training Loss: 0.9211977934790965, Training Accuracy: 72.2925925925926\n",
            "End of Epoch 235, Validation Loss: 0.8755106472904006, Validation Accuracy: 76.08333333333334\n",
            "End of Epoch 236, Training Loss: 0.9171560834326836, Training Accuracy: 72.46666666666667\n",
            "End of Epoch 236, Validation Loss: 0.8713116829137537, Validation Accuracy: 76.26666666666667\n",
            "End of Epoch 237, Training Loss: 0.9131504526151151, Training Accuracy: 72.59629629629629\n",
            "End of Epoch 237, Validation Loss: 0.8671517385913577, Validation Accuracy: 76.31666666666666\n",
            "End of Epoch 238, Training Loss: 0.909180263262285, Training Accuracy: 72.71481481481482\n",
            "End of Epoch 238, Validation Loss: 0.8630301442662487, Validation Accuracy: 76.4\n",
            "End of Epoch 239, Training Loss: 0.9052448935560823, Training Accuracy: 72.84074074074074\n",
            "End of Epoch 239, Validation Loss: 0.8589462456505198, Validation Accuracy: 76.46666666666667\n",
            "End of Epoch 240, Training Loss: 0.9013437372316959, Training Accuracy: 72.97222222222223\n",
            "End of Epoch 240, Validation Loss: 0.8548994038398761, Validation Accuracy: 76.44999999999999\n",
            "End of Epoch 241, Training Loss: 0.8974762032841967, Training Accuracy: 73.1\n",
            "End of Epoch 241, Validation Loss: 0.8508889950232031, Validation Accuracy: 76.55\n",
            "End of Epoch 242, Training Loss: 0.8936417157544135, Training Accuracy: 73.22777777777777\n",
            "End of Epoch 242, Validation Loss: 0.8469144102691245, Validation Accuracy: 76.61666666666666\n",
            "End of Epoch 243, Training Loss: 0.889839713573271, Training Accuracy: 73.36481481481482\n",
            "End of Epoch 243, Validation Loss: 0.8429750553712418, Validation Accuracy: 76.8\n",
            "End of Epoch 244, Training Loss: 0.8860696504432984, Training Accuracy: 73.50740740740741\n",
            "End of Epoch 244, Validation Loss: 0.8390703507338, Validation Accuracy: 76.91666666666667\n",
            "End of Epoch 245, Training Loss: 0.8823309947361422, Training Accuracy: 73.66481481481482\n",
            "End of Epoch 245, Validation Loss: 0.8351997312799928, Validation Accuracy: 77.11666666666666\n",
            "End of Epoch 246, Training Loss: 0.8786232293856212, Training Accuracy: 73.77962962962962\n",
            "End of Epoch 246, Validation Loss: 0.8313626463660071, Validation Accuracy: 77.23333333333333\n",
            "End of Epoch 247, Training Loss: 0.8749458517572721, Training Accuracy: 73.9037037037037\n",
            "End of Epoch 247, Validation Loss: 0.8275585596852522, Validation Accuracy: 77.36666666666666\n",
            "End of Epoch 248, Training Loss: 0.871298373477433, Training Accuracy: 74.04444444444445\n",
            "End of Epoch 248, Validation Loss: 0.8237869491490167, Validation Accuracy: 77.5\n",
            "End of Epoch 249, Training Loss: 0.8676803202077183, Training Accuracy: 74.1537037037037\n",
            "End of Epoch 249, Validation Loss: 0.820047306732003, Validation Accuracy: 77.66666666666666\n",
            "End of Epoch 250, Training Loss: 0.8640912313541559, Training Accuracy: 74.3037037037037\n",
            "End of Epoch 250, Validation Loss: 0.8163391382737714, Validation Accuracy: 77.78333333333333\n",
            "End of Epoch 251, Training Loss: 0.8605306597041464, Training Accuracy: 74.39444444444445\n",
            "End of Epoch 251, Validation Loss: 0.8126619632299555, Validation Accuracy: 77.85\n",
            "End of Epoch 252, Training Loss: 0.8569981709886924, Training Accuracy: 74.52407407407408\n",
            "End of Epoch 252, Validation Loss: 0.8090153143701173, Validation Accuracy: 77.98333333333333\n",
            "End of Epoch 253, Training Loss: 0.853493343371651, Training Accuracy: 74.66481481481482\n",
            "End of Epoch 253, Validation Loss: 0.8053987374221075, Validation Accuracy: 78.08333333333334\n",
            "End of Epoch 254, Training Loss: 0.8500157668720179, Training Accuracy: 74.78148148148148\n",
            "End of Epoch 254, Validation Loss: 0.80181179066569, Validation Accuracy: 78.25\n",
            "End of Epoch 255, Training Loss: 0.8465650427290745, Training Accuracy: 74.92037037037036\n",
            "End of Epoch 255, Validation Loss: 0.7982540444807862, Validation Accuracy: 78.4\n",
            "End of Epoch 256, Training Loss: 0.8431407827235021, Training Accuracy: 75.05\n",
            "End of Epoch 256, Validation Loss: 0.7947250808579082, Validation Accuracy: 78.5\n",
            "End of Epoch 257, Training Loss: 0.8397426084699733, Training Accuracy: 75.16851851851852\n",
            "End of Epoch 257, Validation Loss: 0.7912244928800333, Validation Accuracy: 78.56666666666666\n",
            "End of Epoch 258, Training Loss: 0.8363701506982781, Training Accuracy: 75.29629629629629\n",
            "End of Epoch 258, Validation Loss: 0.7877518841862933, Validation Accuracy: 78.68333333333334\n",
            "End of Epoch 259, Training Loss: 0.8330230485405193, Training Accuracy: 75.39814814814815\n",
            "End of Epoch 259, Validation Loss: 0.7843068684283336, Validation Accuracy: 78.83333333333333\n",
            "End of Epoch 260, Training Loss: 0.8297009488414335, Training Accuracy: 75.50740740740741\n",
            "End of Epoch 260, Validation Loss: 0.7808890687300946, Validation Accuracy: 78.9\n",
            "End of Epoch 261, Training Loss: 0.8264035055074851, Training Accuracy: 75.63333333333333\n",
            "End of Epoch 261, Validation Loss: 0.7774981171610842, Validation Accuracy: 79.01666666666667\n",
            "End of Epoch 262, Training Loss: 0.823130378908196, Training Accuracy: 75.73703703703704\n",
            "End of Epoch 262, Validation Loss: 0.7741336542320679, Validation Accuracy: 79.03333333333333\n",
            "End of Epoch 263, Training Loss: 0.819881235340401, Training Accuracy: 75.87222222222222\n",
            "End of Epoch 263, Validation Loss: 0.7707953284205719, Validation Accuracy: 79.14999999999999\n",
            "End of Epoch 264, Training Loss: 0.8166557465629858, Training Accuracy: 76.00555555555556\n",
            "End of Epoch 264, Validation Loss: 0.7674827957318298, Validation Accuracy: 79.4\n",
            "End of Epoch 265, Training Loss: 0.8134535894063777, Training Accuracy: 76.10925925925926\n",
            "End of Epoch 265, Validation Loss: 0.7641957192989199, Validation Accuracy: 79.55\n",
            "End of Epoch 266, Training Loss: 0.810274445457847, Training Accuracy: 76.2537037037037\n",
            "End of Epoch 266, Validation Loss: 0.7609337690239426, Validation Accuracy: 79.66666666666666\n",
            "End of Epoch 267, Training Loss: 0.8071180008207071, Training Accuracy: 76.38148148148149\n",
            "End of Epoch 267, Validation Loss: 0.7576966212603247, Validation Accuracy: 79.73333333333333\n",
            "End of Epoch 268, Training Loss: 0.8039839459429348, Training Accuracy: 76.48703703703704\n",
            "End of Epoch 268, Validation Loss: 0.7544839585347491, Validation Accuracy: 79.76666666666667\n",
            "End of Epoch 269, Training Loss: 0.8008719755086512, Training Accuracy: 76.6\n",
            "End of Epoch 269, Validation Loss: 0.7512954693058974, Validation Accuracy: 79.78333333333333\n",
            "End of Epoch 270, Training Loss: 0.7977817883843785, Training Accuracy: 76.7388888888889\n",
            "End of Epoch 270, Validation Loss: 0.7481308477561671, Validation Accuracy: 79.88333333333333\n",
            "End of Epoch 271, Training Loss: 0.7947130876109901, Training Accuracy: 76.86111111111111\n",
            "End of Epoch 271, Validation Loss: 0.7449897936118178, Validation Accuracy: 79.93333333333334\n",
            "End of Epoch 272, Training Loss: 0.7916655804318352, Training Accuracy: 76.99814814814815\n",
            "End of Epoch 272, Validation Loss: 0.7418720119865941, Validation Accuracy: 80.08333333333333\n",
            "End of Epoch 273, Training Loss: 0.7886389783475148, Training Accuracy: 77.10555555555555\n",
            "End of Epoch 273, Validation Loss: 0.7387772132437397, Validation Accuracy: 80.11666666666667\n",
            "End of Epoch 274, Training Loss: 0.7856329971882041, Training Accuracy: 77.21666666666667\n",
            "End of Epoch 274, Validation Loss: 0.735705112871435, Validation Accuracy: 80.18333333333332\n",
            "End of Epoch 275, Training Loss: 0.7826473571951137, Training Accuracy: 77.3425925925926\n",
            "End of Epoch 275, Validation Loss: 0.7326554313669892, Validation Accuracy: 80.38333333333333\n",
            "End of Epoch 276, Training Loss: 0.7796817831036403, Training Accuracy: 77.45\n",
            "End of Epoch 276, Validation Loss: 0.7296278941255897, Validation Accuracy: 80.51666666666667\n",
            "End of Epoch 277, Training Loss: 0.7767360042218153, Training Accuracy: 77.58333333333334\n",
            "End of Epoch 277, Validation Loss: 0.7266222313299575, Validation Accuracy: 80.66666666666666\n",
            "End of Epoch 278, Training Loss: 0.773809754498819, Training Accuracy: 77.70185185185186\n",
            "End of Epoch 278, Validation Loss: 0.7236381778378921, Validation Accuracy: 80.80000000000001\n",
            "End of Epoch 279, Training Loss: 0.770902772579472, Training Accuracy: 77.81296296296296\n",
            "End of Epoch 279, Validation Loss: 0.7206754730653258, Validation Accuracy: 81.0\n",
            "End of Epoch 280, Training Loss: 0.7680148018417189, Training Accuracy: 77.91851851851852\n",
            "End of Epoch 280, Validation Loss: 0.7177338608631535, Validation Accuracy: 81.16666666666667\n",
            "End of Epoch 281, Training Loss: 0.765145590415162, Training Accuracy: 78.0537037037037\n",
            "End of Epoch 281, Validation Loss: 0.714813089386703, Validation Accuracy: 81.23333333333333\n",
            "End of Epoch 282, Training Loss: 0.762294891179616, Training Accuracy: 78.14999999999999\n",
            "End of Epoch 282, Validation Loss: 0.7119129109572756, Validation Accuracy: 81.36666666666666\n",
            "End of Epoch 283, Training Loss: 0.7594624617434625, Training Accuracy: 78.26666666666667\n",
            "End of Epoch 283, Validation Loss: 0.7090330819156848, Validation Accuracy: 81.5\n",
            "End of Epoch 284, Training Loss: 0.7566480644022765, Training Accuracy: 78.33703703703704\n",
            "End of Epoch 284, Validation Loss: 0.7061733624681525, Validation Accuracy: 81.6\n",
            "End of Epoch 285, Training Loss: 0.753851466078747, Training Accuracy: 78.44444444444446\n",
            "End of Epoch 285, Validation Loss: 0.7033335165252941, Validation Accuracy: 81.71666666666667\n",
            "End of Epoch 286, Training Loss: 0.7510724382453811, Training Accuracy: 78.58703703703704\n",
            "End of Epoch 286, Validation Loss: 0.7005133115352193, Validation Accuracy: 81.78333333333333\n",
            "End of Epoch 287, Training Loss: 0.7483107568317936, Training Accuracy: 78.68703703703703\n",
            "End of Epoch 287, Validation Loss: 0.6977125183120199, Validation Accuracy: 81.8\n",
            "End of Epoch 288, Training Loss: 0.7455662021186706, Training Accuracy: 78.78703703703704\n",
            "End of Epoch 288, Validation Loss: 0.6949309108610946, Validation Accuracy: 81.95\n",
            "End of Epoch 289, Training Loss: 0.742838558620614, Training Accuracy: 78.89259259259259\n",
            "End of Epoch 289, Validation Loss: 0.6921682662028993, Validation Accuracy: 82.06666666666666\n",
            "End of Epoch 290, Training Loss: 0.7401276149602096, Training Accuracy: 79.0\n",
            "End of Epoch 290, Validation Loss: 0.6894243641967882, Validation Accuracy: 82.13333333333334\n",
            "End of Epoch 291, Training Loss: 0.7374331637356747, Training Accuracy: 79.10555555555555\n",
            "End of Epoch 291, Validation Loss: 0.6866989873666706, Validation Accuracy: 82.18333333333334\n",
            "End of Epoch 292, Training Loss: 0.7347550013844262, Training Accuracy: 79.19259259259259\n",
            "End of Epoch 292, Validation Loss: 0.683991920730221, Validation Accuracy: 82.28333333333333\n",
            "End of Epoch 293, Training Loss: 0.7320929280448782, Training Accuracy: 79.28148148148149\n",
            "End of Epoch 293, Validation Loss: 0.6813029516333661, Validation Accuracy: 82.46666666666667\n",
            "End of Epoch 294, Training Loss: 0.7294467474186628, Training Accuracy: 79.37407407407407\n",
            "End of Epoch 294, Validation Loss: 0.6786318695917406, Validation Accuracy: 82.56666666666666\n",
            "End of Epoch 295, Training Loss: 0.7268162666353976, Training Accuracy: 79.45\n",
            "End of Epoch 295, Validation Loss: 0.6759784661407512, Validation Accuracy: 82.66666666666667\n",
            "End of Epoch 296, Training Loss: 0.7242012961219625, Training Accuracy: 79.5537037037037\n",
            "End of Epoch 296, Validation Loss: 0.6733425346958095, Validation Accuracy: 82.76666666666667\n",
            "End of Epoch 297, Training Loss: 0.7216016494781248, Training Accuracy: 79.64444444444445\n",
            "End of Epoch 297, Validation Loss: 0.6707238704242171, Validation Accuracy: 82.86666666666666\n",
            "End of Epoch 298, Training Loss: 0.719017143360194, Training Accuracy: 79.75185185185185\n",
            "End of Epoch 298, Validation Loss: 0.6681222701300834, Validation Accuracy: 82.95\n",
            "End of Epoch 299, Training Loss: 0.7164475973742217, Training Accuracy: 79.82777777777777\n",
            "End of Epoch 299, Validation Loss: 0.6655375321535423, Validation Accuracy: 83.05\n",
            "End of Epoch 300, Training Loss: 0.7138928339800904, Training Accuracy: 79.92407407407407\n",
            "End of Epoch 300, Validation Loss: 0.6629694562854196, Validation Accuracy: 83.13333333333334\n",
            "End of Epoch 301, Training Loss: 0.711352678407664, Training Accuracy: 80.04444444444444\n",
            "End of Epoch 301, Validation Loss: 0.6604178436983685, Validation Accuracy: 83.26666666666667\n",
            "End of Epoch 302, Training Loss: 0.7088269585859837, Training Accuracy: 80.13703703703705\n",
            "End of Epoch 302, Validation Loss: 0.6578824968953451, Validation Accuracy: 83.33333333333334\n",
            "End of Epoch 303, Training Loss: 0.7063155050863138, Training Accuracy: 80.24814814814815\n",
            "End of Epoch 303, Validation Loss: 0.6553632196761566, Validation Accuracy: 83.33333333333334\n",
            "End of Epoch 304, Training Loss: 0.7038181510796531, Training Accuracy: 80.34074074074074\n",
            "End of Epoch 304, Validation Loss: 0.6528598171226432, Validation Accuracy: 83.43333333333334\n",
            "End of Epoch 305, Training Loss: 0.7013347323091257, Training Accuracy: 80.43148148148148\n",
            "End of Epoch 305, Validation Loss: 0.6503720956028993, Validation Accuracy: 83.48333333333333\n",
            "End of Epoch 306, Training Loss: 0.6988650870774936, Training Accuracy: 80.52037037037037\n",
            "End of Epoch 306, Validation Loss: 0.6478998627947615, Validation Accuracy: 83.6\n",
            "End of Epoch 307, Training Loss: 0.6964090562498151, Training Accuracy: 80.64444444444445\n",
            "End of Epoch 307, Validation Loss: 0.6454429277286107, Validation Accuracy: 83.65\n",
            "End of Epoch 308, Training Loss: 0.6939664832711115, Training Accuracy: 80.76666666666667\n",
            "End of Epoch 308, Validation Loss: 0.6430011008493509, Validation Accuracy: 83.71666666666667\n",
            "End of Epoch 309, Training Loss: 0.6915372141986774, Training Accuracy: 80.87592592592593\n",
            "End of Epoch 309, Validation Loss: 0.6405741940972448, Validation Accuracy: 83.8\n",
            "End of Epoch 310, Training Loss: 0.6891210977485271, Training Accuracy: 80.95185185185186\n",
            "End of Epoch 310, Validation Loss: 0.6381620210070863, Validation Accuracy: 83.91666666666666\n",
            "End of Epoch 311, Training Loss: 0.6867179853552456, Training Accuracy: 81.04259259259258\n",
            "End of Epoch 311, Validation Loss: 0.6357643968250077, Validation Accuracy: 83.95\n",
            "End of Epoch 312, Training Loss: 0.6843277312443798, Training Accuracy: 81.10555555555555\n",
            "End of Epoch 312, Validation Loss: 0.6333811386420276, Validation Accuracy: 84.03333333333333\n",
            "End of Epoch 313, Training Loss: 0.6819501925163054, Training Accuracy: 81.19259259259259\n",
            "End of Epoch 313, Validation Loss: 0.6310120655432596, Validation Accuracy: 84.06666666666666\n",
            "End of Epoch 314, Training Loss: 0.6795852292403809, Training Accuracy: 81.27407407407408\n",
            "End of Epoch 314, Validation Loss: 0.6286569987715244, Validation Accuracy: 84.03333333333333\n",
            "End of Epoch 315, Training Loss: 0.6772327045580427, Training Accuracy: 81.37962962962963\n",
            "End of Epoch 315, Validation Loss: 0.6263157619039387, Validation Accuracy: 84.11666666666666\n",
            "End of Epoch 316, Training Loss: 0.6748924847933775, Training Accuracy: 81.5037037037037\n",
            "End of Epoch 316, Validation Loss: 0.623988181039889, Validation Accuracy: 84.2\n",
            "End of Epoch 317, Training Loss: 0.6725644395695985, Training Accuracy: 81.5962962962963\n",
            "End of Epoch 317, Validation Loss: 0.6216740849986608, Validation Accuracy: 84.26666666666667\n",
            "End of Epoch 318, Training Loss: 0.670248441929766, Training Accuracy: 81.7074074074074\n",
            "End of Epoch 318, Validation Loss: 0.619373305524849, Validation Accuracy: 84.35000000000001\n",
            "End of Epoch 319, Training Loss: 0.6679443684600176, Training Accuracy: 81.81111111111112\n",
            "End of Epoch 319, Validation Loss: 0.6170856774995711, Validation Accuracy: 84.46666666666667\n",
            "End of Epoch 320, Training Loss: 0.6656520994135315, Training Accuracy: 81.87962962962962\n",
            "End of Epoch 320, Validation Loss: 0.6148110391554036, Validation Accuracy: 84.51666666666667\n",
            "End of Epoch 321, Training Loss: 0.6633715188334217, Training Accuracy: 81.95925925925926\n",
            "End of Epoch 321, Validation Loss: 0.6125492322928832, Validation Accuracy: 84.61666666666666\n",
            "End of Epoch 322, Training Loss: 0.6611025146727417, Training Accuracy: 82.03703703703704\n",
            "End of Epoch 322, Validation Loss: 0.6103001024963691, Validation Accuracy: 84.7\n",
            "End of Epoch 323, Training Loss: 0.6588449789098317, Training Accuracy: 82.1037037037037\n",
            "End of Epoch 323, Validation Loss: 0.6080634993470182, Validation Accuracy: 84.75\n",
            "End of Epoch 324, Training Loss: 0.6565988076572377, Training Accuracy: 82.16296296296296\n",
            "End of Epoch 324, Validation Loss: 0.605839276630633, Validation Accuracy: 84.81666666666666\n",
            "End of Epoch 325, Training Loss: 0.6543639012625306, Training Accuracy: 82.23703703703704\n",
            "End of Epoch 325, Validation Loss: 0.603627292538147, Validation Accuracy: 84.93333333333334\n",
            "End of Epoch 326, Training Loss: 0.6521401643994258, Training Accuracy: 82.2962962962963\n",
            "End of Epoch 326, Validation Loss: 0.6014274098565611, Validation Accuracy: 85.0\n",
            "End of Epoch 327, Training Loss: 0.6499275061476971, Training Accuracy: 82.36296296296295\n",
            "End of Epoch 327, Validation Loss: 0.5992394961482111, Validation Accuracy: 85.1\n",
            "End of Epoch 328, Training Loss: 0.6477258400605218, Training Accuracy: 82.42407407407407\n",
            "End of Epoch 328, Validation Loss: 0.5970634239163379, Validation Accuracy: 85.15\n",
            "End of Epoch 329, Training Loss: 0.645535084218011, Training Accuracy: 82.52777777777777\n",
            "End of Epoch 329, Validation Loss: 0.5948990707550473, Validation Accuracy: 85.18333333333334\n",
            "End of Epoch 330, Training Loss: 0.6433551612658484, Training Accuracy: 82.62777777777778\n",
            "End of Epoch 330, Validation Loss: 0.5927463194818885, Validation Accuracy: 85.25\n",
            "End of Epoch 331, Training Loss: 0.6411859984381058, Training Accuracy: 82.71481481481482\n",
            "End of Epoch 331, Validation Loss: 0.5906050582514434, Validation Accuracy: 85.35000000000001\n",
            "End of Epoch 332, Training Loss: 0.6390275275635093, Training Accuracy: 82.78148148148148\n",
            "End of Epoch 332, Validation Loss: 0.5884751806484991, Validation Accuracy: 85.41666666666666\n",
            "End of Epoch 333, Training Loss: 0.6368796850545679, Training Accuracy: 82.85555555555555\n",
            "End of Epoch 333, Validation Loss: 0.5863565857595777, Validation Accuracy: 85.45\n",
            "End of Epoch 334, Training Loss: 0.6347424118792093, Training Accuracy: 82.92407407407407\n",
            "End of Epoch 334, Validation Loss: 0.5842491782218233, Validation Accuracy: 85.5\n",
            "End of Epoch 335, Training Loss: 0.6326156535147226, Training Accuracy: 82.99629629629631\n",
            "End of Epoch 335, Validation Loss: 0.5821528682484691, Validation Accuracy: 85.55\n",
            "End of Epoch 336, Training Loss: 0.6304993598840161, Training Accuracy: 83.0537037037037\n",
            "End of Epoch 336, Validation Loss: 0.580067571630361, Validation Accuracy: 85.68333333333334\n",
            "End of Epoch 337, Training Loss: 0.628393485274382, Training Accuracy: 83.15\n",
            "End of Epoch 337, Validation Loss: 0.57799320971326, Validation Accuracy: 85.73333333333333\n",
            "End of Epoch 338, Training Loss: 0.6262979882391406, Training Accuracy: 83.22962962962963\n",
            "End of Epoch 338, Validation Loss: 0.5759297093509068, Validation Accuracy: 85.83333333333333\n",
            "End of Epoch 339, Training Loss: 0.6242128314827181, Training Accuracy: 83.31851851851852\n",
            "End of Epoch 339, Validation Loss: 0.5738770028340898, Validation Accuracy: 85.9\n",
            "End of Epoch 340, Training Loss: 0.622137981729883, Training Accuracy: 83.38703703703703\n",
            "End of Epoch 340, Validation Loss: 0.5718350277962075, Validation Accuracy: 86.08333333333333\n",
            "End of Epoch 341, Training Loss: 0.6200734095800134, Training Accuracy: 83.4574074074074\n",
            "End of Epoch 341, Validation Loss: 0.569803727096073, Validation Accuracy: 86.21666666666667\n",
            "End of Epoch 342, Training Loss: 0.6180190893474198, Training Accuracy: 83.52592592592593\n",
            "End of Epoch 342, Validation Loss: 0.5677830486789301, Validation Accuracy: 86.18333333333334\n",
            "End of Epoch 343, Training Loss: 0.6159749988888822, Training Accuracy: 83.61296296296297\n",
            "End of Epoch 343, Validation Loss: 0.5657729454168926, Validation Accuracy: 86.26666666666667\n",
            "End of Epoch 344, Training Loss: 0.6139411194196572, Training Accuracy: 83.6962962962963\n",
            "End of Epoch 344, Validation Loss: 0.5637733749301942, Validation Accuracy: 86.26666666666667\n",
            "End of Epoch 345, Training Loss: 0.6119174353193215, Training Accuracy: 83.76666666666667\n",
            "End of Epoch 345, Validation Loss: 0.5617842993908403, Validation Accuracy: 86.28333333333333\n",
            "End of Epoch 346, Training Loss: 0.609903933928882, Training Accuracy: 83.8351851851852\n",
            "End of Epoch 346, Validation Loss: 0.5598056853103921, Validation Accuracy: 86.3\n",
            "End of Epoch 347, Training Loss: 0.6079006053406438, Training Accuracy: 83.88518518518518\n",
            "End of Epoch 347, Validation Loss: 0.5578375033137453, Validation Accuracy: 86.38333333333334\n",
            "End of Epoch 348, Training Loss: 0.6059074421823581, Training Accuracy: 83.93888888888888\n",
            "End of Epoch 348, Validation Loss: 0.555879727900863, Validation Accuracy: 86.46666666666667\n",
            "End of Epoch 349, Training Loss: 0.6039244393971798, Training Accuracy: 83.96851851851852\n",
            "End of Epoch 349, Validation Loss: 0.5539323371984891, Validation Accuracy: 86.53333333333333\n",
            "End of Epoch 350, Training Loss: 0.6019515940209675, Training Accuracy: 84.03703703703704\n",
            "End of Epoch 350, Validation Loss: 0.5519953127039017, Validation Accuracy: 86.65\n",
            "End of Epoch 351, Training Loss: 0.5999889049584232, Training Accuracy: 84.11481481481482\n",
            "End of Epoch 351, Validation Loss: 0.5500686390227749, Validation Accuracy: 86.76666666666667\n",
            "End of Epoch 352, Training Loss: 0.5980363727595162, Training Accuracy: 84.17037037037038\n",
            "End of Epoch 352, Validation Loss: 0.5481523036031879, Validation Accuracy: 86.8\n",
            "End of Epoch 353, Training Loss: 0.5960939993975866, Training Accuracy: 84.22777777777777\n",
            "End of Epoch 353, Validation Loss: 0.5462462964677746, Validation Accuracy: 86.83333333333333\n",
            "End of Epoch 354, Training Loss: 0.5941617880504326, Training Accuracy: 84.30185185185185\n",
            "End of Epoch 354, Validation Loss: 0.5443506099459187, Validation Accuracy: 86.88333333333334\n",
            "End of Epoch 355, Training Loss: 0.5922397428855941, Training Accuracy: 84.34444444444445\n",
            "End of Epoch 355, Validation Loss: 0.5424652384078129, Validation Accuracy: 86.88333333333334\n",
            "End of Epoch 356, Training Loss: 0.5903278688509488, Training Accuracy: 84.39814814814814\n",
            "End of Epoch 356, Validation Loss: 0.5405901780020649, Validation Accuracy: 86.95\n",
            "End of Epoch 357, Training Loss: 0.5884261714716205, Training Accuracy: 84.46296296296296\n",
            "End of Epoch 357, Validation Loss: 0.5387254263984081, Validation Accuracy: 87.0\n",
            "End of Epoch 358, Training Loss: 0.586534656654079, Training Accuracy: 84.52777777777779\n",
            "End of Epoch 358, Validation Loss: 0.5368709825369242, Validation Accuracy: 87.06666666666666\n",
            "End of Epoch 359, Training Loss: 0.5846533304982044, Training Accuracy: 84.60185185185185\n",
            "End of Epoch 359, Validation Loss: 0.5350268463850233, Validation Accuracy: 87.1\n",
            "End of Epoch 360, Training Loss: 0.582782199117947, Training Accuracy: 84.67037037037038\n",
            "End of Epoch 360, Validation Loss: 0.5331930187032708, Validation Accuracy: 87.1\n",
            "End of Epoch 361, Training Loss: 0.5809212684711174, Training Accuracy: 84.73703703703703\n",
            "End of Epoch 361, Validation Loss: 0.5313695008209837, Validation Accuracy: 87.16666666666667\n",
            "End of Epoch 362, Training Loss: 0.5790705441987049, Training Accuracy: 84.8\n",
            "End of Epoch 362, Validation Loss: 0.5295562944223587, Validation Accuracy: 87.23333333333333\n",
            "End of Epoch 363, Training Loss: 0.5772300314740308, Training Accuracy: 84.85925925925926\n",
            "End of Epoch 363, Validation Loss: 0.5277534013437349, Validation Accuracy: 87.28333333333333\n",
            "End of Epoch 364, Training Loss: 0.5753997348619233, Training Accuracy: 84.93518518518518\n",
            "End of Epoch 364, Validation Loss: 0.5259608233824465, Validation Accuracy: 87.3\n",
            "End of Epoch 365, Training Loss: 0.5735796581880083, Training Accuracy: 84.98148148148148\n",
            "End of Epoch 365, Validation Loss: 0.5241785621175754, Validation Accuracy: 87.31666666666666\n",
            "End of Epoch 366, Training Loss: 0.5717698044181362, Training Accuracy: 85.02962962962964\n",
            "End of Epoch 366, Validation Loss: 0.5224066187427855, Validation Accuracy: 87.38333333333334\n",
            "End of Epoch 367, Training Loss: 0.5699701755478587, Training Accuracy: 85.09444444444443\n",
            "End of Epoch 367, Validation Loss: 0.5206449939112986, Validation Accuracy: 87.46666666666667\n",
            "End of Epoch 368, Training Loss: 0.5681807725018296, Training Accuracy: 85.1574074074074\n",
            "End of Epoch 368, Validation Loss: 0.5188936875929631, Validation Accuracy: 87.53333333333333\n",
            "End of Epoch 369, Training Loss: 0.566401595042918, Training Accuracy: 85.20925925925926\n",
            "End of Epoch 369, Validation Loss: 0.5171526989432748, Validation Accuracy: 87.55\n",
            "End of Epoch 370, Training Loss: 0.5646326416907872, Training Accuracy: 85.27222222222223\n",
            "End of Epoch 370, Validation Loss: 0.5154220261841255, Validation Accuracy: 87.63333333333333\n",
            "End of Epoch 371, Training Loss: 0.5628739096496417, Training Accuracy: 85.33333333333334\n",
            "End of Epoch 371, Validation Loss: 0.51370166649598, Validation Accuracy: 87.76666666666667\n",
            "End of Epoch 372, Training Loss: 0.561125394744808, Training Accuracy: 85.38148148148149\n",
            "End of Epoch 372, Validation Loss: 0.5119916159211324, Validation Accuracy: 87.81666666666666\n",
            "End of Epoch 373, Training Loss: 0.5593870913677997, Training Accuracy: 85.42777777777778\n",
            "End of Epoch 373, Validation Loss: 0.5102918692776371, Validation Accuracy: 87.85\n",
            "End of Epoch 374, Training Loss: 0.5576589924294739, Training Accuracy: 85.48333333333333\n",
            "End of Epoch 374, Validation Loss: 0.5086024200834787, Validation Accuracy: 87.91666666666667\n",
            "End of Epoch 375, Training Loss: 0.5559410893208995, Training Accuracy: 85.53518518518518\n",
            "End of Epoch 375, Validation Loss: 0.5069232604905133, Validation Accuracy: 87.94999999999999\n",
            "End of Epoch 376, Training Loss: 0.5542333718815202, Training Accuracy: 85.58518518518518\n",
            "End of Epoch 376, Validation Loss: 0.5052543812276956, Validation Accuracy: 87.98333333333333\n",
            "End of Epoch 377, Training Loss: 0.5525358283742183, Training Accuracy: 85.63333333333333\n",
            "End of Epoch 377, Validation Loss: 0.5035957715530972, Validation Accuracy: 88.08333333333334\n",
            "End of Epoch 378, Training Loss: 0.5508484454668588, Training Accuracy: 85.68148148148148\n",
            "End of Epoch 378, Validation Loss: 0.501947419214211, Validation Accuracy: 88.08333333333334\n",
            "End of Epoch 379, Training Loss: 0.5491712082199162, Training Accuracy: 85.72037037037038\n",
            "End of Epoch 379, Validation Loss: 0.5003093104160412, Validation Accuracy: 88.11666666666666\n",
            "End of Epoch 380, Training Loss: 0.5475041000797818, Training Accuracy: 85.77592592592592\n",
            "End of Epoch 380, Validation Loss: 0.4986814297964786, Validation Accuracy: 88.16666666666667\n",
            "End of Epoch 381, Training Loss: 0.545847102877363, Training Accuracy: 85.83703703703704\n",
            "End of Epoch 381, Validation Loss: 0.4970637604084779, Validation Accuracy: 88.2\n",
            "End of Epoch 382, Training Loss: 0.5442001968315895, Training Accuracy: 85.90555555555555\n",
            "End of Epoch 382, Validation Loss: 0.4954562837085524, Validation Accuracy: 88.23333333333333\n",
            "End of Epoch 383, Training Loss: 0.5425633605574609, Training Accuracy: 85.96851851851852\n",
            "End of Epoch 383, Validation Loss: 0.49385897955113006, Validation Accuracy: 88.23333333333333\n",
            "End of Epoch 384, Training Loss: 0.5409365710782784, Training Accuracy: 86.00555555555556\n",
            "End of Epoch 384, Validation Loss: 0.4922718261883188, Validation Accuracy: 88.26666666666667\n",
            "End of Epoch 385, Training Loss: 0.5393198038417165, Training Accuracy: 86.06111111111112\n",
            "End of Epoch 385, Validation Loss: 0.490694800274656, Validation Accuracy: 88.3\n",
            "End of Epoch 386, Training Loss: 0.5377130327394076, Training Accuracy: 86.12222222222222\n",
            "End of Epoch 386, Validation Loss: 0.4891278768764282, Validation Accuracy: 88.31666666666666\n",
            "End of Epoch 387, Training Loss: 0.5361162301297239, Training Accuracy: 86.17407407407407\n",
            "End of Epoch 387, Validation Loss: 0.4875710294851697, Validation Accuracy: 88.3\n",
            "End of Epoch 388, Training Loss: 0.534529366863458, Training Accuracy: 86.22592592592594\n",
            "End of Epoch 388, Validation Loss: 0.4860242300349704, Validation Accuracy: 88.33333333333333\n",
            "End of Epoch 389, Training Loss: 0.5329524123121162, Training Accuracy: 86.2611111111111\n",
            "End of Epoch 389, Validation Loss: 0.4844874489232353, Validation Accuracy: 88.36666666666667\n",
            "End of Epoch 390, Training Loss: 0.5313853343985578, Training Accuracy: 86.32222222222222\n",
            "End of Epoch 390, Validation Loss: 0.48296065503456515, Validation Accuracy: 88.4\n",
            "End of Epoch 391, Training Loss: 0.5298280996297206, Training Accuracy: 86.38148148148149\n",
            "End of Epoch 391, Validation Loss: 0.4814438157674427, Validation Accuracy: 88.41666666666667\n",
            "End of Epoch 392, Training Loss: 0.5282806731311911, Training Accuracy: 86.42037037037036\n",
            "End of Epoch 392, Validation Loss: 0.4799368970634273, Validation Accuracy: 88.5\n",
            "End of Epoch 393, Training Loss: 0.5267430186833978, Training Accuracy: 86.46481481481482\n",
            "End of Epoch 393, Validation Loss: 0.47843986343858, Validation Accuracy: 88.51666666666667\n",
            "End of Epoch 394, Training Loss: 0.525215098759204, Training Accuracy: 86.51296296296296\n",
            "End of Epoch 394, Validation Loss: 0.47695267801685837, Validation Accuracy: 88.55\n",
            "End of Epoch 395, Training Loss: 0.5236968745627096, Training Accuracy: 86.56666666666666\n",
            "End of Epoch 395, Validation Loss: 0.47547530256523735, Validation Accuracy: 88.58333333333334\n",
            "End of Epoch 396, Training Loss: 0.5221883060690646, Training Accuracy: 86.61111111111111\n",
            "End of Epoch 396, Validation Loss: 0.47400769753032773, Validation Accuracy: 88.6\n",
            "End of Epoch 397, Training Loss: 0.5206893520651241, Training Accuracy: 86.66296296296296\n",
            "End of Epoch 397, Validation Loss: 0.4725498220762787, Validation Accuracy: 88.64999999999999\n",
            "End of Epoch 398, Training Loss: 0.5191999701907773, Training Accuracy: 86.71111111111112\n",
            "End of Epoch 398, Validation Loss: 0.4711016341237702, Validation Accuracy: 88.64999999999999\n",
            "End of Epoch 399, Training Loss: 0.5177201169808007, Training Accuracy: 86.75\n",
            "End of Epoch 399, Validation Loss: 0.4696630903899075, Validation Accuracy: 88.66666666666667\n",
            "End of Epoch 400, Training Loss: 0.5162497479070897, Training Accuracy: 86.79629629629629\n",
            "End of Epoch 400, Validation Loss: 0.46823414642885125, Validation Accuracy: 88.71666666666667\n",
            "End of Epoch 401, Training Loss: 0.5147888174211402, Training Accuracy: 86.83518518518518\n",
            "End of Epoch 401, Validation Loss: 0.4668147566730249, Validation Accuracy: 88.76666666666667\n",
            "End of Epoch 402, Training Loss: 0.5133372789966593, Training Accuracy: 86.86481481481482\n",
            "End of Epoch 402, Validation Loss: 0.46540487447475326, Validation Accuracy: 88.83333333333333\n",
            "End of Epoch 403, Training Loss: 0.5118950851721861, Training Accuracy: 86.9\n",
            "End of Epoch 403, Validation Loss: 0.4640044521482015, Validation Accuracy: 88.86666666666667\n",
            "End of Epoch 404, Training Loss: 0.510462187593629, Training Accuracy: 86.92777777777778\n",
            "End of Epoch 404, Validation Loss: 0.4626134410114898, Validation Accuracy: 88.93333333333334\n",
            "End of Epoch 405, Training Loss: 0.5090385370566172, Training Accuracy: 86.96111111111111\n",
            "End of Epoch 405, Validation Loss: 0.4612317914288735, Validation Accuracy: 88.94999999999999\n",
            "End of Epoch 406, Training Loss: 0.5076240835485841, Training Accuracy: 87.00925925925927\n",
            "End of Epoch 406, Validation Loss: 0.4598594528528854, Validation Accuracy: 88.93333333333334\n",
            "End of Epoch 407, Training Loss: 0.5062187762904999, Training Accuracy: 87.05555555555556\n",
            "End of Epoch 407, Validation Loss: 0.45849637386634806, Validation Accuracy: 88.98333333333333\n",
            "End of Epoch 408, Training Loss: 0.5048225637781865, Training Accuracy: 87.08703703703704\n",
            "End of Epoch 408, Validation Loss: 0.4571425022241726, Validation Accuracy: 89.03333333333333\n",
            "End of Epoch 409, Training Loss: 0.5034353938231445, Training Accuracy: 87.10555555555555\n",
            "End of Epoch 409, Validation Loss: 0.45579778489486633, Validation Accuracy: 89.1\n",
            "End of Epoch 410, Training Loss: 0.5020572135928381, Training Accuracy: 87.15\n",
            "End of Epoch 410, Validation Loss: 0.45446216810168405, Validation Accuracy: 89.11666666666666\n",
            "End of Epoch 411, Training Loss: 0.5006879696503809, Training Accuracy: 87.21481481481482\n",
            "End of Epoch 411, Validation Loss: 0.4531355973633588, Validation Accuracy: 89.13333333333333\n",
            "End of Epoch 412, Training Loss: 0.4993276079935805, Training Accuracy: 87.24814814814815\n",
            "End of Epoch 412, Validation Loss: 0.4518180175343608, Validation Accuracy: 89.18333333333334\n",
            "End of Epoch 413, Training Loss: 0.4979760740932966, Training Accuracy: 87.27777777777777\n",
            "End of Epoch 413, Validation Loss: 0.45050937284463666, Validation Accuracy: 89.2\n",
            "End of Epoch 414, Training Loss: 0.49663331293107427, Training Accuracy: 87.31296296296296\n",
            "End of Epoch 414, Validation Loss: 0.4492096069387847, Validation Accuracy: 89.21666666666667\n",
            "End of Epoch 415, Training Loss: 0.4952992690360276, Training Accuracy: 87.33333333333333\n",
            "End of Epoch 415, Validation Loss: 0.44791866291463356, Validation Accuracy: 89.3\n",
            "End of Epoch 416, Training Loss: 0.4939738865209349, Training Accuracy: 87.36666666666667\n",
            "End of Epoch 416, Validation Loss: 0.4466364833611896, Validation Accuracy: 89.3\n",
            "End of Epoch 417, Training Loss: 0.4926571091175301, Training Accuracy: 87.39074074074074\n",
            "End of Epoch 417, Validation Loss: 0.4453630103959287, Validation Accuracy: 89.31666666666666\n",
            "End of Epoch 418, Training Loss: 0.4913488802109658, Training Accuracy: 87.42222222222222\n",
            "End of Epoch 418, Validation Loss: 0.44409818570140974, Validation Accuracy: 89.33333333333333\n",
            "End of Epoch 419, Training Loss: 0.4900491428734306, Training Accuracy: 87.4537037037037\n",
            "End of Epoch 419, Validation Loss: 0.44284195056118936, Validation Accuracy: 89.38333333333334\n",
            "End of Epoch 420, Training Loss: 0.4887578398969076, Training Accuracy: 87.48333333333333\n",
            "End of Epoch 420, Validation Loss: 0.4415942458950255, Validation Accuracy: 89.41666666666667\n",
            "End of Epoch 421, Training Loss: 0.4874749138250637, Training Accuracy: 87.50925925925927\n",
            "End of Epoch 421, Validation Loss: 0.4403550122933562, Validation Accuracy: 89.43333333333334\n",
            "End of Epoch 422, Training Loss: 0.4862003069842591, Training Accuracy: 87.54444444444445\n",
            "End of Epoch 422, Validation Loss: 0.43912419005104486, Validation Accuracy: 89.48333333333333\n",
            "End of Epoch 423, Training Loss: 0.48493396151367046, Training Accuracy: 87.59444444444443\n",
            "End of Epoch 423, Validation Loss: 0.4379017192003851, Validation Accuracy: 89.48333333333333\n",
            "End of Epoch 424, Training Loss: 0.48367581939452786, Training Accuracy: 87.6111111111111\n",
            "End of Epoch 424, Validation Loss: 0.4366875395433617, Validation Accuracy: 89.51666666666667\n",
            "End of Epoch 425, Training Loss: 0.4824258224784574, Training Accuracy: 87.63333333333333\n",
            "End of Epoch 425, Validation Loss: 0.4354815906831617, Validation Accuracy: 89.56666666666668\n",
            "End of Epoch 426, Training Loss: 0.4811839125149362, Training Accuracy: 87.65925925925926\n",
            "End of Epoch 426, Validation Loss: 0.43428381205493966, Validation Accuracy: 89.60000000000001\n",
            "End of Epoch 427, Training Loss: 0.4799500311778555, Training Accuracy: 87.6888888888889\n",
            "End of Epoch 427, Validation Loss: 0.43309414295583204, Validation Accuracy: 89.66666666666666\n",
            "End of Epoch 428, Training Loss: 0.4787241200911977, Training Accuracy: 87.72222222222223\n",
            "End of Epoch 428, Validation Loss: 0.43191252257422486, Validation Accuracy: 89.68333333333334\n",
            "End of Epoch 429, Training Loss: 0.47750612085383465, Training Accuracy: 87.74814814814815\n",
            "End of Epoch 429, Validation Loss: 0.4307388900182747, Validation Accuracy: 89.71666666666667\n",
            "End of Epoch 430, Training Loss: 0.4762959750634477, Training Accuracy: 87.77777777777777\n",
            "End of Epoch 430, Validation Loss: 0.42957318434368863, Validation Accuracy: 89.71666666666667\n",
            "End of Epoch 431, Training Loss: 0.47509362433958136, Training Accuracy: 87.82037037037037\n",
            "End of Epoch 431, Validation Loss: 0.42841534458076147, Validation Accuracy: 89.75\n",
            "End of Epoch 432, Training Loss: 0.4738990103458371, Training Accuracy: 87.85\n",
            "End of Epoch 432, Validation Loss: 0.4272653097606781, Validation Accuracy: 89.76666666666667\n",
            "End of Epoch 433, Training Loss: 0.47271207481121785, Training Accuracy: 87.87222222222222\n",
            "End of Epoch 433, Validation Loss: 0.42612301894108123, Validation Accuracy: 89.8\n",
            "End of Epoch 434, Training Loss: 0.471532759550632, Training Accuracy: 87.90740740740742\n",
            "End of Epoch 434, Validation Loss: 0.42498841123090886, Validation Accuracy: 89.83333333333333\n",
            "End of Epoch 435, Training Loss: 0.47036100648456997, Training Accuracy: 87.93703703703703\n",
            "End of Epoch 435, Validation Loss: 0.42386142581450614, Validation Accuracy: 89.88333333333334\n",
            "End of Epoch 436, Training Loss: 0.46919675765796404, Training Accuracy: 87.96666666666667\n",
            "End of Epoch 436, Validation Loss: 0.42274200197501144, Validation Accuracy: 89.88333333333334\n",
            "End of Epoch 437, Training Loss: 0.4680399552582467, Training Accuracy: 88.0\n",
            "End of Epoch 437, Validation Loss: 0.42163007911702294, Validation Accuracy: 89.91666666666667\n",
            "End of Epoch 438, Training Loss: 0.46689054163261795, Training Accuracy: 88.03148148148148\n",
            "End of Epoch 438, Validation Loss: 0.4205255967885465, Validation Accuracy: 89.91666666666667\n",
            "End of Epoch 439, Training Loss: 0.46574845930453784, Training Accuracy: 88.05185185185185\n",
            "End of Epoch 439, Validation Loss: 0.41942849470222593, Validation Accuracy: 89.93333333333334\n",
            "End of Epoch 440, Training Loss: 0.4646136509894599, Training Accuracy: 88.05555555555556\n",
            "End of Epoch 440, Validation Loss: 0.41833871275586193, Validation Accuracy: 89.95\n",
            "End of Epoch 441, Training Loss: 0.4634860596098167, Training Accuracy: 88.08518518518518\n",
            "End of Epoch 441, Validation Loss: 0.4172561910522158, Validation Accuracy: 89.95\n",
            "End of Epoch 442, Training Loss: 0.46236562830928085, Training Accuracy: 88.10740740740741\n",
            "End of Epoch 442, Validation Loss: 0.41618086991810327, Validation Accuracy: 89.96666666666667\n",
            "End of Epoch 443, Training Loss: 0.46125230046630844, Training Accuracy: 88.12962962962962\n",
            "End of Epoch 443, Validation Loss: 0.41511268992277867, Validation Accuracy: 89.96666666666667\n",
            "End of Epoch 444, Training Loss: 0.4601460197069882, Training Accuracy: 88.1537037037037\n",
            "End of Epoch 444, Validation Loss: 0.4140515918956096, Validation Accuracy: 89.96666666666667\n",
            "End of Epoch 445, Training Loss: 0.4590467299172119, Training Accuracy: 88.2\n",
            "End of Epoch 445, Validation Loss: 0.41299751694304443, Validation Accuracy: 89.95\n",
            "End of Epoch 446, Training Loss: 0.45795437525418337, Training Accuracy: 88.22777777777779\n",
            "End of Epoch 446, Validation Loss: 0.41195040646487285, Validation Accuracy: 89.96666666666667\n",
            "End of Epoch 447, Training Loss: 0.4568689001572788, Training Accuracy: 88.25740740740741\n",
            "End of Epoch 447, Validation Loss: 0.41091020216977997, Validation Accuracy: 90.01666666666667\n",
            "End of Epoch 448, Training Loss: 0.4557902493582856, Training Accuracy: 88.30185185185185\n",
            "End of Epoch 448, Validation Loss: 0.4098768460901968, Validation Accuracy: 90.05\n",
            "End of Epoch 449, Training Loss: 0.4547183678910278, Training Accuracy: 88.34444444444445\n",
            "End of Epoch 449, Validation Loss: 0.4088502805964459, Validation Accuracy: 90.06666666666666\n",
            "End of Epoch 450, Training Loss: 0.4536532011004018, Training Accuracy: 88.36111111111111\n",
            "End of Epoch 450, Validation Loss: 0.4078304484101875, Validation Accuracy: 90.08333333333334\n",
            "End of Epoch 451, Training Loss: 0.45259469465083874, Training Accuracy: 88.3962962962963\n",
            "End of Epoch 451, Validation Loss: 0.40681729261716426, Validation Accuracy: 90.10000000000001\n",
            "End of Epoch 452, Training Loss: 0.45154279453421137, Training Accuracy: 88.42592592592592\n",
            "End of Epoch 452, Validation Loss: 0.40581075667925054, Validation Accuracy: 90.11666666666667\n",
            "End of Epoch 453, Training Loss: 0.45049744707720185, Training Accuracy: 88.4462962962963\n",
            "End of Epoch 453, Validation Loss: 0.4048107844458073, Validation Accuracy: 90.14999999999999\n",
            "End of Epoch 454, Training Loss: 0.4494585989481509, Training Accuracy: 88.47592592592592\n",
            "End of Epoch 454, Validation Loss: 0.40381732016434785, Validation Accuracy: 90.18333333333334\n",
            "End of Epoch 455, Training Loss: 0.4484261971634046, Training Accuracy: 88.49814814814815\n",
            "End of Epoch 455, Validation Loss: 0.4028303084905178, Validation Accuracy: 90.21666666666667\n",
            "End of Epoch 456, Training Loss: 0.4474001890931755, Training Accuracy: 88.51296296296296\n",
            "End of Epoch 456, Validation Loss: 0.401849694497396, Validation Accuracy: 90.16666666666666\n",
            "End of Epoch 457, Training Loss: 0.446380522466936, Training Accuracy: 88.54444444444445\n",
            "End of Epoch 457, Validation Loss: 0.4008754236841212, Validation Accuracy: 90.16666666666666\n",
            "End of Epoch 458, Training Loss: 0.44536714537836075, Training Accuracy: 88.56666666666668\n",
            "End of Epoch 458, Validation Loss: 0.3999074419838531, Validation Accuracy: 90.16666666666666\n",
            "End of Epoch 459, Training Loss: 0.44436000628983696, Training Accuracy: 88.59074074074074\n",
            "End of Epoch 459, Validation Loss: 0.39894569577107414, Validation Accuracy: 90.2\n",
            "End of Epoch 460, Training Loss: 0.4433590540365532, Training Accuracy: 88.63148148148147\n",
            "End of Epoch 460, Validation Loss: 0.3979901318682426, Validation Accuracy: 90.2\n",
            "End of Epoch 461, Training Loss: 0.4423642378301921, Training Accuracy: 88.65925925925926\n",
            "End of Epoch 461, Validation Loss: 0.3970406975518057, Validation Accuracy: 90.2\n",
            "End of Epoch 462, Training Loss: 0.44137550726222996, Training Accuracy: 88.67407407407407\n",
            "End of Epoch 462, Validation Loss: 0.3960973405575847, Validation Accuracy: 90.23333333333333\n",
            "End of Epoch 463, Training Loss: 0.4403928123068737, Training Accuracy: 88.69814814814815\n",
            "End of Epoch 463, Validation Loss: 0.39516000908554316, Validation Accuracy: 90.23333333333333\n",
            "End of Epoch 464, Training Loss: 0.43941610332363656, Training Accuracy: 88.71111111111111\n",
            "End of Epoch 464, Validation Loss: 0.394228651803951, Validation Accuracy: 90.28333333333333\n",
            "End of Epoch 465, Training Loss: 0.4384453310595744, Training Accuracy: 88.7388888888889\n",
            "End of Epoch 465, Validation Loss: 0.3933032178529596, Validation Accuracy: 90.31666666666666\n",
            "End of Epoch 466, Training Loss: 0.43748044665119673, Training Accuracy: 88.77222222222223\n",
            "End of Epoch 466, Validation Loss: 0.3923836568476006, Validation Accuracy: 90.3\n",
            "End of Epoch 467, Training Loss: 0.43652140162606284, Training Accuracy: 88.79074074074073\n",
            "End of Epoch 467, Validation Loss: 0.39146991888022536, Validation Accuracy: 90.33333333333333\n",
            "End of Epoch 468, Training Loss: 0.4355681479040794, Training Accuracy: 88.82407407407408\n",
            "End of Epoch 468, Validation Loss: 0.3905619545224009, Validation Accuracy: 90.31666666666666\n",
            "End of Epoch 469, Training Loss: 0.43462063779851046, Training Accuracy: 88.8462962962963\n",
            "End of Epoch 469, Validation Loss: 0.38965971482627954, Validation Accuracy: 90.31666666666666\n",
            "End of Epoch 470, Training Loss: 0.4336788240167144, Training Accuracy: 88.85925925925926\n",
            "End of Epoch 470, Validation Loss: 0.3887631513254603, Validation Accuracy: 90.31666666666666\n",
            "End of Epoch 471, Training Loss: 0.4327426596606186, Training Accuracy: 88.89814814814815\n",
            "End of Epoch 471, Validation Loss: 0.387872216035359, Validation Accuracy: 90.31666666666666\n",
            "End of Epoch 472, Training Loss: 0.4318120982269423, Training Accuracy: 88.91296296296296\n",
            "End of Epoch 472, Validation Loss: 0.3869868614531089, Validation Accuracy: 90.33333333333333\n",
            "End of Epoch 473, Training Loss: 0.43088709360718425, Training Accuracy: 88.93703703703704\n",
            "End of Epoch 473, Validation Loss: 0.38610704055700756, Validation Accuracy: 90.36666666666666\n",
            "End of Epoch 474, Training Loss: 0.42996760008737883, Training Accuracy: 88.96666666666667\n",
            "End of Epoch 474, Validation Loss: 0.38523270680553373, Validation Accuracy: 90.38333333333334\n",
            "End of Epoch 475, Training Loss: 0.4290535723476365, Training Accuracy: 88.98333333333333\n",
            "End of Epoch 475, Validation Loss: 0.3843638141359508, Validation Accuracy: 90.38333333333334\n",
            "End of Epoch 476, Training Loss: 0.4281449654614798, Training Accuracy: 88.99444444444444\n",
            "End of Epoch 476, Validation Loss: 0.38350031696252007, Validation Accuracy: 90.38333333333334\n",
            "End of Epoch 477, Training Loss: 0.42724173489497774, Training Accuracy: 89.0111111111111\n",
            "End of Epoch 477, Validation Loss: 0.38264217017434227, Validation Accuracy: 90.41666666666667\n",
            "End of Epoch 478, Training Loss: 0.4263438365056978, Training Accuracy: 89.02592592592593\n",
            "End of Epoch 478, Validation Loss: 0.3817893291328489, Validation Accuracy: 90.4\n",
            "End of Epoch 479, Training Loss: 0.4254512265414782, Training Accuracy: 89.04814814814814\n",
            "End of Epoch 479, Validation Loss: 0.3809417496689651, Validation Accuracy: 90.45\n",
            "End of Epoch 480, Training Loss: 0.4245638616390313, Training Accuracy: 89.07777777777778\n",
            "End of Epoch 480, Validation Loss: 0.38009938807996174, Validation Accuracy: 90.45\n",
            "End of Epoch 481, Training Loss: 0.4236816988223882, Training Accuracy: 89.1\n",
            "End of Epoch 481, Validation Loss: 0.37926220112602027, Validation Accuracy: 90.46666666666667\n",
            "End of Epoch 482, Training Loss: 0.4228046955011924, Training Accuracy: 89.1148148148148\n",
            "End of Epoch 482, Validation Loss: 0.3784301460265286, Validation Accuracy: 90.51666666666667\n",
            "End of Epoch 483, Training Loss: 0.4219328094688499, Training Accuracy: 89.13333333333333\n",
            "End of Epoch 483, Validation Loss: 0.3776031804561285, Validation Accuracy: 90.5\n",
            "End of Epoch 484, Training Loss: 0.4210659989005475, Training Accuracy: 89.1537037037037\n",
            "End of Epoch 484, Validation Loss: 0.3767812625405338, Validation Accuracy: 90.51666666666667\n",
            "End of Epoch 485, Training Loss: 0.42020422235113913, Training Accuracy: 89.16481481481482\n",
            "End of Epoch 485, Validation Loss: 0.37596435085213964, Validation Accuracy: 90.53333333333333\n",
            "End of Epoch 486, Training Loss: 0.41934743875291997, Training Accuracy: 89.18888888888888\n",
            "End of Epoch 486, Validation Loss: 0.37515240440544007, Validation Accuracy: 90.56666666666666\n",
            "End of Epoch 487, Training Loss: 0.4184956074132841, Training Accuracy: 89.22037037037038\n",
            "End of Epoch 487, Validation Loss: 0.37434538265227235, Validation Accuracy: 90.56666666666666\n",
            "End of Epoch 488, Training Loss: 0.4176486880122814, Training Accuracy: 89.24444444444445\n",
            "End of Epoch 488, Validation Loss: 0.37354324547690804, Validation Accuracy: 90.56666666666666\n",
            "End of Epoch 489, Training Loss: 0.41680664060007583, Training Accuracy: 89.27407407407408\n",
            "End of Epoch 489, Validation Loss: 0.372745953191003, Validation Accuracy: 90.53333333333333\n",
            "End of Epoch 490, Training Loss: 0.4159694255943152, Training Accuracy: 89.29814814814814\n",
            "End of Epoch 490, Validation Loss: 0.3719534665284293, Validation Accuracy: 90.53333333333333\n",
            "End of Epoch 491, Training Loss: 0.4151370037774179, Training Accuracy: 89.31296296296296\n",
            "End of Epoch 491, Validation Loss: 0.3711657466399989, Validation Accuracy: 90.56666666666666\n",
            "End of Epoch 492, Training Loss: 0.414309336293782, Training Accuracy: 89.33148148148148\n",
            "End of Epoch 492, Validation Loss: 0.3703827550880978, Validation Accuracy: 90.55\n",
            "End of Epoch 493, Training Loss: 0.41348638464692783, Training Accuracy: 89.3425925925926\n",
            "End of Epoch 493, Validation Loss: 0.369604453841244, Validation Accuracy: 90.56666666666666\n",
            "End of Epoch 494, Training Loss: 0.4126681106965742, Training Accuracy: 89.35555555555555\n",
            "End of Epoch 494, Validation Loss: 0.3688308052685837, Validation Accuracy: 90.56666666666666\n",
            "End of Epoch 495, Training Loss: 0.4118544766556584, Training Accuracy: 89.37962962962963\n",
            "End of Epoch 495, Validation Loss: 0.3680617721343376, Validation Accuracy: 90.60000000000001\n",
            "End of Epoch 496, Training Loss: 0.4110454450873065, Training Accuracy: 89.39074074074074\n",
            "End of Epoch 496, Validation Loss: 0.3672973175922122, Validation Accuracy: 90.61666666666667\n",
            "End of Epoch 497, Training Loss: 0.41024097890175715, Training Accuracy: 89.40925925925926\n",
            "End of Epoch 497, Validation Loss: 0.3665374051797836, Validation Accuracy: 90.63333333333333\n",
            "End of Epoch 498, Training Loss: 0.4094410413532465, Training Accuracy: 89.43333333333334\n",
            "End of Epoch 498, Validation Loss: 0.36578199881287066, Validation Accuracy: 90.66666666666666\n",
            "End of Epoch 499, Training Loss: 0.40864559603685935, Training Accuracy: 89.45555555555555\n",
            "End of Epoch 499, Validation Loss: 0.36503106277990194, Validation Accuracy: 90.7\n",
            "End of Epoch 500, Training Loss: 0.40785460688535224, Training Accuracy: 89.47037037037036\n",
            "End of Epoch 500, Validation Loss: 0.36428456173629054, Validation Accuracy: 90.7\n",
            "End of Epoch 501, Training Loss: 0.40706803816595183, Training Accuracy: 89.5\n",
            "End of Epoch 501, Validation Loss: 0.3635424606988234, Validation Accuracy: 90.71666666666667\n",
            "End of Epoch 502, Training Loss: 0.40628585447713816, Training Accuracy: 89.52222222222223\n",
            "End of Epoch 502, Validation Loss: 0.3628047250400751, Validation Accuracy: 90.71666666666667\n",
            "End of Epoch 503, Training Loss: 0.4055080207454118, Training Accuracy: 89.54444444444445\n",
            "End of Epoch 503, Validation Loss: 0.36207132048285207, Validation Accuracy: 90.73333333333333\n",
            "End of Epoch 504, Training Loss: 0.4047345022220544, Training Accuracy: 89.55555555555556\n",
            "End of Epoch 504, Validation Loss: 0.3613422130946761, Validation Accuracy: 90.75\n",
            "End of Epoch 505, Training Loss: 0.4039652644798847, Training Accuracy: 89.57222222222222\n",
            "End of Epoch 505, Validation Loss: 0.36061736928231336, Validation Accuracy: 90.8\n",
            "End of Epoch 506, Training Loss: 0.4032002734100148, Training Accuracy: 89.5925925925926\n",
            "End of Epoch 506, Validation Loss: 0.35989675578635383, Validation Accuracy: 90.78333333333333\n",
            "End of Epoch 507, Training Loss: 0.4024394952186124, Training Accuracy: 89.60925925925926\n",
            "End of Epoch 507, Validation Loss: 0.3591803396758488, Validation Accuracy: 90.8\n",
            "End of Epoch 508, Training Loss: 0.40168289642366867, Training Accuracy: 89.63518518518518\n",
            "End of Epoch 508, Validation Loss: 0.3584680883430097, Validation Accuracy: 90.83333333333333\n",
            "End of Epoch 509, Training Loss: 0.4009304438517825, Training Accuracy: 89.64999999999999\n",
            "End of Epoch 509, Validation Loss: 0.35775996949797323, Validation Accuracy: 90.85\n",
            "End of Epoch 510, Training Loss: 0.4001821046349579, Training Accuracy: 89.67222222222222\n",
            "End of Epoch 510, Validation Loss: 0.3570559511636379, Validation Accuracy: 90.88333333333334\n",
            "End of Epoch 511, Training Loss: 0.39943784620742023, Training Accuracy: 89.69259259259259\n",
            "End of Epoch 511, Validation Loss: 0.356356001670574, Validation Accuracy: 90.9\n",
            "End of Epoch 512, Training Loss: 0.3986976363024577, Training Accuracy: 89.7074074074074\n",
            "End of Epoch 512, Validation Loss: 0.35566008965201185, Validation Accuracy: 90.9\n",
            "End of Epoch 513, Training Loss: 0.3979614429492861, Training Accuracy: 89.72962962962963\n",
            "End of Epoch 513, Validation Loss: 0.35496818403891134, Validation Accuracy: 90.95\n",
            "End of Epoch 514, Training Loss: 0.39722923446994207, Training Accuracy: 89.75\n",
            "End of Epoch 514, Validation Loss: 0.35428025405511304, Validation Accuracy: 90.96666666666667\n",
            "End of Epoch 515, Training Loss: 0.3965009794762075, Training Accuracy: 89.77037037037037\n",
            "End of Epoch 515, Validation Loss: 0.35359626921257603, Validation Accuracy: 91.0\n",
            "End of Epoch 516, Training Loss: 0.39577664686656655, Training Accuracy: 89.77777777777777\n",
            "End of Epoch 516, Validation Loss: 0.352916199306703, Validation Accuracy: 91.0\n",
            "End of Epoch 517, Training Loss: 0.3950562058231979, Training Accuracy: 89.78518518518518\n",
            "End of Epoch 517, Validation Loss: 0.3522400144117541, Validation Accuracy: 91.03333333333333\n",
            "End of Epoch 518, Training Loss: 0.39433962580900433, Training Accuracy: 89.8\n",
            "End of Epoch 518, Validation Loss: 0.351567684876352, Validation Accuracy: 91.03333333333333\n",
            "End of Epoch 519, Training Loss: 0.39362687656468115, Training Accuracy: 89.80925925925925\n",
            "End of Epoch 519, Validation Loss: 0.3508991813190772, Validation Accuracy: 91.01666666666667\n",
            "End of Epoch 520, Training Loss: 0.3929179281058239, Training Accuracy: 89.82222222222222\n",
            "End of Epoch 520, Validation Loss: 0.350234474624158, Validation Accuracy: 91.01666666666667\n",
            "End of Epoch 521, Training Loss: 0.392212750720079, Training Accuracy: 89.84074074074074\n",
            "End of Epoch 521, Validation Loss: 0.3495735359372521, Validation Accuracy: 91.08333333333334\n",
            "End of Epoch 522, Training Loss: 0.3915113149643363, Training Accuracy: 89.86111111111111\n",
            "End of Epoch 522, Validation Loss: 0.3489163366613229, Validation Accuracy: 91.10000000000001\n",
            "End of Epoch 523, Training Loss: 0.39081359166196494, Training Accuracy: 89.87592592592593\n",
            "End of Epoch 523, Validation Loss: 0.34826284845260913, Validation Accuracy: 91.11666666666667\n",
            "End of Epoch 524, Training Loss: 0.39011955190009473, Training Accuracy: 89.88703703703703\n",
            "End of Epoch 524, Validation Loss: 0.3476130432166907, Validation Accuracy: 91.14999999999999\n",
            "End of Epoch 525, Training Loss: 0.38942916702694136, Training Accuracy: 89.9074074074074\n",
            "End of Epoch 525, Validation Loss: 0.346966893104646, Validation Accuracy: 91.13333333333333\n",
            "End of Epoch 526, Training Loss: 0.3887424086491767, Training Accuracy: 89.91666666666667\n",
            "End of Epoch 526, Validation Loss: 0.34632437050930603, Validation Accuracy: 91.13333333333333\n",
            "End of Epoch 527, Training Loss: 0.3880592486293468, Training Accuracy: 89.92222222222223\n",
            "End of Epoch 527, Validation Loss: 0.34568544806160056, Validation Accuracy: 91.14999999999999\n",
            "End of Epoch 528, Training Loss: 0.3873796590833311, Training Accuracy: 89.93333333333334\n",
            "End of Epoch 528, Validation Loss: 0.3450500986269983, Validation Accuracy: 91.18333333333334\n",
            "End of Epoch 529, Training Loss: 0.38670361237785283, Training Accuracy: 89.94259259259259\n",
            "End of Epoch 529, Validation Loss: 0.3444182953020405, Validation Accuracy: 91.18333333333334\n",
            "End of Epoch 530, Training Loss: 0.3860310811280298, Training Accuracy: 89.9537037037037\n",
            "End of Epoch 530, Validation Loss: 0.3437900114109654, Validation Accuracy: 91.18333333333334\n",
            "End of Epoch 531, Training Loss: 0.385362038194972, Training Accuracy: 89.97407407407407\n",
            "End of Epoch 531, Validation Loss: 0.34316522050242476, Validation Accuracy: 91.18333333333334\n",
            "End of Epoch 532, Training Loss: 0.3846964566834242, Training Accuracy: 89.9962962962963\n",
            "End of Epoch 532, Validation Loss: 0.342543896346291, Validation Accuracy: 91.21666666666667\n",
            "End of Epoch 533, Training Loss: 0.3840343099394515, Training Accuracy: 90.01666666666667\n",
            "End of Epoch 533, Validation Loss: 0.3419260129305532, Validation Accuracy: 91.23333333333333\n",
            "End of Epoch 534, Training Loss: 0.38337557154816665, Training Accuracy: 90.02777777777777\n",
            "End of Epoch 534, Validation Loss: 0.3413115444583016, Validation Accuracy: 91.25\n",
            "End of Epoch 535, Training Loss: 0.3827202153315025, Training Accuracy: 90.03703703703704\n",
            "End of Epoch 535, Validation Loss: 0.34070046534479925, Validation Accuracy: 91.26666666666667\n",
            "End of Epoch 536, Training Loss: 0.38206821534602414, Training Accuracy: 90.05925925925926\n",
            "End of Epoch 536, Validation Loss: 0.34009275021463936, Validation Accuracy: 91.25\n",
            "End of Epoch 537, Training Loss: 0.3814195458807797, Training Accuracy: 90.07777777777778\n",
            "End of Epoch 537, Validation Loss: 0.33948837389898734, Validation Accuracy: 91.26666666666667\n",
            "End of Epoch 538, Training Loss: 0.38077418145519387, Training Accuracy: 90.08148148148149\n",
            "End of Epoch 538, Validation Loss: 0.3388873114329054, Validation Accuracy: 91.28333333333333\n",
            "End of Epoch 539, Training Loss: 0.38013209681699595, Training Accuracy: 90.09074074074074\n",
            "End of Epoch 539, Validation Loss: 0.3382895380527592, Validation Accuracy: 91.3\n",
            "End of Epoch 540, Training Loss: 0.379493266940188, Training Accuracy: 90.12037037037037\n",
            "End of Epoch 540, Validation Loss: 0.33769502919370387, Validation Accuracy: 91.3\n",
            "End of Epoch 541, Training Loss: 0.3788576670230455, Training Accuracy: 90.14629629629628\n",
            "End of Epoch 541, Validation Loss: 0.337103760487249, Validation Accuracy: 91.3\n",
            "End of Epoch 542, Training Loss: 0.3782252724861554, Training Accuracy: 90.17037037037036\n",
            "End of Epoch 542, Validation Loss: 0.3365157077588995, Validation Accuracy: 91.31666666666666\n",
            "End of Epoch 543, Training Loss: 0.37759605897048426, Training Accuracy: 90.17777777777778\n",
            "End of Epoch 543, Validation Loss: 0.3359308470258714, Validation Accuracy: 91.38333333333334\n",
            "End of Epoch 544, Training Loss: 0.37697000233548, Training Accuracy: 90.19814814814815\n",
            "End of Epoch 544, Validation Loss: 0.3353491544948816, Validation Accuracy: 91.4\n",
            "End of Epoch 545, Training Loss: 0.376347078657204, Training Accuracy: 90.20925925925926\n",
            "End of Epoch 545, Validation Loss: 0.3347706065600075, Validation Accuracy: 91.43333333333334\n",
            "End of Epoch 546, Training Loss: 0.3757272642264908, Training Accuracy: 90.22407407407408\n",
            "End of Epoch 546, Validation Loss: 0.33419517980061675, Validation Accuracy: 91.45\n",
            "End of Epoch 547, Training Loss: 0.3751105355471357, Training Accuracy: 90.23333333333333\n",
            "End of Epoch 547, Validation Loss: 0.3336228509793639, Validation Accuracy: 91.41666666666667\n",
            "End of Epoch 548, Training Loss: 0.37449686933411036, Training Accuracy: 90.25185185185185\n",
            "End of Epoch 548, Validation Loss: 0.33305359704025295, Validation Accuracy: 91.43333333333334\n",
            "End of Epoch 549, Training Loss: 0.3738862425118001, Training Accuracy: 90.25925925925927\n",
            "End of Epoch 549, Validation Loss: 0.332487395106763, Validation Accuracy: 91.43333333333334\n",
            "End of Epoch 550, Training Loss: 0.3732786322122671, Training Accuracy: 90.27222222222223\n",
            "End of Epoch 550, Validation Loss: 0.33192422248003495, Validation Accuracy: 91.43333333333334\n",
            "End of Epoch 551, Training Loss: 0.37267401577353426, Training Accuracy: 90.28703703703704\n",
            "End of Epoch 551, Validation Loss: 0.3313640566371185, Validation Accuracy: 91.46666666666667\n",
            "End of Epoch 552, Training Loss: 0.3720723707378912, Training Accuracy: 90.29629629629629\n",
            "End of Epoch 552, Validation Loss: 0.33080687522927527, Validation Accuracy: 91.48333333333333\n",
            "End of Epoch 553, Training Loss: 0.3714736748502178, Training Accuracy: 90.3111111111111\n",
            "End of Epoch 553, Validation Loss: 0.33025265608033816, Validation Accuracy: 91.5\n",
            "End of Epoch 554, Training Loss: 0.3708779060563278, Training Accuracy: 90.33148148148148\n",
            "End of Epoch 554, Validation Loss: 0.32970137718512316, Validation Accuracy: 91.53333333333333\n",
            "End of Epoch 555, Training Loss: 0.3702850425013271, Training Accuracy: 90.34629629629629\n",
            "End of Epoch 555, Validation Loss: 0.3291530167078929, Validation Accuracy: 91.53333333333333\n",
            "End of Epoch 556, Training Loss: 0.36969506252799045, Training Accuracy: 90.36481481481482\n",
            "End of Epoch 556, Validation Loss: 0.32860755298086836, Validation Accuracy: 91.55\n",
            "End of Epoch 557, Training Loss: 0.3691079446751506, Training Accuracy: 90.36851851851851\n",
            "End of Epoch 557, Validation Loss: 0.32806496450278866, Validation Accuracy: 91.55\n",
            "End of Epoch 558, Training Loss: 0.368523667676102, Training Accuracy: 90.39074074074074\n",
            "End of Epoch 558, Validation Loss: 0.32752522993751443, Validation Accuracy: 91.56666666666666\n",
            "End of Epoch 559, Training Loss: 0.3679422104570151, Training Accuracy: 90.40555555555555\n",
            "End of Epoch 559, Validation Loss: 0.32698832811267514, Validation Accuracy: 91.60000000000001\n",
            "End of Epoch 560, Training Loss: 0.36736355213536553, Training Accuracy: 90.41666666666667\n",
            "End of Epoch 560, Validation Loss: 0.3264542380183566, Validation Accuracy: 91.60000000000001\n",
            "End of Epoch 561, Training Loss: 0.3667876720183684, Training Accuracy: 90.42777777777778\n",
            "End of Epoch 561, Validation Loss: 0.32592293880582757, Validation Accuracy: 91.61666666666667\n",
            "End of Epoch 562, Training Loss: 0.36621454960142713, Training Accuracy: 90.4462962962963\n",
            "End of Epoch 562, Validation Loss: 0.32539440978630296, Validation Accuracy: 91.61666666666667\n",
            "End of Epoch 563, Training Loss: 0.36564416456658666, Training Accuracy: 90.46851851851852\n",
            "End of Epoch 563, Validation Loss: 0.32486863042974184, Validation Accuracy: 91.61666666666667\n",
            "End of Epoch 564, Training Loss: 0.3650764967809966, Training Accuracy: 90.47222222222221\n",
            "End of Epoch 564, Validation Loss: 0.3243455803636792, Validation Accuracy: 91.61666666666667\n",
            "End of Epoch 565, Training Loss: 0.36451152629538053, Training Accuracy: 90.47777777777777\n",
            "End of Epoch 565, Validation Loss: 0.3238252393720869, Validation Accuracy: 91.64999999999999\n",
            "End of Epoch 566, Training Loss: 0.36394923334251006, Training Accuracy: 90.4962962962963\n",
            "End of Epoch 566, Validation Loss: 0.32330758739426646, Validation Accuracy: 91.66666666666666\n",
            "End of Epoch 567, Training Loss: 0.3633895983356871, Training Accuracy: 90.51481481481481\n",
            "End of Epoch 567, Validation Loss: 0.3227926045237664, Validation Accuracy: 91.68333333333332\n",
            "End of Epoch 568, Training Loss: 0.36283260186722704, Training Accuracy: 90.53333333333333\n",
            "End of Epoch 568, Validation Loss: 0.3222802710073277, Validation Accuracy: 91.73333333333333\n",
            "End of Epoch 569, Training Loss: 0.3622782247069491, Training Accuracy: 90.53518518518518\n",
            "End of Epoch 569, Validation Loss: 0.32177056724385106, Validation Accuracy: 91.75\n",
            "End of Epoch 570, Training Loss: 0.36172644780066804, Training Accuracy: 90.55\n",
            "End of Epoch 570, Validation Loss: 0.3212634737833879, Validation Accuracy: 91.75\n",
            "End of Epoch 571, Training Loss: 0.36117725226869096, Training Accuracy: 90.55\n",
            "End of Epoch 571, Validation Loss: 0.320758971326151, Validation Accuracy: 91.75\n",
            "End of Epoch 572, Training Loss: 0.3606306194043137, Training Accuracy: 90.55740740740741\n",
            "End of Epoch 572, Validation Loss: 0.3202570407215446, Validation Accuracy: 91.78333333333333\n",
            "End of Epoch 573, Training Loss: 0.36008653067232194, Training Accuracy: 90.56666666666666\n",
            "End of Epoch 573, Validation Loss: 0.3197576629672117, Validation Accuracy: 91.8\n",
            "End of Epoch 574, Training Loss: 0.3595449677074929, Training Accuracy: 90.58148148148149\n",
            "End of Epoch 574, Validation Loss: 0.3192608192080973, Validation Accuracy: 91.81666666666666\n",
            "End of Epoch 575, Training Loss: 0.3590059123130986, Training Accuracy: 90.60185185185186\n",
            "End of Epoch 575, Validation Loss: 0.3187664907355267, Validation Accuracy: 91.81666666666666\n",
            "End of Epoch 576, Training Loss: 0.35846934645940937, Training Accuracy: 90.61481481481482\n",
            "End of Epoch 576, Validation Loss: 0.31827465898629637, Validation Accuracy: 91.83333333333333\n",
            "End of Epoch 577, Training Loss: 0.3579352522822006, Training Accuracy: 90.62592592592593\n",
            "End of Epoch 577, Validation Loss: 0.3177853055417768, Validation Accuracy: 91.85\n",
            "End of Epoch 578, Training Loss: 0.35740361208125715, Training Accuracy: 90.62962962962963\n",
            "End of Epoch 578, Validation Loss: 0.3172984121270268, Validation Accuracy: 91.83333333333333\n",
            "End of Epoch 579, Training Loss: 0.356874408318881, Training Accuracy: 90.63518518518518\n",
            "End of Epoch 579, Validation Loss: 0.31681396060991573, Validation Accuracy: 91.83333333333333\n",
            "End of Epoch 580, Training Loss: 0.35634762361839695, Training Accuracy: 90.64444444444445\n",
            "End of Epoch 580, Validation Loss: 0.31633193300025586, Validation Accuracy: 91.86666666666666\n",
            "End of Epoch 581, Training Loss: 0.35582324076266125, Training Accuracy: 90.65185185185184\n",
            "End of Epoch 581, Validation Loss: 0.31585231144894055, Validation Accuracy: 91.9\n",
            "End of Epoch 582, Training Loss: 0.35530124269256796, Training Accuracy: 90.66666666666666\n",
            "End of Epoch 582, Validation Loss: 0.3153750782470891, Validation Accuracy: 91.9\n",
            "End of Epoch 583, Training Loss: 0.3547816125055575, Training Accuracy: 90.68148148148148\n",
            "End of Epoch 583, Validation Loss: 0.3149002158251974, Validation Accuracy: 91.9\n",
            "End of Epoch 584, Training Loss: 0.3542643334541256, Training Accuracy: 90.6962962962963\n",
            "End of Epoch 584, Validation Loss: 0.3144277067522921, Validation Accuracy: 91.93333333333334\n",
            "End of Epoch 585, Training Loss: 0.3537493889443308, Training Accuracy: 90.71481481481482\n",
            "End of Epoch 585, Validation Loss: 0.31395753373508883, Validation Accuracy: 91.93333333333334\n",
            "End of Epoch 586, Training Loss: 0.3532367625343046, Training Accuracy: 90.73148148148148\n",
            "End of Epoch 586, Validation Loss: 0.3134896796171532, Validation Accuracy: 91.96666666666667\n",
            "End of Epoch 587, Training Loss: 0.3527264379327625, Training Accuracy: 90.7425925925926\n",
            "End of Epoch 587, Validation Loss: 0.31302412737806345, Validation Accuracy: 91.96666666666667\n",
            "End of Epoch 588, Training Loss: 0.3522183989975131, Training Accuracy: 90.75555555555556\n",
            "End of Epoch 588, Validation Loss: 0.3125608601325745, Validation Accuracy: 91.95\n",
            "End of Epoch 589, Training Loss: 0.35171262973397105, Training Accuracy: 90.77037037037037\n",
            "End of Epoch 589, Validation Loss: 0.3120998611297833, Validation Accuracy: 91.96666666666667\n",
            "End of Epoch 590, Training Loss: 0.3512091142936718, Training Accuracy: 90.78333333333333\n",
            "End of Epoch 590, Validation Loss: 0.3116411137522932, Validation Accuracy: 91.96666666666667\n",
            "End of Epoch 591, Training Loss: 0.3507078369727838, Training Accuracy: 90.79629629629629\n",
            "End of Epoch 591, Validation Loss: 0.311184601515379, Validation Accuracy: 91.98333333333333\n",
            "End of Epoch 592, Training Loss: 0.35020878221062757, Training Accuracy: 90.8074074074074\n",
            "End of Epoch 592, Validation Loss: 0.310730308066151, Validation Accuracy: 92.03333333333333\n",
            "End of Epoch 593, Training Loss: 0.3497119345881936, Training Accuracy: 90.81481481481481\n",
            "End of Epoch 593, Validation Loss: 0.31027821718271636, Validation Accuracy: 92.06666666666666\n",
            "End of Epoch 594, Training Loss: 0.3492172788266642, Training Accuracy: 90.82407407407408\n",
            "End of Epoch 594, Validation Loss: 0.30982831277334094, Validation Accuracy: 92.06666666666666\n",
            "End of Epoch 595, Training Loss: 0.34872479978593635, Training Accuracy: 90.84074074074074\n",
            "End of Epoch 595, Validation Loss: 0.3093805788756077, Validation Accuracy: 92.06666666666666\n",
            "End of Epoch 596, Training Loss: 0.3482344824631506, Training Accuracy: 90.84629629629629\n",
            "End of Epoch 596, Validation Loss: 0.3089349996555733, Validation Accuracy: 92.10000000000001\n",
            "End of Epoch 597, Training Loss: 0.3477463119912205, Training Accuracy: 90.84629629629629\n",
            "End of Epoch 597, Validation Loss: 0.3084915594069217, Validation Accuracy: 92.11666666666667\n",
            "End of Epoch 598, Training Loss: 0.3472602736373676, Training Accuracy: 90.85740740740741\n",
            "End of Epoch 598, Validation Loss: 0.3080502425501159, Validation Accuracy: 92.13333333333334\n",
            "End of Epoch 599, Training Loss: 0.34677635280165997, Training Accuracy: 90.87592592592593\n",
            "End of Epoch 599, Validation Loss: 0.3076110336315462, Validation Accuracy: 92.15\n",
            "End of Epoch 600, Training Loss: 0.34629453501555596, Training Accuracy: 90.88888888888889\n",
            "End of Epoch 600, Validation Loss: 0.3071739173226749, Validation Accuracy: 92.16666666666666\n",
            "End of Epoch 601, Training Loss: 0.34581480594045255, Training Accuracy: 90.89444444444445\n",
            "End of Epoch 601, Validation Loss: 0.30673887841917935, Validation Accuracy: 92.15\n",
            "End of Epoch 602, Training Loss: 0.34533715136623866, Training Accuracy: 90.89259259259259\n",
            "End of Epoch 602, Validation Loss: 0.3063059018400904, Validation Accuracy: 92.2\n",
            "End of Epoch 603, Training Loss: 0.34486155720985645, Training Accuracy: 90.90185185185186\n",
            "End of Epoch 603, Validation Loss: 0.3058749726269278, Validation Accuracy: 92.23333333333333\n",
            "End of Epoch 604, Training Loss: 0.34438800951386556, Training Accuracy: 90.9074074074074\n",
            "End of Epoch 604, Validation Loss: 0.3054460759428337, Validation Accuracy: 92.23333333333333\n",
            "End of Epoch 605, Training Loss: 0.34391649444501865, Training Accuracy: 90.92222222222223\n",
            "End of Epoch 605, Validation Loss: 0.30501919707170083, Validation Accuracy: 92.23333333333333\n",
            "End of Epoch 606, Training Loss: 0.3434469982928386, Training Accuracy: 90.92777777777778\n",
            "End of Epoch 606, Validation Loss: 0.30459432141729986, Validation Accuracy: 92.26666666666667\n",
            "End of Epoch 607, Training Loss: 0.3429795074682088, Training Accuracy: 90.93148148148148\n",
            "End of Epoch 607, Validation Loss: 0.3041714345024016, Validation Accuracy: 92.28333333333333\n",
            "End of Epoch 608, Training Loss: 0.34251400850196767, Training Accuracy: 90.9388888888889\n",
            "End of Epoch 608, Validation Loss: 0.3037505219678976, Validation Accuracy: 92.28333333333333\n",
            "End of Epoch 609, Training Loss: 0.342050488043514, Training Accuracy: 90.94074074074075\n",
            "End of Epoch 609, Validation Loss: 0.3033315695719174, Validation Accuracy: 92.26666666666667\n",
            "End of Epoch 610, Training Loss: 0.3415889328594192, Training Accuracy: 90.94814814814815\n",
            "End of Epoch 610, Validation Loss: 0.30291456318894355, Validation Accuracy: 92.28333333333333\n",
            "End of Epoch 611, Training Loss: 0.3411293298320512, Training Accuracy: 90.95185185185186\n",
            "End of Epoch 611, Validation Loss: 0.30249948880892347, Validation Accuracy: 92.28333333333333\n",
            "End of Epoch 612, Training Loss: 0.34067166595820647, Training Accuracy: 90.9574074074074\n",
            "End of Epoch 612, Validation Loss: 0.3020863325363793, Validation Accuracy: 92.28333333333333\n",
            "End of Epoch 613, Training Loss: 0.34021592834775227, Training Accuracy: 90.96481481481482\n",
            "End of Epoch 613, Validation Loss: 0.3016750805895159, Validation Accuracy: 92.28333333333333\n",
            "End of Epoch 614, Training Loss: 0.33976210422228165, Training Accuracy: 90.97592592592592\n",
            "End of Epoch 614, Validation Loss: 0.3012657192993269, Validation Accuracy: 92.28333333333333\n",
            "End of Epoch 615, Training Loss: 0.3393101809137755, Training Accuracy: 90.98518518518517\n",
            "End of Epoch 615, Validation Loss: 0.300858235108698, Validation Accuracy: 92.26666666666667\n",
            "End of Epoch 616, Training Loss: 0.33886014586328017, Training Accuracy: 90.99444444444444\n",
            "End of Epoch 616, Validation Loss: 0.30045261457151107, Validation Accuracy: 92.28333333333333\n",
            "End of Epoch 617, Training Loss: 0.3384119866195941, Training Accuracy: 91.00925925925925\n",
            "End of Epoch 617, Validation Loss: 0.3000488443517443, Validation Accuracy: 92.30000000000001\n",
            "End of Epoch 618, Training Loss: 0.33796569083796835, Training Accuracy: 91.02037037037037\n",
            "End of Epoch 618, Validation Loss: 0.299646911222574, Validation Accuracy: 92.30000000000001\n",
            "End of Epoch 619, Training Loss: 0.337521246278817, Training Accuracy: 91.02222222222223\n",
            "End of Epoch 619, Validation Loss: 0.29924680206547427, Validation Accuracy: 92.30000000000001\n",
            "End of Epoch 620, Training Loss: 0.3370786408064446, Training Accuracy: 91.03148148148148\n",
            "End of Epoch 620, Validation Loss: 0.29884850386931633, Validation Accuracy: 92.30000000000001\n",
            "End of Epoch 621, Training Loss: 0.33663786238778143, Training Accuracy: 91.0425925925926\n",
            "End of Epoch 621, Validation Loss: 0.29845200372946856, Validation Accuracy: 92.28333333333333\n",
            "End of Epoch 622, Training Loss: 0.3361988990911376, Training Accuracy: 91.05740740740741\n",
            "End of Epoch 622, Validation Loss: 0.29805728884689603, Validation Accuracy: 92.30000000000001\n",
            "End of Epoch 623, Training Loss: 0.33576173908496604, Training Accuracy: 91.07407407407408\n",
            "End of Epoch 623, Validation Loss: 0.2976643465272609, Validation Accuracy: 92.30000000000001\n",
            "End of Epoch 624, Training Loss: 0.33532637063664417, Training Accuracy: 91.08333333333334\n",
            "End of Epoch 624, Validation Loss: 0.2972731641800234, Validation Accuracy: 92.30000000000001\n",
            "End of Epoch 625, Training Loss: 0.3348927821112645, Training Accuracy: 91.07777777777778\n",
            "End of Epoch 625, Validation Loss: 0.296883729317544, Validation Accuracy: 92.31666666666666\n",
            "End of Epoch 626, Training Loss: 0.334460961970445, Training Accuracy: 91.08888888888889\n",
            "End of Epoch 626, Validation Loss: 0.2964960295541871, Validation Accuracy: 92.31666666666666\n",
            "End of Epoch 627, Training Loss: 0.33403089877115033, Training Accuracy: 91.0925925925926\n",
            "End of Epoch 627, Validation Loss: 0.2961100526054264, Validation Accuracy: 92.30000000000001\n",
            "End of Epoch 628, Training Loss: 0.33360258116453, Training Accuracy: 91.10000000000001\n",
            "End of Epoch 628, Validation Loss: 0.29572578628695173, Validation Accuracy: 92.31666666666666\n",
            "End of Epoch 629, Training Loss: 0.3331759978947711, Training Accuracy: 91.10000000000001\n",
            "End of Epoch 629, Validation Loss: 0.2953432185137791, Validation Accuracy: 92.31666666666666\n",
            "End of Epoch 630, Training Loss: 0.3327511377979653, Training Accuracy: 91.10555555555555\n",
            "End of Epoch 630, Validation Loss: 0.29496233729936244, Validation Accuracy: 92.30000000000001\n",
            "End of Epoch 631, Training Loss: 0.3323279898009931, Training Accuracy: 91.11111111111111\n",
            "End of Epoch 631, Validation Loss: 0.29458313075470915, Validation Accuracy: 92.31666666666666\n",
            "End of Epoch 632, Training Loss: 0.3319065429204222, Training Accuracy: 91.12222222222222\n",
            "End of Epoch 632, Validation Loss: 0.2942055870874978, Validation Accuracy: 92.31666666666666\n",
            "End of Epoch 633, Training Loss: 0.3314867862614221, Training Accuracy: 91.12962962962963\n",
            "End of Epoch 633, Validation Loss: 0.29382969460119995, Validation Accuracy: 92.33333333333333\n",
            "End of Epoch 634, Training Loss: 0.33106870901669355, Training Accuracy: 91.13333333333333\n",
            "End of Epoch 634, Validation Loss: 0.2934554416942058, Validation Accuracy: 92.33333333333333\n",
            "End of Epoch 635, Training Loss: 0.33065230046541505, Training Accuracy: 91.1462962962963\n",
            "End of Epoch 635, Validation Loss: 0.29308281685895327, Validation Accuracy: 92.36666666666666\n",
            "End of Epoch 636, Training Loss: 0.33023754997220467, Training Accuracy: 91.15555555555555\n",
            "End of Epoch 636, Validation Loss: 0.29271180868106234, Validation Accuracy: 92.36666666666666\n",
            "End of Epoch 637, Training Loss: 0.3298244469860973, Training Accuracy: 91.16851851851851\n",
            "End of Epoch 637, Validation Loss: 0.2923424058384723, Validation Accuracy: 92.36666666666666\n",
            "End of Epoch 638, Training Loss: 0.3294129810395379, Training Accuracy: 91.17962962962963\n",
            "End of Epoch 638, Validation Loss: 0.29197459710058615, Validation Accuracy: 92.38333333333333\n",
            "End of Epoch 639, Training Loss: 0.32900314174739115, Training Accuracy: 91.19074074074074\n",
            "End of Epoch 639, Validation Loss: 0.29160837132741785, Validation Accuracy: 92.38333333333333\n",
            "End of Epoch 640, Training Loss: 0.32859491880596703, Training Accuracy: 91.19814814814815\n",
            "End of Epoch 640, Validation Loss: 0.2912437174687462, Validation Accuracy: 92.38333333333333\n",
            "End of Epoch 641, Training Loss: 0.32818830199206056, Training Accuracy: 91.20370370370371\n",
            "End of Epoch 641, Validation Loss: 0.2908806245632736, Validation Accuracy: 92.38333333333333\n",
            "End of Epoch 642, Training Loss: 0.3277832811620104, Training Accuracy: 91.20555555555555\n",
            "End of Epoch 642, Validation Loss: 0.29051908173779123, Validation Accuracy: 92.4\n",
            "End of Epoch 643, Training Loss: 0.3273798462507694, Training Accuracy: 91.21666666666667\n",
            "End of Epoch 643, Validation Loss: 0.29015907820634984, Validation Accuracy: 92.41666666666667\n",
            "End of Epoch 644, Training Loss: 0.326977987270994, Training Accuracy: 91.23148148148148\n",
            "End of Epoch 644, Validation Loss: 0.2898006032694361, Validation Accuracy: 92.43333333333334\n",
            "End of Epoch 645, Training Loss: 0.3265776943121475, Training Accuracy: 91.24074074074075\n",
            "End of Epoch 645, Validation Loss: 0.2894436463131567, Validation Accuracy: 92.41666666666667\n",
            "End of Epoch 646, Training Loss: 0.32617895753961923, Training Accuracy: 91.2537037037037\n",
            "End of Epoch 646, Validation Loss: 0.28908819680842857, Validation Accuracy: 92.41666666666667\n",
            "End of Epoch 647, Training Loss: 0.3257817671938581, Training Accuracy: 91.26851851851852\n",
            "End of Epoch 647, Validation Loss: 0.2887342443101746, Validation Accuracy: 92.41666666666667\n",
            "End of Epoch 648, Training Loss: 0.32538611358952335, Training Accuracy: 91.27222222222223\n",
            "End of Epoch 648, Validation Loss: 0.28838177845652807, Validation Accuracy: 92.41666666666667\n",
            "End of Epoch 649, Training Loss: 0.32499198711464783, Training Accuracy: 91.27962962962964\n",
            "End of Epoch 649, Validation Loss: 0.2880307889680436, Validation Accuracy: 92.41666666666667\n",
            "End of Epoch 650, Training Loss: 0.32459937822981827, Training Accuracy: 91.29074074074074\n",
            "End of Epoch 650, Validation Loss: 0.2876812656469147, Validation Accuracy: 92.41666666666667\n",
            "End of Epoch 651, Training Loss: 0.3242082774673686, Training Accuracy: 91.30185185185185\n",
            "End of Epoch 651, Validation Loss: 0.2873331983761995, Validation Accuracy: 92.43333333333334\n",
            "End of Epoch 652, Training Loss: 0.3238186754305888, Training Accuracy: 91.32222222222222\n",
            "End of Epoch 652, Validation Loss: 0.2869865771190538, Validation Accuracy: 92.43333333333334\n",
            "End of Epoch 653, Training Loss: 0.3234305627929477, Training Accuracy: 91.33518518518518\n",
            "End of Epoch 653, Validation Loss: 0.28664139191797144, Validation Accuracy: 92.43333333333334\n",
            "End of Epoch 654, Training Loss: 0.32304393029732986, Training Accuracy: 91.35185185185185\n",
            "End of Epoch 654, Validation Loss: 0.2862976328940329, Validation Accuracy: 92.45\n",
            "End of Epoch 655, Training Loss: 0.32265876875528643, Training Accuracy: 91.35740740740741\n",
            "End of Epoch 655, Validation Loss: 0.28595529024616145, Validation Accuracy: 92.46666666666667\n",
            "End of Epoch 656, Training Loss: 0.32227506904629977, Training Accuracy: 91.35925925925926\n",
            "End of Epoch 656, Validation Loss: 0.2856143542503872, Validation Accuracy: 92.48333333333333\n",
            "End of Epoch 657, Training Loss: 0.32189282211706094, Training Accuracy: 91.36296296296297\n",
            "End of Epoch 657, Validation Loss: 0.2852748152591195, Validation Accuracy: 92.48333333333333\n",
            "End of Epoch 658, Training Loss: 0.32151201898076204, Training Accuracy: 91.37037037037037\n",
            "End of Epoch 658, Validation Loss: 0.28493666370042714, Validation Accuracy: 92.48333333333333\n",
            "End of Epoch 659, Training Loss: 0.32113265071639985, Training Accuracy: 91.38518518518518\n",
            "End of Epoch 659, Validation Loss: 0.2845998900773266, Validation Accuracy: 92.48333333333333\n",
            "End of Epoch 660, Training Loss: 0.3207547084680927, Training Accuracy: 91.39444444444445\n",
            "End of Epoch 660, Validation Loss: 0.284264484967079, Validation Accuracy: 92.46666666666667\n",
            "End of Epoch 661, Training Loss: 0.32037818344441127, Training Accuracy: 91.4074074074074\n",
            "End of Epoch 661, Validation Loss: 0.2839304390204947, Validation Accuracy: 92.48333333333333\n",
            "End of Epoch 662, Training Loss: 0.32000306691771935, Training Accuracy: 91.41851851851852\n",
            "End of Epoch 662, Validation Loss: 0.28359774296124673, Validation Accuracy: 92.48333333333333\n",
            "End of Epoch 663, Training Loss: 0.31962935022352834, Training Accuracy: 91.43333333333334\n",
            "End of Epoch 663, Validation Loss: 0.28326638758519174, Validation Accuracy: 92.51666666666667\n",
            "End of Epoch 664, Training Loss: 0.3192570247598645, Training Accuracy: 91.44444444444444\n",
            "End of Epoch 664, Validation Loss: 0.2829363637597002, Validation Accuracy: 92.5\n",
            "End of Epoch 665, Training Loss: 0.3188860819866442, Training Accuracy: 91.45\n",
            "End of Epoch 665, Validation Loss: 0.28260766242299434, Validation Accuracy: 92.5\n",
            "End of Epoch 666, Training Loss: 0.31851651342506543, Training Accuracy: 91.45925925925926\n",
            "End of Epoch 666, Validation Loss: 0.28228027458349436, Validation Accuracy: 92.5\n",
            "End of Epoch 667, Training Loss: 0.3181483106570059, Training Accuracy: 91.46481481481482\n",
            "End of Epoch 667, Validation Loss: 0.28195419131917304, Validation Accuracy: 92.5\n",
            "End of Epoch 668, Training Loss: 0.3177814653244349, Training Accuracy: 91.47222222222223\n",
            "End of Epoch 668, Validation Loss: 0.28162940377691903, Validation Accuracy: 92.51666666666667\n",
            "End of Epoch 669, Training Loss: 0.31741596912883535, Training Accuracy: 91.48888888888888\n",
            "End of Epoch 669, Validation Loss: 0.2813059031719075, Validation Accuracy: 92.51666666666667\n",
            "End of Epoch 670, Training Loss: 0.3170518138306347, Training Accuracy: 91.49259259259259\n",
            "End of Epoch 670, Validation Loss: 0.2809836807869798, Validation Accuracy: 92.53333333333333\n",
            "End of Epoch 671, Training Loss: 0.31668899124864786, Training Accuracy: 91.50370370370369\n",
            "End of Epoch 671, Validation Loss: 0.28066272797203085, Validation Accuracy: 92.53333333333333\n",
            "End of Epoch 672, Training Loss: 0.31632749325952925, Training Accuracy: 91.5074074074074\n",
            "End of Epoch 672, Validation Loss: 0.2803430361434051, Validation Accuracy: 92.51666666666667\n",
            "End of Epoch 673, Training Loss: 0.31596731179723475, Training Accuracy: 91.51481481481481\n",
            "End of Epoch 673, Validation Loss: 0.28002459678329966, Validation Accuracy: 92.51666666666667\n",
            "End of Epoch 674, Training Loss: 0.3156084388524924, Training Accuracy: 91.52222222222223\n",
            "End of Epoch 674, Validation Loss: 0.2797074014391767, Validation Accuracy: 92.53333333333333\n",
            "End of Epoch 675, Training Loss: 0.31525086647228306, Training Accuracy: 91.53703703703704\n",
            "End of Epoch 675, Validation Loss: 0.2793914417231824, Validation Accuracy: 92.53333333333333\n",
            "End of Epoch 676, Training Loss: 0.31489458675933035, Training Accuracy: 91.54629629629629\n",
            "End of Epoch 676, Validation Loss: 0.27907670931157497, Validation Accuracy: 92.53333333333333\n",
            "End of Epoch 677, Training Loss: 0.3145395918715988, Training Accuracy: 91.55\n",
            "End of Epoch 677, Validation Loss: 0.27876319594415955, Validation Accuracy: 92.55\n",
            "End of Epoch 678, Training Loss: 0.3141858740217999, Training Accuracy: 91.55740740740741\n",
            "End of Epoch 678, Validation Loss: 0.27845089342373164, Validation Accuracy: 92.56666666666666\n",
            "End of Epoch 679, Training Loss: 0.31383342547690823, Training Accuracy: 91.56666666666666\n",
            "End of Epoch 679, Validation Loss: 0.2781397936155271, Validation Accuracy: 92.55\n",
            "End of Epoch 680, Training Loss: 0.3134822385576844, Training Accuracy: 91.57592592592593\n",
            "End of Epoch 680, Validation Loss: 0.27782988844668083, Validation Accuracy: 92.55\n",
            "End of Epoch 681, Training Loss: 0.31313230563820654, Training Accuracy: 91.59074074074074\n",
            "End of Epoch 681, Validation Loss: 0.2775211699056918, Validation Accuracy: 92.51666666666667\n",
            "End of Epoch 682, Training Loss: 0.31278361914540864, Training Accuracy: 91.5962962962963\n",
            "End of Epoch 682, Validation Loss: 0.27721363004189653, Validation Accuracy: 92.55\n",
            "End of Epoch 683, Training Loss: 0.3124361715586277, Training Accuracy: 91.61296296296297\n",
            "End of Epoch 683, Validation Loss: 0.2769072609649484, Validation Accuracy: 92.56666666666666\n",
            "End of Epoch 684, Training Loss: 0.3120899554091574, Training Accuracy: 91.62777777777778\n",
            "End of Epoch 684, Validation Loss: 0.2766020548443057, Validation Accuracy: 92.56666666666666\n",
            "End of Epoch 685, Training Loss: 0.31174496327980916, Training Accuracy: 91.63148148148149\n",
            "End of Epoch 685, Validation Loss: 0.2762980039087252, Validation Accuracy: 92.56666666666666\n",
            "End of Epoch 686, Training Loss: 0.3114011878044793, Training Accuracy: 91.64999999999999\n",
            "End of Epoch 686, Validation Loss: 0.2759951004457636, Validation Accuracy: 92.56666666666666\n",
            "End of Epoch 687, Training Loss: 0.31105862166772563, Training Accuracy: 91.6648148148148\n",
            "End of Epoch 687, Validation Loss: 0.27569333680128544, Validation Accuracy: 92.56666666666666\n",
            "End of Epoch 688, Training Loss: 0.3107172576043468, Training Accuracy: 91.66296296296296\n",
            "End of Epoch 688, Validation Loss: 0.2753927053789776, Validation Accuracy: 92.60000000000001\n",
            "End of Epoch 689, Training Loss: 0.31037708839897143, Training Accuracy: 91.67222222222222\n",
            "End of Epoch 689, Validation Loss: 0.2750931986398708, Validation Accuracy: 92.60000000000001\n",
            "End of Epoch 690, Training Loss: 0.3100381068856513, Training Accuracy: 91.67222222222222\n",
            "End of Epoch 690, Validation Loss: 0.2747948091018669, Validation Accuracy: 92.65\n",
            "End of Epoch 691, Training Loss: 0.3097003059474623, Training Accuracy: 91.67777777777778\n",
            "End of Epoch 691, Validation Loss: 0.27449752933927285, Validation Accuracy: 92.65\n",
            "End of Epoch 692, Training Loss: 0.30936367851611024, Training Accuracy: 91.68518518518518\n",
            "End of Epoch 692, Validation Loss: 0.2742013519823413, Validation Accuracy: 92.66666666666666\n",
            "End of Epoch 693, Training Loss: 0.30902821757154236, Training Accuracy: 91.68703703703703\n",
            "End of Epoch 693, Validation Loss: 0.2739062697168165, Validation Accuracy: 92.66666666666666\n",
            "End of Epoch 694, Training Loss: 0.30869391614156655, Training Accuracy: 91.68888888888888\n",
            "End of Epoch 694, Validation Loss: 0.2736122752834868, Validation Accuracy: 92.68333333333332\n",
            "End of Epoch 695, Training Loss: 0.30836076730147166, Training Accuracy: 91.69259259259259\n",
            "End of Epoch 695, Validation Loss: 0.2733193614777426, Validation Accuracy: 92.73333333333333\n",
            "End of Epoch 696, Training Loss: 0.3080287641736591, Training Accuracy: 91.7037037037037\n",
            "End of Epoch 696, Validation Loss: 0.2730275211491406, Validation Accuracy: 92.73333333333333\n",
            "End of Epoch 697, Training Loss: 0.30769789992727453, Training Accuracy: 91.71296296296296\n",
            "End of Epoch 697, Validation Loss: 0.2727367472009728, Validation Accuracy: 92.75\n",
            "End of Epoch 698, Training Loss: 0.30736816777784776, Training Accuracy: 91.72407407407407\n",
            "End of Epoch 698, Validation Loss: 0.2724470325898421, Validation Accuracy: 92.76666666666667\n",
            "End of Epoch 699, Training Loss: 0.30703956098693563, Training Accuracy: 91.72962962962963\n",
            "End of Epoch 699, Validation Loss: 0.27215837032524254, Validation Accuracy: 92.76666666666667\n",
            "End of Epoch 700, Training Loss: 0.30671207286177216, Training Accuracy: 91.74074074074075\n",
            "End of Epoch 700, Validation Loss: 0.2718707534691447, Validation Accuracy: 92.80000000000001\n",
            "End of Epoch 701, Training Loss: 0.3063856967549204, Training Accuracy: 91.74814814814815\n",
            "End of Epoch 701, Validation Loss: 0.2715841751355872, Validation Accuracy: 92.80000000000001\n",
            "End of Epoch 702, Training Loss: 0.30606042606393197, Training Accuracy: 91.7537037037037\n",
            "End of Epoch 702, Validation Loss: 0.2712986284902723, Validation Accuracy: 92.83333333333333\n",
            "End of Epoch 703, Training Loss: 0.305736254231009, Training Accuracy: 91.76481481481481\n",
            "End of Epoch 703, Validation Loss: 0.2710141067501666, Validation Accuracy: 92.85\n",
            "End of Epoch 704, Training Loss: 0.3054131747426705, Training Accuracy: 91.76666666666667\n",
            "End of Epoch 704, Validation Loss: 0.2707306031831074, Validation Accuracy: 92.85\n",
            "End of Epoch 705, Training Loss: 0.305091181129426, Training Accuracy: 91.77777777777779\n",
            "End of Epoch 705, Validation Loss: 0.2704481111074124, Validation Accuracy: 92.85\n",
            "End of Epoch 706, Training Loss: 0.3047702669654481, Training Accuracy: 91.78148148148149\n",
            "End of Epoch 706, Validation Loss: 0.2701666238914957, Validation Accuracy: 92.86666666666666\n",
            "End of Epoch 707, Training Loss: 0.30445042586825455, Training Accuracy: 91.7925925925926\n",
            "End of Epoch 707, Validation Loss: 0.26988613495348657, Validation Accuracy: 92.86666666666666\n",
            "End of Epoch 708, Training Loss: 0.3041316514983906, Training Accuracy: 91.8\n",
            "End of Epoch 708, Validation Loss: 0.2696066377608537, Validation Accuracy: 92.86666666666666\n",
            "End of Epoch 709, Training Loss: 0.30381393755911773, Training Accuracy: 91.80185185185185\n",
            "End of Epoch 709, Validation Loss: 0.2693281258300339, Validation Accuracy: 92.86666666666666\n",
            "End of Epoch 710, Training Loss: 0.30349727779610375, Training Accuracy: 91.80925925925926\n",
            "End of Epoch 710, Validation Loss: 0.2690505927260646, Validation Accuracy: 92.86666666666666\n",
            "End of Epoch 711, Training Loss: 0.3031816659971196, Training Accuracy: 91.81296296296296\n",
            "End of Epoch 711, Validation Loss: 0.26877403206221967, Validation Accuracy: 92.88333333333333\n",
            "End of Epoch 712, Training Loss: 0.302867095991737, Training Accuracy: 91.82222222222222\n",
            "End of Epoch 712, Validation Loss: 0.26849843749965163, Validation Accuracy: 92.88333333333333\n",
            "End of Epoch 713, Training Loss: 0.302553561651032, Training Accuracy: 91.82962962962962\n",
            "End of Epoch 713, Validation Loss: 0.2682238027470349, Validation Accuracy: 92.88333333333333\n",
            "End of Epoch 714, Training Loss: 0.3022410568872901, Training Accuracy: 91.83148148148148\n",
            "End of Epoch 714, Validation Loss: 0.2679501215602151, Validation Accuracy: 92.91666666666667\n",
            "End of Epoch 715, Training Loss: 0.30192957565371614, Training Accuracy: 91.8425925925926\n",
            "End of Epoch 715, Validation Loss: 0.26767738774186123, Validation Accuracy: 92.93333333333334\n",
            "End of Epoch 716, Training Loss: 0.3016191119441474, Training Accuracy: 91.85\n",
            "End of Epoch 716, Validation Loss: 0.2674055951411213, Validation Accuracy: 92.93333333333334\n",
            "End of Epoch 717, Training Loss: 0.3013096597927695, Training Accuracy: 91.85740740740741\n",
            "End of Epoch 717, Validation Loss: 0.26713473765328216, Validation Accuracy: 92.93333333333334\n",
            "End of Epoch 718, Training Loss: 0.30100121327383583, Training Accuracy: 91.86296296296297\n",
            "End of Epoch 718, Validation Loss: 0.2668648092194327, Validation Accuracy: 92.93333333333334\n",
            "End of Epoch 719, Training Loss: 0.3006937665013905, Training Accuracy: 91.87407407407407\n",
            "End of Epoch 719, Validation Loss: 0.2665958038261298, Validation Accuracy: 92.95\n",
            "End of Epoch 720, Training Loss: 0.30038731362899296, Training Accuracy: 91.88703703703703\n",
            "End of Epoch 720, Validation Loss: 0.2663277155050689, Validation Accuracy: 92.95\n",
            "End of Epoch 721, Training Loss: 0.3000818488494476, Training Accuracy: 91.89074074074074\n",
            "End of Epoch 721, Validation Loss: 0.26606053833275706, Validation Accuracy: 92.96666666666667\n",
            "End of Epoch 722, Training Loss: 0.299777366394536, Training Accuracy: 91.89814814814815\n",
            "End of Epoch 722, Validation Loss: 0.265794266430189, Validation Accuracy: 92.96666666666667\n",
            "End of Epoch 723, Training Loss: 0.2994738605347501, Training Accuracy: 91.91111111111111\n",
            "End of Epoch 723, Validation Loss: 0.26552889396252743, Validation Accuracy: 92.98333333333333\n",
            "End of Epoch 724, Training Loss: 0.29917132557903053, Training Accuracy: 91.92037037037038\n",
            "End of Epoch 724, Validation Loss: 0.2652644151387852, Validation Accuracy: 92.98333333333333\n",
            "End of Epoch 725, Training Loss: 0.2988697558745071, Training Accuracy: 91.92222222222223\n",
            "End of Epoch 725, Validation Loss: 0.2650008242115116, Validation Accuracy: 92.98333333333333\n",
            "End of Epoch 726, Training Loss: 0.2985691458062414, Training Accuracy: 91.93148148148148\n",
            "End of Epoch 726, Validation Loss: 0.2647381154764808, Validation Accuracy: 92.98333333333333\n",
            "End of Epoch 727, Training Loss: 0.298269489796973, Training Accuracy: 91.94074074074075\n",
            "End of Epoch 727, Validation Loss: 0.2644762832723841, Validation Accuracy: 93.0\n",
            "End of Epoch 728, Training Loss: 0.29797078230686735, Training Accuracy: 91.9425925925926\n",
            "End of Epoch 728, Validation Loss: 0.2642153219805243, Validation Accuracy: 93.01666666666667\n",
            "End of Epoch 729, Training Loss: 0.29767301783326755, Training Accuracy: 91.95555555555556\n",
            "End of Epoch 729, Validation Loss: 0.2639552260245134, Validation Accuracy: 93.01666666666667\n",
            "End of Epoch 730, Training Loss: 0.2973761909104474, Training Accuracy: 91.96666666666667\n",
            "End of Epoch 730, Validation Loss: 0.2636959898699731, Validation Accuracy: 93.01666666666667\n",
            "End of Epoch 731, Training Loss: 0.2970802961093677, Training Accuracy: 91.97777777777777\n",
            "End of Epoch 731, Validation Loss: 0.26343760802423793, Validation Accuracy: 93.03333333333333\n",
            "End of Epoch 732, Training Loss: 0.29678532803743496, Training Accuracy: 91.98703703703703\n",
            "End of Epoch 732, Validation Loss: 0.263180075036061, Validation Accuracy: 93.05\n",
            "End of Epoch 733, Training Loss: 0.2964912813382623, Training Accuracy: 91.99444444444444\n",
            "End of Epoch 733, Validation Loss: 0.26292338549532307, Validation Accuracy: 93.06666666666666\n",
            "End of Epoch 734, Training Loss: 0.2961981506914327, Training Accuracy: 92.00925925925925\n",
            "End of Epoch 734, Validation Loss: 0.2626675340327434, Validation Accuracy: 93.05\n",
            "End of Epoch 735, Training Loss: 0.29590593081226535, Training Accuracy: 92.01296296296296\n",
            "End of Epoch 735, Validation Loss: 0.26241251531959425, Validation Accuracy: 93.05\n",
            "End of Epoch 736, Training Loss: 0.2956146164515829, Training Accuracy: 92.02222222222223\n",
            "End of Epoch 736, Validation Loss: 0.26215832406741707, Validation Accuracy: 93.05\n",
            "End of Epoch 737, Training Loss: 0.29532420239548235, Training Accuracy: 92.02777777777777\n",
            "End of Epoch 737, Validation Loss: 0.26190495502774186, Validation Accuracy: 93.05\n",
            "End of Epoch 738, Training Loss: 0.2950346834651077, Training Accuracy: 92.0425925925926\n",
            "End of Epoch 738, Validation Loss: 0.2616524029918092, Validation Accuracy: 93.05\n",
            "End of Epoch 739, Training Loss: 0.2947460545164238, Training Accuracy: 92.04814814814814\n",
            "End of Epoch 739, Validation Loss: 0.26140066279029434, Validation Accuracy: 93.05\n",
            "End of Epoch 740, Training Loss: 0.29445831043999415, Training Accuracy: 92.05555555555556\n",
            "End of Epoch 740, Validation Loss: 0.2611497292930342, Validation Accuracy: 93.06666666666666\n",
            "End of Epoch 741, Training Loss: 0.2941714461607597, Training Accuracy: 92.05555555555556\n",
            "End of Epoch 741, Validation Loss: 0.2608995974087569, Validation Accuracy: 93.08333333333333\n",
            "End of Epoch 742, Training Loss: 0.29388545663781973, Training Accuracy: 92.05925925925926\n",
            "End of Epoch 742, Validation Loss: 0.2606502620848138, Validation Accuracy: 93.08333333333333\n",
            "End of Epoch 743, Training Loss: 0.29360033686421466, Training Accuracy: 92.06481481481481\n",
            "End of Epoch 743, Validation Loss: 0.26040171830691383, Validation Accuracy: 93.08333333333333\n",
            "End of Epoch 744, Training Loss: 0.293316081866712, Training Accuracy: 92.07037037037037\n",
            "End of Epoch 744, Validation Loss: 0.2601539610988607, Validation Accuracy: 93.08333333333333\n",
            "End of Epoch 745, Training Loss: 0.2930326867055929, Training Accuracy: 92.08333333333333\n",
            "End of Epoch 745, Validation Loss: 0.2599069855222924, Validation Accuracy: 93.08333333333333\n",
            "End of Epoch 746, Training Loss: 0.2927501464744412, Training Accuracy: 92.08518518518518\n",
            "End of Epoch 746, Validation Loss: 0.2596607866764232, Validation Accuracy: 93.08333333333333\n",
            "End of Epoch 747, Training Loss: 0.2924684562999345, Training Accuracy: 92.08518518518518\n",
            "End of Epoch 747, Validation Loss: 0.25941535969778823, Validation Accuracy: 93.08333333333333\n",
            "End of Epoch 748, Training Loss: 0.2921876113416374, Training Accuracy: 92.0925925925926\n",
            "End of Epoch 748, Validation Loss: 0.25917069975999063, Validation Accuracy: 93.08333333333333\n",
            "End of Epoch 749, Training Loss: 0.29190760679179595, Training Accuracy: 92.0925925925926\n",
            "End of Epoch 749, Validation Loss: 0.25892680207345076, Validation Accuracy: 93.08333333333333\n",
            "End of Epoch 750, Training Loss: 0.2916284378751339, Training Accuracy: 92.0925925925926\n",
            "End of Epoch 750, Validation Loss: 0.25868366188515896, Validation Accuracy: 93.08333333333333\n",
            "End of Epoch 751, Training Loss: 0.29135009984865107, Training Accuracy: 92.09444444444445\n",
            "End of Epoch 751, Validation Loss: 0.2584412744784295, Validation Accuracy: 93.06666666666666\n",
            "End of Epoch 752, Training Loss: 0.2910725880014252, Training Accuracy: 92.09814814814816\n",
            "End of Epoch 752, Validation Loss: 0.25819963517265837, Validation Accuracy: 93.06666666666666\n",
            "End of Epoch 753, Training Loss: 0.2907958976544113, Training Accuracy: 92.11296296296297\n",
            "End of Epoch 753, Validation Loss: 0.25795873932308244, Validation Accuracy: 93.08333333333333\n",
            "End of Epoch 754, Training Loss: 0.2905200241602469, Training Accuracy: 92.12222222222223\n",
            "End of Epoch 754, Validation Loss: 0.25771858232054184, Validation Accuracy: 93.10000000000001\n",
            "End of Epoch 755, Training Loss: 0.2902449629030576, Training Accuracy: 92.12962962962963\n",
            "End of Epoch 755, Validation Loss: 0.25747915959124507, Validation Accuracy: 93.10000000000001\n",
            "End of Epoch 756, Training Loss: 0.28997070929826313, Training Accuracy: 92.1351851851852\n",
            "End of Epoch 756, Validation Loss: 0.25724046659653643, Validation Accuracy: 93.10000000000001\n",
            "End of Epoch 757, Training Loss: 0.2896972587923869, Training Accuracy: 92.13888888888889\n",
            "End of Epoch 757, Validation Loss: 0.2570024988326654, Validation Accuracy: 93.10000000000001\n",
            "End of Epoch 758, Training Loss: 0.28942460686286603, Training Accuracy: 92.1462962962963\n",
            "End of Epoch 758, Validation Loss: 0.25676525183056, Validation Accuracy: 93.10000000000001\n",
            "End of Epoch 759, Training Loss: 0.28915274901786325, Training Accuracy: 92.15\n",
            "End of Epoch 759, Validation Loss: 0.2565287211556017, Validation Accuracy: 93.10000000000001\n",
            "End of Epoch 760, Training Loss: 0.28888168079608123, Training Accuracy: 92.16111111111111\n",
            "End of Epoch 760, Validation Loss: 0.2562929024074035, Validation Accuracy: 93.10000000000001\n",
            "End of Epoch 761, Training Loss: 0.2886113977665767, Training Accuracy: 92.17222222222222\n",
            "End of Epoch 761, Validation Loss: 0.25605779121959016, Validation Accuracy: 93.10000000000001\n",
            "End of Epoch 762, Training Loss: 0.28834189552857814, Training Accuracy: 92.18333333333332\n",
            "End of Epoch 762, Validation Loss: 0.2558233832595818, Validation Accuracy: 93.13333333333334\n",
            "End of Epoch 763, Training Loss: 0.28807316971130376, Training Accuracy: 92.19074074074074\n",
            "End of Epoch 763, Validation Loss: 0.2555896742283797, Validation Accuracy: 93.13333333333334\n",
            "End of Epoch 764, Training Loss: 0.28780521597378095, Training Accuracy: 92.19814814814815\n",
            "End of Epoch 764, Validation Loss: 0.2553566598603551, Validation Accuracy: 93.13333333333334\n",
            "End of Epoch 765, Training Loss: 0.28753803000466893, Training Accuracy: 92.2037037037037\n",
            "End of Epoch 765, Validation Loss: 0.2551243359230403, Validation Accuracy: 93.13333333333334\n",
            "End of Epoch 766, Training Loss: 0.28727160752207986, Training Accuracy: 92.2074074074074\n",
            "End of Epoch 766, Validation Loss: 0.25489269821692323, Validation Accuracy: 93.13333333333334\n",
            "End of Epoch 767, Training Loss: 0.2870059442734045, Training Accuracy: 92.21296296296296\n",
            "End of Epoch 767, Validation Loss: 0.25466174257524415, Validation Accuracy: 93.15\n",
            "End of Epoch 768, Training Loss: 0.2867410360351376, Training Accuracy: 92.22222222222223\n",
            "End of Epoch 768, Validation Loss: 0.25443146486379564, Validation Accuracy: 93.15\n",
            "End of Epoch 769, Training Loss: 0.2864768786127056, Training Accuracy: 92.22407407407407\n",
            "End of Epoch 769, Validation Loss: 0.25420186098072467, Validation Accuracy: 93.15\n",
            "End of Epoch 770, Training Loss: 0.2862134678402943, Training Accuracy: 92.23333333333333\n",
            "End of Epoch 770, Validation Loss: 0.25397292685633865, Validation Accuracy: 93.15\n",
            "End of Epoch 771, Training Loss: 0.2859507995806805, Training Accuracy: 92.2388888888889\n",
            "End of Epoch 771, Validation Loss: 0.2537446584529132, Validation Accuracy: 93.15\n",
            "End of Epoch 772, Training Loss: 0.28568886972506247, Training Accuracy: 92.24444444444444\n",
            "End of Epoch 772, Validation Loss: 0.2535170517645034, Validation Accuracy: 93.2\n",
            "End of Epoch 773, Training Loss: 0.28542767419289333, Training Accuracy: 92.2537037037037\n",
            "End of Epoch 773, Validation Loss: 0.25329010281675784, Validation Accuracy: 93.2\n",
            "End of Epoch 774, Training Loss: 0.2851672089317159, Training Accuracy: 92.25185185185185\n",
            "End of Epoch 774, Validation Loss: 0.2530638076667356, Validation Accuracy: 93.21666666666667\n",
            "End of Epoch 775, Training Loss: 0.2849074699169971, Training Accuracy: 92.25555555555556\n",
            "End of Epoch 775, Validation Loss: 0.25283816240272594, Validation Accuracy: 93.23333333333333\n",
            "End of Epoch 776, Training Loss: 0.2846484531519672, Training Accuracy: 92.25555555555556\n",
            "End of Epoch 776, Validation Loss: 0.2526131631440711, Validation Accuracy: 93.23333333333333\n",
            "End of Epoch 777, Training Loss: 0.2843901546674567, Training Accuracy: 92.26666666666667\n",
            "End of Epoch 777, Validation Loss: 0.252388806040992, Validation Accuracy: 93.23333333333333\n",
            "End of Epoch 778, Training Loss: 0.2841325705217366, Training Accuracy: 92.27592592592593\n",
            "End of Epoch 778, Validation Loss: 0.25216508727441705, Validation Accuracy: 93.25\n",
            "End of Epoch 779, Training Loss: 0.28387569680035996, Training Accuracy: 92.27962962962964\n",
            "End of Epoch 779, Validation Loss: 0.2519420030558132, Validation Accuracy: 93.25\n",
            "End of Epoch 780, Training Loss: 0.28361952961600423, Training Accuracy: 92.29074074074074\n",
            "End of Epoch 780, Validation Loss: 0.2517195496270214, Validation Accuracy: 93.26666666666667\n",
            "End of Epoch 781, Training Loss: 0.283364065108315, Training Accuracy: 92.29444444444445\n",
            "End of Epoch 781, Validation Loss: 0.25149772326009345, Validation Accuracy: 93.28333333333333\n",
            "End of Epoch 782, Training Loss: 0.28310929944375185, Training Accuracy: 92.29814814814816\n",
            "End of Epoch 782, Validation Loss: 0.2512765202571329, Validation Accuracy: 93.28333333333333\n",
            "End of Epoch 783, Training Loss: 0.28285522881543385, Training Accuracy: 92.30555555555556\n",
            "End of Epoch 783, Validation Loss: 0.2510559369501385, Validation Accuracy: 93.28333333333333\n",
            "End of Epoch 784, Training Loss: 0.2826018494429884, Training Accuracy: 92.31666666666666\n",
            "End of Epoch 784, Validation Loss: 0.2508359697008507, Validation Accuracy: 93.28333333333333\n",
            "End of Epoch 785, Training Loss: 0.28234915757239953, Training Accuracy: 92.31666666666666\n",
            "End of Epoch 785, Validation Loss: 0.25061661490060133, Validation Accuracy: 93.30000000000001\n",
            "End of Epoch 786, Training Loss: 0.2820971494758589, Training Accuracy: 92.32222222222222\n",
            "End of Epoch 786, Validation Loss: 0.25039786897016614, Validation Accuracy: 93.33333333333333\n",
            "End of Epoch 787, Training Loss: 0.281845821451617, Training Accuracy: 92.31666666666666\n",
            "End of Epoch 787, Validation Loss: 0.2501797283596198, Validation Accuracy: 93.35\n",
            "End of Epoch 788, Training Loss: 0.28159516982383637, Training Accuracy: 92.33518518518518\n",
            "End of Epoch 788, Validation Loss: 0.24996218954819535, Validation Accuracy: 93.35\n",
            "End of Epoch 789, Training Loss: 0.28134519094244587, Training Accuracy: 92.33888888888889\n",
            "End of Epoch 789, Validation Loss: 0.2497452490441446, Validation Accuracy: 93.36666666666666\n",
            "End of Epoch 790, Training Loss: 0.28109588118299533, Training Accuracy: 92.34074074074074\n",
            "End of Epoch 790, Validation Loss: 0.24952890338460326, Validation Accuracy: 93.36666666666666\n",
            "End of Epoch 791, Training Loss: 0.2808472369465137, Training Accuracy: 92.34629629629629\n",
            "End of Epoch 791, Validation Loss: 0.24931314913545832, Validation Accuracy: 93.36666666666666\n",
            "End of Epoch 792, Training Loss: 0.28059925465936514, Training Accuracy: 92.34629629629629\n",
            "End of Epoch 792, Validation Loss: 0.2490979828912178, Validation Accuracy: 93.36666666666666\n",
            "End of Epoch 793, Training Loss: 0.2803519307731098, Training Accuracy: 92.34814814814814\n",
            "End of Epoch 793, Validation Loss: 0.24888340127488456, Validation Accuracy: 93.36666666666666\n",
            "End of Epoch 794, Training Loss: 0.2801052617643625, Training Accuracy: 92.35740740740741\n",
            "End of Epoch 794, Validation Loss: 0.2486694009378322, Validation Accuracy: 93.4\n",
            "End of Epoch 795, Training Loss: 0.27985924413465557, Training Accuracy: 92.3648148148148\n",
            "End of Epoch 795, Validation Loss: 0.24845597855968393, Validation Accuracy: 93.41666666666667\n",
            "End of Epoch 796, Training Loss: 0.2796138744103008, Training Accuracy: 92.37407407407407\n",
            "End of Epoch 796, Validation Loss: 0.24824313084819463, Validation Accuracy: 93.41666666666667\n",
            "End of Epoch 797, Training Loss: 0.27936914914225325, Training Accuracy: 92.37962962962963\n",
            "End of Epoch 797, Validation Loss: 0.2480308545391352, Validation Accuracy: 93.41666666666667\n",
            "End of Epoch 798, Training Loss: 0.2791250649059772, Training Accuracy: 92.38888888888889\n",
            "End of Epoch 798, Validation Loss: 0.2478191463961806, Validation Accuracy: 93.41666666666667\n",
            "End of Epoch 799, Training Loss: 0.2788816183013102, Training Accuracy: 92.3962962962963\n",
            "End of Epoch 799, Validation Loss: 0.24760800321079915, Validation Accuracy: 93.43333333333334\n",
            "End of Epoch 800, Training Loss: 0.27863880595233276, Training Accuracy: 92.4\n",
            "End of Epoch 800, Validation Loss: 0.2473974218021461, Validation Accuracy: 93.43333333333334\n",
            "End of Epoch 801, Training Loss: 0.2783966245072346, Training Accuracy: 92.40555555555555\n",
            "End of Epoch 801, Validation Loss: 0.24718739901695905, Validation Accuracy: 93.43333333333334\n",
            "End of Epoch 802, Training Loss: 0.2781550706381857, Training Accuracy: 92.40555555555555\n",
            "End of Epoch 802, Validation Loss: 0.24697793172945584, Validation Accuracy: 93.43333333333334\n",
            "End of Epoch 803, Training Loss: 0.2779141410412054, Training Accuracy: 92.41296296296296\n",
            "End of Epoch 803, Validation Loss: 0.24676901684123576, Validation Accuracy: 93.43333333333334\n",
            "End of Epoch 804, Training Loss: 0.27767383243603483, Training Accuracy: 92.41666666666667\n",
            "End of Epoch 804, Validation Loss: 0.24656065128118237, Validation Accuracy: 93.43333333333334\n",
            "End of Epoch 805, Training Loss: 0.2774341415660095, Training Accuracy: 92.42777777777778\n",
            "End of Epoch 805, Validation Loss: 0.2463528320053696, Validation Accuracy: 93.43333333333334\n",
            "End of Epoch 806, Training Loss: 0.2771950651979325, Training Accuracy: 92.42962962962963\n",
            "End of Epoch 806, Validation Loss: 0.2461455559969695, Validation Accuracy: 93.43333333333334\n",
            "End of Epoch 807, Training Loss: 0.2769566001219487, Training Accuracy: 92.43518518518519\n",
            "End of Epoch 807, Validation Loss: 0.24593882026616326, Validation Accuracy: 93.43333333333334\n",
            "End of Epoch 808, Training Loss: 0.27671874315142175, Training Accuracy: 92.44444444444444\n",
            "End of Epoch 808, Validation Loss: 0.24573262185005326, Validation Accuracy: 93.43333333333334\n",
            "End of Epoch 809, Training Loss: 0.27648149112280906, Training Accuracy: 92.45370370370371\n",
            "End of Epoch 809, Validation Loss: 0.24552695781257888, Validation Accuracy: 93.41666666666667\n",
            "End of Epoch 810, Training Loss: 0.2762448408955397, Training Accuracy: 92.45740740740742\n",
            "End of Epoch 810, Validation Loss: 0.2453218252444328, Validation Accuracy: 93.46666666666667\n",
            "End of Epoch 811, Training Loss: 0.27600878935189294, Training Accuracy: 92.46111111111112\n",
            "End of Epoch 811, Validation Loss: 0.24511722126298047, Validation Accuracy: 93.46666666666667\n",
            "End of Epoch 812, Training Loss: 0.2757733333968773, Training Accuracy: 92.47037037037038\n",
            "End of Epoch 812, Validation Loss: 0.24491314301218164, Validation Accuracy: 93.45\n",
            "End of Epoch 813, Training Loss: 0.27553846995811043, Training Accuracy: 92.47407407407408\n",
            "End of Epoch 813, Validation Loss: 0.2447095876625125, Validation Accuracy: 93.45\n",
            "End of Epoch 814, Training Loss: 0.27530419598569944, Training Accuracy: 92.47962962962963\n",
            "End of Epoch 814, Validation Loss: 0.244506552410891, Validation Accuracy: 93.45\n",
            "End of Epoch 815, Training Loss: 0.27507050845212383, Training Accuracy: 92.49074074074073\n",
            "End of Epoch 815, Validation Loss: 0.24430403448060345, Validation Accuracy: 93.43333333333334\n",
            "End of Epoch 816, Training Loss: 0.27483740435211634, Training Accuracy: 92.4962962962963\n",
            "End of Epoch 816, Validation Loss: 0.24410203112123216, Validation Accuracy: 93.43333333333334\n",
            "End of Epoch 817, Training Loss: 0.27460488070254707, Training Accuracy: 92.50185185185185\n",
            "End of Epoch 817, Validation Loss: 0.2439005396085855, Validation Accuracy: 93.46666666666667\n",
            "End of Epoch 818, Training Loss: 0.27437293454230616, Training Accuracy: 92.50555555555555\n",
            "End of Epoch 818, Validation Loss: 0.24369955724462833, Validation Accuracy: 93.48333333333333\n",
            "End of Epoch 819, Training Loss: 0.2741415629321889, Training Accuracy: 92.50555555555555\n",
            "End of Epoch 819, Validation Loss: 0.24349908135741474, Validation Accuracy: 93.5\n",
            "End of Epoch 820, Training Loss: 0.273910762954781, Training Accuracy: 92.52222222222221\n",
            "End of Epoch 820, Validation Loss: 0.24329910930102128, Validation Accuracy: 93.48333333333333\n",
            "End of Epoch 821, Training Loss: 0.2736805317143426, Training Accuracy: 92.52592592592592\n",
            "End of Epoch 821, Validation Loss: 0.2430996384554811, Validation Accuracy: 93.46666666666667\n",
            "End of Epoch 822, Training Loss: 0.27345086633669613, Training Accuracy: 92.53888888888889\n",
            "End of Epoch 822, Validation Loss: 0.2429006662267199, Validation Accuracy: 93.48333333333333\n",
            "End of Epoch 823, Training Loss: 0.2732217639691114, Training Accuracy: 92.54444444444444\n",
            "End of Epoch 823, Validation Loss: 0.2427021900464915, Validation Accuracy: 93.5\n",
            "End of Epoch 824, Training Loss: 0.2729932217801936, Training Accuracy: 92.55\n",
            "End of Epoch 824, Validation Loss: 0.242504207372315, Validation Accuracy: 93.51666666666667\n",
            "End of Epoch 825, Training Loss: 0.27276523695976995, Training Accuracy: 92.5537037037037\n",
            "End of Epoch 825, Validation Loss: 0.2423067156874121, Validation Accuracy: 93.51666666666667\n",
            "End of Epoch 826, Training Loss: 0.27253780671877803, Training Accuracy: 92.55740740740741\n",
            "End of Epoch 826, Validation Loss: 0.24210971250064434, Validation Accuracy: 93.51666666666667\n",
            "End of Epoch 827, Training Loss: 0.27231092828915354, Training Accuracy: 92.56666666666666\n",
            "End of Epoch 827, Validation Loss: 0.2419131953464519, Validation Accuracy: 93.53333333333333\n",
            "End of Epoch 828, Training Loss: 0.2720845989237186, Training Accuracy: 92.56851851851852\n",
            "End of Epoch 828, Validation Loss: 0.2417171617847905, Validation Accuracy: 93.53333333333333\n",
            "End of Epoch 829, Training Loss: 0.2718588158960703, Training Accuracy: 92.57962962962964\n",
            "End of Epoch 829, Validation Loss: 0.24152160940107054, Validation Accuracy: 93.53333333333333\n",
            "End of Epoch 830, Training Loss: 0.2716335765004701, Training Accuracy: 92.59074074074074\n",
            "End of Epoch 830, Validation Loss: 0.24132653580609378, Validation Accuracy: 93.53333333333333\n",
            "End of Epoch 831, Training Loss: 0.271408878051732, Training Accuracy: 92.60185185185185\n",
            "End of Epoch 831, Validation Loss: 0.24113193863599136, Validation Accuracy: 93.53333333333333\n",
            "End of Epoch 832, Training Loss: 0.271184717885112, Training Accuracy: 92.6037037037037\n",
            "End of Epoch 832, Validation Loss: 0.24093781555216018, Validation Accuracy: 93.53333333333333\n",
            "End of Epoch 833, Training Loss: 0.27096109335619695, Training Accuracy: 92.60740740740741\n",
            "End of Epoch 833, Validation Loss: 0.24074416424119915, Validation Accuracy: 93.55\n",
            "End of Epoch 834, Training Loss: 0.2707380018407939, Training Accuracy: 92.61296296296297\n",
            "End of Epoch 834, Validation Loss: 0.24055098241484446, Validation Accuracy: 93.53333333333333\n",
            "End of Epoch 835, Training Loss: 0.27051544073481887, Training Accuracy: 92.61666666666667\n",
            "End of Epoch 835, Validation Loss: 0.24035826780990335, Validation Accuracy: 93.51666666666667\n",
            "End of Epoch 836, Training Loss: 0.2702934074541861, Training Accuracy: 92.62222222222222\n",
            "End of Epoch 836, Validation Loss: 0.24016601818818742, Validation Accuracy: 93.51666666666667\n",
            "End of Epoch 837, Training Loss: 0.2700718994346969, Training Accuracy: 92.62962962962963\n",
            "End of Epoch 837, Validation Loss: 0.23997423133644363, Validation Accuracy: 93.53333333333333\n",
            "End of Epoch 838, Training Loss: 0.2698509141319279, Training Accuracy: 92.63333333333334\n",
            "End of Epoch 838, Validation Loss: 0.2397829050662845, Validation Accuracy: 93.55\n",
            "End of Epoch 839, Training Loss: 0.2696304490211206, Training Accuracy: 92.63888888888889\n",
            "End of Epoch 839, Validation Loss: 0.23959203721411587, Validation Accuracy: 93.55\n",
            "End of Epoch 840, Training Loss: 0.269410501597068, Training Accuracy: 92.64444444444445\n",
            "End of Epoch 840, Validation Loss: 0.2394016256410632, Validation Accuracy: 93.55\n",
            "End of Epoch 841, Training Loss: 0.2691910693740041, Training Accuracy: 92.64444444444445\n",
            "End of Epoch 841, Validation Loss: 0.23921166823289575, Validation Accuracy: 93.55\n",
            "End of Epoch 842, Training Loss: 0.26897214988549023, Training Accuracy: 92.65\n",
            "End of Epoch 842, Validation Loss: 0.2390221628999473, Validation Accuracy: 93.53333333333333\n",
            "End of Epoch 843, Training Loss: 0.26875374068430347, Training Accuracy: 92.65555555555555\n",
            "End of Epoch 843, Validation Loss: 0.23883310757703594, Validation Accuracy: 93.53333333333333\n",
            "End of Epoch 844, Training Loss: 0.26853583934232217, Training Accuracy: 92.6574074074074\n",
            "End of Epoch 844, Validation Loss: 0.23864450022338016, Validation Accuracy: 93.53333333333333\n",
            "End of Epoch 845, Training Loss: 0.26831844345041284, Training Accuracy: 92.6574074074074\n",
            "End of Epoch 845, Validation Loss: 0.23845633882251197, Validation Accuracy: 93.53333333333333\n",
            "End of Epoch 846, Training Loss: 0.2681015506183164, Training Accuracy: 92.66851851851851\n",
            "End of Epoch 846, Validation Loss: 0.23826862138218755, Validation Accuracy: 93.55\n",
            "End of Epoch 847, Training Loss: 0.26788515847453287, Training Accuracy: 92.67222222222222\n",
            "End of Epoch 847, Validation Loss: 0.2380813459342935, Validation Accuracy: 93.53333333333333\n",
            "End of Epoch 848, Training Loss: 0.2676692646662062, Training Accuracy: 92.67037037037036\n",
            "End of Epoch 848, Validation Loss: 0.23789451053475114, Validation Accuracy: 93.53333333333333\n",
            "End of Epoch 849, Training Loss: 0.26745386685900807, Training Accuracy: 92.67407407407407\n",
            "End of Epoch 849, Validation Loss: 0.23770811326341526, Validation Accuracy: 93.53333333333333\n",
            "End of Epoch 850, Training Loss: 0.26723896273702247, Training Accuracy: 92.68518518518518\n",
            "End of Epoch 850, Validation Loss: 0.23752215222397058, Validation Accuracy: 93.55\n",
            "End of Epoch 851, Training Loss: 0.26702455000262715, Training Accuracy: 92.68518518518518\n",
            "End of Epoch 851, Validation Loss: 0.2373366255438233, Validation Accuracy: 93.56666666666666\n",
            "End of Epoch 852, Training Loss: 0.2668106263763766, Training Accuracy: 92.69444444444444\n",
            "End of Epoch 852, Validation Loss: 0.23715153137398823, Validation Accuracy: 93.56666666666666\n",
            "End of Epoch 853, Training Loss: 0.26659718959688317, Training Accuracy: 92.69444444444444\n",
            "End of Epoch 853, Validation Loss: 0.23696686788897234, Validation Accuracy: 93.58333333333333\n",
            "End of Epoch 854, Training Loss: 0.2663842374206978, Training Accuracy: 92.7037037037037\n",
            "End of Epoch 854, Validation Loss: 0.23678263328665286, Validation Accuracy: 93.58333333333333\n",
            "End of Epoch 855, Training Loss: 0.2661717676221901, Training Accuracy: 92.7074074074074\n",
            "End of Epoch 855, Validation Loss: 0.23659882578815059, Validation Accuracy: 93.60000000000001\n",
            "End of Epoch 856, Training Loss: 0.26595977799342696, Training Accuracy: 92.7074074074074\n",
            "End of Epoch 856, Validation Loss: 0.2364154436376991, Validation Accuracy: 93.60000000000001\n",
            "End of Epoch 857, Training Loss: 0.2657482663440513, Training Accuracy: 92.71111111111111\n",
            "End of Epoch 857, Validation Loss: 0.2362324851025074, Validation Accuracy: 93.60000000000001\n",
            "End of Epoch 858, Training Loss: 0.2655372305011603, Training Accuracy: 92.71481481481482\n",
            "End of Epoch 858, Validation Loss: 0.2360499484726186, Validation Accuracy: 93.61666666666667\n",
            "End of Epoch 859, Training Loss: 0.2653266683091802, Training Accuracy: 92.72222222222221\n",
            "End of Epoch 859, Validation Loss: 0.23586783206076248, Validation Accuracy: 93.61666666666667\n",
            "End of Epoch 860, Training Loss: 0.2651165776297446, Training Accuracy: 92.73148148148148\n",
            "End of Epoch 860, Validation Loss: 0.2356861342022023, Validation Accuracy: 93.61666666666667\n",
            "End of Epoch 861, Training Loss: 0.2649069563415677, Training Accuracy: 92.7388888888889\n",
            "End of Epoch 861, Validation Loss: 0.23550485325457637, Validation Accuracy: 93.61666666666667\n",
            "End of Epoch 862, Training Loss: 0.26469780234031925, Training Accuracy: 92.7462962962963\n",
            "End of Epoch 862, Validation Loss: 0.2353239875977331, Validation Accuracy: 93.61666666666667\n",
            "End of Epoch 863, Training Loss: 0.2644891135384986, Training Accuracy: 92.7462962962963\n",
            "End of Epoch 863, Validation Loss: 0.23514353563356047, Validation Accuracy: 93.63333333333334\n",
            "End of Epoch 864, Training Loss: 0.2642808878653059, Training Accuracy: 92.75555555555556\n",
            "End of Epoch 864, Validation Loss: 0.23496349578580933, Validation Accuracy: 93.65\n",
            "End of Epoch 865, Training Loss: 0.26407312326651505, Training Accuracy: 92.76296296296296\n",
            "End of Epoch 865, Validation Loss: 0.23478386649991026, Validation Accuracy: 93.65\n",
            "End of Epoch 866, Training Loss: 0.26386581770434453, Training Accuracy: 92.76111111111112\n",
            "End of Epoch 866, Validation Loss: 0.23460464624278377, Validation Accuracy: 93.63333333333334\n",
            "End of Epoch 867, Training Loss: 0.26365896915732684, Training Accuracy: 92.76851851851852\n",
            "End of Epoch 867, Validation Loss: 0.23442583350264526, Validation Accuracy: 93.63333333333334\n",
            "End of Epoch 868, Training Loss: 0.2634525756201788, Training Accuracy: 92.77407407407408\n",
            "End of Epoch 868, Validation Loss: 0.2342474267888016, Validation Accuracy: 93.63333333333334\n",
            "End of Epoch 869, Training Loss: 0.2632466351036698, Training Accuracy: 92.77592592592593\n",
            "End of Epoch 869, Validation Loss: 0.23406942463144273, Validation Accuracy: 93.63333333333334\n",
            "End of Epoch 870, Training Loss: 0.2630411456344892, Training Accuracy: 92.78333333333333\n",
            "End of Epoch 870, Validation Loss: 0.2338918255814257, Validation Accuracy: 93.65\n",
            "End of Epoch 871, Training Loss: 0.2628361052551144, Training Accuracy: 92.78888888888889\n",
            "End of Epoch 871, Validation Loss: 0.2337146282100526, Validation Accuracy: 93.66666666666667\n",
            "End of Epoch 872, Training Loss: 0.262631512023677, Training Accuracy: 92.7925925925926\n",
            "End of Epoch 872, Validation Loss: 0.23353783110884074, Validation Accuracy: 93.66666666666667\n",
            "End of Epoch 873, Training Loss: 0.262427364013828, Training Accuracy: 92.80185185185185\n",
            "End of Epoch 873, Validation Loss: 0.23336143288928732, Validation Accuracy: 93.66666666666667\n",
            "End of Epoch 874, Training Loss: 0.26222365931460345, Training Accuracy: 92.80185185185185\n",
            "End of Epoch 874, Validation Loss: 0.2331854321826266, Validation Accuracy: 93.66666666666667\n",
            "End of Epoch 875, Training Loss: 0.2620203960302892, Training Accuracy: 92.80000000000001\n",
            "End of Epoch 875, Validation Loss: 0.23300982763958028, Validation Accuracy: 93.66666666666667\n",
            "End of Epoch 876, Training Loss: 0.2618175722802841, Training Accuracy: 92.80925925925926\n",
            "End of Epoch 876, Validation Loss: 0.2328346179301012, Validation Accuracy: 93.66666666666667\n",
            "End of Epoch 877, Training Loss: 0.26161518619896407, Training Accuracy: 92.81296296296297\n",
            "End of Epoch 877, Validation Loss: 0.23265980174311082, Validation Accuracy: 93.66666666666667\n",
            "End of Epoch 878, Training Loss: 0.2614132359355453, Training Accuracy: 92.81666666666666\n",
            "End of Epoch 878, Validation Loss: 0.23248537778622907, Validation Accuracy: 93.68333333333332\n",
            "End of Epoch 879, Training Loss: 0.26121171965394496, Training Accuracy: 92.82222222222222\n",
            "End of Epoch 879, Validation Loss: 0.2323113447854985, Validation Accuracy: 93.7\n",
            "End of Epoch 880, Training Loss: 0.26101063553264586, Training Accuracy: 92.82222222222222\n",
            "End of Epoch 880, Validation Loss: 0.23213770148510135, Validation Accuracy: 93.7\n",
            "End of Epoch 881, Training Loss: 0.2608099817645557, Training Accuracy: 92.82037037037037\n",
            "End of Epoch 881, Validation Loss: 0.2319644466470705, Validation Accuracy: 93.7\n",
            "End of Epoch 882, Training Loss: 0.2606097565568699, Training Accuracy: 92.82592592592593\n",
            "End of Epoch 882, Validation Loss: 0.23179157905099376, Validation Accuracy: 93.71666666666667\n",
            "End of Epoch 883, Training Loss: 0.26040995813093204, Training Accuracy: 92.84074074074074\n",
            "End of Epoch 883, Validation Loss: 0.2316190974937125, Validation Accuracy: 93.71666666666667\n",
            "End of Epoch 884, Training Loss: 0.2602105847220948, Training Accuracy: 92.85185185185185\n",
            "End of Epoch 884, Validation Loss: 0.23144700078901329, Validation Accuracy: 93.71666666666667\n",
            "End of Epoch 885, Training Loss: 0.2600116345795804, Training Accuracy: 92.85740740740741\n",
            "End of Epoch 885, Validation Loss: 0.23127528776731504, Validation Accuracy: 93.71666666666667\n",
            "End of Epoch 886, Training Loss: 0.2598131059663418, Training Accuracy: 92.85555555555555\n",
            "End of Epoch 886, Validation Loss: 0.2311039572753488, Validation Accuracy: 93.71666666666667\n",
            "End of Epoch 887, Training Loss: 0.25961499715892217, Training Accuracy: 92.86111111111111\n",
            "End of Epoch 887, Validation Loss: 0.23093300817583315, Validation Accuracy: 93.73333333333333\n",
            "End of Epoch 888, Training Loss: 0.2594173064473159, Training Accuracy: 92.86296296296295\n",
            "End of Epoch 888, Validation Loss: 0.23076243934714422, Validation Accuracy: 93.73333333333333\n",
            "End of Epoch 889, Training Loss: 0.2592200321348292, Training Accuracy: 92.86666666666666\n",
            "End of Epoch 889, Validation Loss: 0.23059224968297948, Validation Accuracy: 93.73333333333333\n",
            "End of Epoch 890, Training Loss: 0.2590231725379399, Training Accuracy: 92.87407407407407\n",
            "End of Epoch 890, Validation Loss: 0.23042243809201823, Validation Accuracy: 93.76666666666667\n",
            "End of Epoch 891, Training Loss: 0.2588267259861586, Training Accuracy: 92.88148148148147\n",
            "End of Epoch 891, Validation Loss: 0.2302530034975761, Validation Accuracy: 93.75\n",
            "End of Epoch 892, Training Loss: 0.25863069082188994, Training Accuracy: 92.88703703703703\n",
            "End of Epoch 892, Validation Loss: 0.23008394483725583, Validation Accuracy: 93.75\n",
            "End of Epoch 893, Training Loss: 0.2584350654002928, Training Accuracy: 92.88333333333333\n",
            "End of Epoch 893, Validation Loss: 0.22991526106259394, Validation Accuracy: 93.75\n",
            "End of Epoch 894, Training Loss: 0.25823984808914247, Training Accuracy: 92.88888888888889\n",
            "End of Epoch 894, Validation Loss: 0.22974695113870344, Validation Accuracy: 93.75\n",
            "End of Epoch 895, Training Loss: 0.2580450372686919, Training Accuracy: 92.9\n",
            "End of Epoch 895, Validation Loss: 0.22957901404391273, Validation Accuracy: 93.75\n",
            "End of Epoch 896, Training Loss: 0.2578506313315342, Training Accuracy: 92.89814814814815\n",
            "End of Epoch 896, Validation Loss: 0.229411448769402, Validation Accuracy: 93.75\n",
            "End of Epoch 897, Training Loss: 0.2576566286824654, Training Accuracy: 92.91296296296296\n",
            "End of Epoch 897, Validation Loss: 0.22924425431883605, Validation Accuracy: 93.75\n",
            "End of Epoch 898, Training Loss: 0.25746302773834745, Training Accuracy: 92.91851851851852\n",
            "End of Epoch 898, Validation Loss: 0.2290774297079952, Validation Accuracy: 93.76666666666667\n",
            "End of Epoch 899, Training Loss: 0.257269826927972, Training Accuracy: 92.92777777777778\n",
            "End of Epoch 899, Validation Loss: 0.2289109739644032, Validation Accuracy: 93.76666666666667\n",
            "End of Epoch 900, Training Loss: 0.257077024691925, Training Accuracy: 92.92962962962963\n",
            "End of Epoch 900, Validation Loss: 0.22874488612695457, Validation Accuracy: 93.76666666666667\n",
            "End of Epoch 901, Training Loss: 0.2568846194824514, Training Accuracy: 92.92777777777778\n",
            "End of Epoch 901, Validation Loss: 0.22857916524553884, Validation Accuracy: 93.81666666666668\n",
            "End of Epoch 902, Training Loss: 0.2566926097633219, Training Accuracy: 92.93148148148148\n",
            "End of Epoch 902, Validation Loss: 0.22841381038066527, Validation Accuracy: 93.81666666666668\n",
            "End of Epoch 903, Training Loss: 0.25650099400969867, Training Accuracy: 92.94074074074075\n",
            "End of Epoch 903, Validation Loss: 0.22824882060308596, Validation Accuracy: 93.85\n",
            "End of Epoch 904, Training Loss: 0.2563097707080028, Training Accuracy: 92.94074074074075\n",
            "End of Epoch 904, Validation Loss: 0.2280841949934187, Validation Accuracy: 93.85\n",
            "End of Epoch 905, Training Loss: 0.2561189383557835, Training Accuracy: 92.94814814814815\n",
            "End of Epoch 905, Validation Loss: 0.22791993264177035, Validation Accuracy: 93.85\n",
            "End of Epoch 906, Training Loss: 0.2559284954615869, Training Accuracy: 92.95555555555556\n",
            "End of Epoch 906, Validation Loss: 0.2277560326473606, Validation Accuracy: 93.85\n",
            "End of Epoch 907, Training Loss: 0.25573844054482636, Training Accuracy: 92.95555555555556\n",
            "End of Epoch 907, Validation Loss: 0.22759249411814608, Validation Accuracy: 93.85\n",
            "End of Epoch 908, Training Loss: 0.25554877213565375, Training Accuracy: 92.96481481481482\n",
            "End of Epoch 908, Validation Loss: 0.22742931617044648, Validation Accuracy: 93.85\n",
            "End of Epoch 909, Training Loss: 0.25535948877483194, Training Accuracy: 92.97037037037038\n",
            "End of Epoch 909, Validation Loss: 0.22726649792857198, Validation Accuracy: 93.85\n",
            "End of Epoch 910, Training Loss: 0.2551705890136082, Training Accuracy: 92.97407407407408\n",
            "End of Epoch 910, Validation Loss: 0.2271040385244529, Validation Accuracy: 93.85\n",
            "End of Epoch 911, Training Loss: 0.2549820714135893, Training Accuracy: 92.97592592592594\n",
            "End of Epoch 911, Validation Loss: 0.22694193709727223, Validation Accuracy: 93.85\n",
            "End of Epoch 912, Training Loss: 0.25479393454661664, Training Accuracy: 92.9888888888889\n",
            "End of Epoch 912, Validation Loss: 0.22678019279310024, Validation Accuracy: 93.85\n",
            "End of Epoch 913, Training Loss: 0.2546061769946441, Training Accuracy: 92.9888888888889\n",
            "End of Epoch 913, Validation Loss: 0.22661880476453353, Validation Accuracy: 93.86666666666666\n",
            "End of Epoch 914, Training Loss: 0.254418797349616, Training Accuracy: 92.9888888888889\n",
            "End of Epoch 914, Validation Loss: 0.22645777217033686, Validation Accuracy: 93.86666666666666\n",
            "End of Epoch 915, Training Loss: 0.2542317942133465, Training Accuracy: 92.99444444444444\n",
            "End of Epoch 915, Validation Loss: 0.22629709417508997, Validation Accuracy: 93.89999999999999\n",
            "End of Epoch 916, Training Loss: 0.25404516619740136, Training Accuracy: 92.99814814814815\n",
            "End of Epoch 916, Validation Loss: 0.22613676994883827, Validation Accuracy: 93.91666666666667\n",
            "End of Epoch 917, Training Loss: 0.25385891192297916, Training Accuracy: 93.00555555555555\n",
            "End of Epoch 917, Validation Loss: 0.2259767986667487, Validation Accuracy: 93.91666666666667\n",
            "End of Epoch 918, Training Loss: 0.25367303002079694, Training Accuracy: 93.00925925925925\n",
            "End of Epoch 918, Validation Loss: 0.22581717950877128, Validation Accuracy: 93.93333333333334\n",
            "End of Epoch 919, Training Loss: 0.25348751913097256, Training Accuracy: 93.01296296296296\n",
            "End of Epoch 919, Validation Loss: 0.22565791165930565, Validation Accuracy: 93.93333333333334\n",
            "End of Epoch 920, Training Loss: 0.2533023779029144, Training Accuracy: 93.01851851851852\n",
            "End of Epoch 920, Validation Loss: 0.22549899430687417, Validation Accuracy: 93.91666666666667\n",
            "End of Epoch 921, Training Loss: 0.25311760499520725, Training Accuracy: 93.02592592592592\n",
            "End of Epoch 921, Validation Loss: 0.22534042664380116, Validation Accuracy: 93.91666666666667\n",
            "End of Epoch 922, Training Loss: 0.2529331990755025, Training Accuracy: 93.03518518518518\n",
            "End of Epoch 922, Validation Loss: 0.22518220786589885, Validation Accuracy: 93.93333333333334\n",
            "End of Epoch 923, Training Loss: 0.25274915882040977, Training Accuracy: 93.03518518518518\n",
            "End of Epoch 923, Validation Loss: 0.22502433717216067, Validation Accuracy: 93.93333333333334\n",
            "End of Epoch 924, Training Loss: 0.25256548291538883, Training Accuracy: 93.04074074074074\n",
            "End of Epoch 924, Validation Loss: 0.22486681376446135, Validation Accuracy: 93.93333333333334\n",
            "End of Epoch 925, Training Loss: 0.25238217005464403, Training Accuracy: 93.04444444444444\n",
            "End of Epoch 925, Validation Loss: 0.2247096368472653, Validation Accuracy: 93.95\n",
            "End of Epoch 926, Training Loss: 0.2521992189410201, Training Accuracy: 93.05\n",
            "End of Epoch 926, Validation Loss: 0.22455280562734223, Validation Accuracy: 93.95\n",
            "End of Epoch 927, Training Loss: 0.25201662828589927, Training Accuracy: 93.06481481481481\n",
            "End of Epoch 927, Validation Loss: 0.22439631931349174, Validation Accuracy: 93.95\n",
            "End of Epoch 928, Training Loss: 0.2518343968091, Training Accuracy: 93.06851851851852\n",
            "End of Epoch 928, Validation Loss: 0.2242401771162755, Validation Accuracy: 93.95\n",
            "End of Epoch 929, Training Loss: 0.2516525232387781, Training Accuracy: 93.07407407407408\n",
            "End of Epoch 929, Validation Loss: 0.22408437824775873, Validation Accuracy: 93.96666666666667\n",
            "End of Epoch 930, Training Loss: 0.25147100631132807, Training Accuracy: 93.07777777777778\n",
            "End of Epoch 930, Validation Loss: 0.22392892192126038, Validation Accuracy: 93.96666666666667\n",
            "End of Epoch 931, Training Loss: 0.25128984477128724, Training Accuracy: 93.08703703703704\n",
            "End of Epoch 931, Validation Loss: 0.22377380735111213, Validation Accuracy: 93.96666666666667\n",
            "End of Epoch 932, Training Loss: 0.2511090373712404, Training Accuracy: 93.09074074074074\n",
            "End of Epoch 932, Validation Loss: 0.22361903375242695, Validation Accuracy: 93.96666666666667\n",
            "End of Epoch 933, Training Loss: 0.2509285828717279, Training Accuracy: 93.0962962962963\n",
            "End of Epoch 933, Validation Loss: 0.2234646003408772, Validation Accuracy: 93.96666666666667\n",
            "End of Epoch 934, Training Loss: 0.25074848004115297, Training Accuracy: 93.0962962962963\n",
            "End of Epoch 934, Validation Loss: 0.22331050633248137, Validation Accuracy: 93.96666666666667\n",
            "End of Epoch 935, Training Loss: 0.2505687276556925, Training Accuracy: 93.1037037037037\n",
            "End of Epoch 935, Validation Loss: 0.2231567509434017, Validation Accuracy: 93.98333333333333\n",
            "End of Epoch 936, Training Loss: 0.25038932449920875, Training Accuracy: 93.10555555555555\n",
            "End of Epoch 936, Validation Loss: 0.22300333338975062, Validation Accuracy: 93.98333333333333\n",
            "End of Epoch 937, Training Loss: 0.2502102693631623, Training Accuracy: 93.10740740740741\n",
            "End of Epoch 937, Validation Loss: 0.22285025288740745, Validation Accuracy: 93.98333333333333\n",
            "End of Epoch 938, Training Loss: 0.2500315610465279, Training Accuracy: 93.11296296296297\n",
            "End of Epoch 938, Validation Loss: 0.222697508651845, Validation Accuracy: 93.98333333333333\n",
            "End of Epoch 939, Training Loss: 0.24985319835571002, Training Accuracy: 93.11481481481482\n",
            "End of Epoch 939, Validation Loss: 0.2225450998979656, Validation Accuracy: 93.98333333333333\n",
            "End of Epoch 940, Training Loss: 0.24967518010446182, Training Accuracy: 93.11666666666667\n",
            "End of Epoch 940, Validation Loss: 0.22239302583994772, Validation Accuracy: 94.0\n",
            "End of Epoch 941, Training Loss: 0.2494975051138045, Training Accuracy: 93.12037037037038\n",
            "End of Epoch 941, Validation Loss: 0.22224128569110213, Validation Accuracy: 94.0\n",
            "End of Epoch 942, Training Loss: 0.24932017221194905, Training Accuracy: 93.12407407407407\n",
            "End of Epoch 942, Validation Loss: 0.22208987866373814, Validation Accuracy: 94.0\n",
            "End of Epoch 943, Training Loss: 0.24914318023421933, Training Accuracy: 93.12962962962963\n",
            "End of Epoch 943, Validation Loss: 0.2219388039690395, Validation Accuracy: 94.03333333333333\n",
            "End of Epoch 944, Training Loss: 0.24896652802297548, Training Accuracy: 93.13703703703705\n",
            "End of Epoch 944, Validation Loss: 0.2217880608169503, Validation Accuracy: 94.03333333333333\n",
            "End of Epoch 945, Training Loss: 0.24879021442754098, Training Accuracy: 93.1388888888889\n",
            "End of Epoch 945, Validation Loss: 0.22163764841607056, Validation Accuracy: 94.05\n",
            "End of Epoch 946, Training Loss: 0.2486142383041307, Training Accuracy: 93.1462962962963\n",
            "End of Epoch 946, Validation Loss: 0.22148756597356173, Validation Accuracy: 94.06666666666666\n",
            "End of Epoch 947, Training Loss: 0.2484385985157785, Training Accuracy: 93.15185185185186\n",
            "End of Epoch 947, Validation Loss: 0.22133781269506078, Validation Accuracy: 94.06666666666666\n",
            "End of Epoch 948, Training Loss: 0.24826329393226937, Training Accuracy: 93.15370370370371\n",
            "End of Epoch 948, Validation Loss: 0.221188387784605, Validation Accuracy: 94.06666666666666\n",
            "End of Epoch 949, Training Loss: 0.2480883234300706, Training Accuracy: 93.16851851851851\n",
            "End of Epoch 949, Validation Loss: 0.22103929044456538, Validation Accuracy: 94.08333333333333\n",
            "End of Epoch 950, Training Loss: 0.2479136858922658, Training Accuracy: 93.17222222222222\n",
            "End of Epoch 950, Validation Loss: 0.22089051987558855, Validation Accuracy: 94.1\n",
            "End of Epoch 951, Training Loss: 0.24773938020849007, Training Accuracy: 93.18148148148147\n",
            "End of Epoch 951, Validation Loss: 0.22074207527654907, Validation Accuracy: 94.1\n",
            "End of Epoch 952, Training Loss: 0.24756540527486637, Training Accuracy: 93.18703703703703\n",
            "End of Epoch 952, Validation Loss: 0.2205939558445088, Validation Accuracy: 94.1\n",
            "End of Epoch 953, Training Loss: 0.24739175999394314, Training Accuracy: 93.19074074074074\n",
            "End of Epoch 953, Validation Loss: 0.22044616077468646, Validation Accuracy: 94.1\n",
            "End of Epoch 954, Training Loss: 0.24721844327463444, Training Accuracy: 93.19259259259259\n",
            "End of Epoch 954, Validation Loss: 0.220298689260434, Validation Accuracy: 94.1\n",
            "End of Epoch 955, Training Loss: 0.24704545403215977, Training Accuracy: 93.1962962962963\n",
            "End of Epoch 955, Validation Loss: 0.2201515404932221, Validation Accuracy: 94.11666666666667\n",
            "End of Epoch 956, Training Loss: 0.24687279118798694, Training Accuracy: 93.2\n",
            "End of Epoch 956, Validation Loss: 0.22000471366263347, Validation Accuracy: 94.13333333333334\n",
            "End of Epoch 957, Training Loss: 0.2467004536697751, Training Accuracy: 93.20185185185184\n",
            "End of Epoch 957, Validation Loss: 0.21985820795636285, Validation Accuracy: 94.15\n",
            "End of Epoch 958, Training Loss: 0.2465284404113195, Training Accuracy: 93.21481481481482\n",
            "End of Epoch 958, Validation Loss: 0.2197120225602258, Validation Accuracy: 94.15\n",
            "End of Epoch 959, Training Loss: 0.24635675035249782, Training Accuracy: 93.22222222222221\n",
            "End of Epoch 959, Validation Loss: 0.21956615665817333, Validation Accuracy: 94.15\n",
            "End of Epoch 960, Training Loss: 0.24618538243921712, Training Accuracy: 93.22592592592592\n",
            "End of Epoch 960, Validation Loss: 0.21942060943231445, Validation Accuracy: 94.15\n",
            "End of Epoch 961, Training Loss: 0.2460143356233625, Training Accuracy: 93.23518518518519\n",
            "End of Epoch 961, Validation Loss: 0.21927538006294442, Validation Accuracy: 94.16666666666667\n",
            "End of Epoch 962, Training Loss: 0.24584360886274742, Training Accuracy: 93.24074074074075\n",
            "End of Epoch 962, Validation Loss: 0.21913046772857986, Validation Accuracy: 94.16666666666667\n",
            "End of Epoch 963, Training Loss: 0.24567320112106383, Training Accuracy: 93.25\n",
            "End of Epoch 963, Validation Loss: 0.21898587160599967, Validation Accuracy: 94.19999999999999\n",
            "End of Epoch 964, Training Loss: 0.24550311136783506, Training Accuracy: 93.25555555555556\n",
            "End of Epoch 964, Validation Loss: 0.21884159087029156, Validation Accuracy: 94.19999999999999\n",
            "End of Epoch 965, Training Loss: 0.2453333385783689, Training Accuracy: 93.2611111111111\n",
            "End of Epoch 965, Validation Loss: 0.21869762469490456, Validation Accuracy: 94.19999999999999\n",
            "End of Epoch 966, Training Loss: 0.24516388173371204, Training Accuracy: 93.27592592592593\n",
            "End of Epoch 966, Validation Loss: 0.21855397225170609, Validation Accuracy: 94.21666666666667\n",
            "End of Epoch 967, Training Loss: 0.24499473982060563, Training Accuracy: 93.27962962962964\n",
            "End of Epoch 967, Validation Loss: 0.21841063271104413, Validation Accuracy: 94.21666666666667\n",
            "End of Epoch 968, Training Loss: 0.24482591183144198, Training Accuracy: 93.28148148148148\n",
            "End of Epoch 968, Validation Loss: 0.21826760524181418, Validation Accuracy: 94.21666666666667\n",
            "End of Epoch 969, Training Loss: 0.24465739676422207, Training Accuracy: 93.28333333333333\n",
            "End of Epoch 969, Validation Loss: 0.21812488901153, Validation Accuracy: 94.21666666666667\n",
            "End of Epoch 970, Training Loss: 0.24448919362251428, Training Accuracy: 93.28888888888889\n",
            "End of Epoch 970, Validation Loss: 0.21798248318639904, Validation Accuracy: 94.21666666666667\n",
            "End of Epoch 971, Training Loss: 0.24432130141541436, Training Accuracy: 93.30000000000001\n",
            "End of Epoch 971, Validation Loss: 0.21784038693140104, Validation Accuracy: 94.23333333333333\n",
            "End of Epoch 972, Training Loss: 0.24415371915750594, Training Accuracy: 93.30555555555556\n",
            "End of Epoch 972, Validation Loss: 0.21769859941036998, Validation Accuracy: 94.21666666666667\n",
            "End of Epoch 973, Training Loss: 0.24398644586882146, Training Accuracy: 93.31111111111112\n",
            "End of Epoch 973, Validation Loss: 0.2175571197860801, Validation Accuracy: 94.21666666666667\n",
            "End of Epoch 974, Training Loss: 0.24381948057480635, Training Accuracy: 93.31666666666666\n",
            "End of Epoch 974, Validation Loss: 0.21741594722033342, Validation Accuracy: 94.25\n",
            "End of Epoch 975, Training Loss: 0.24365282230628024, Training Accuracy: 93.32037037037037\n",
            "End of Epoch 975, Validation Loss: 0.21727508087405106, Validation Accuracy: 94.25\n",
            "End of Epoch 976, Training Loss: 0.2434864700994029, Training Accuracy: 93.32777777777778\n",
            "End of Epoch 976, Validation Loss: 0.2171345199073661, Validation Accuracy: 94.26666666666667\n",
            "End of Epoch 977, Training Loss: 0.2433204229956387, Training Accuracy: 93.32777777777778\n",
            "End of Epoch 977, Validation Loss: 0.216994263479719, Validation Accuracy: 94.26666666666667\n",
            "End of Epoch 978, Training Loss: 0.24315468004172214, Training Accuracy: 93.33518518518518\n",
            "End of Epoch 978, Validation Loss: 0.21685431074995473, Validation Accuracy: 94.26666666666667\n",
            "End of Epoch 979, Training Loss: 0.24298924028962526, Training Accuracy: 93.33703703703704\n",
            "End of Epoch 979, Validation Loss: 0.21671466087642105, Validation Accuracy: 94.26666666666667\n",
            "End of Epoch 980, Training Loss: 0.24282410279652464, Training Accuracy: 93.33333333333333\n",
            "End of Epoch 980, Validation Loss: 0.21657531301706884, Validation Accuracy: 94.26666666666667\n",
            "End of Epoch 981, Training Loss: 0.24265926662476928, Training Accuracy: 93.33333333333333\n",
            "End of Epoch 981, Validation Loss: 0.21643626632955276, Validation Accuracy: 94.26666666666667\n",
            "End of Epoch 982, Training Loss: 0.24249473084184986, Training Accuracy: 93.33703703703704\n",
            "End of Epoch 982, Validation Loss: 0.21629751997133326, Validation Accuracy: 94.26666666666667\n",
            "End of Epoch 983, Training Loss: 0.24233049452036795, Training Accuracy: 93.34074074074074\n",
            "End of Epoch 983, Validation Loss: 0.21615907309977933, Validation Accuracy: 94.26666666666667\n",
            "End of Epoch 984, Training Loss: 0.24216655673800613, Training Accuracy: 93.34259259259258\n",
            "End of Epoch 984, Validation Loss: 0.216020924872271, Validation Accuracy: 94.26666666666667\n",
            "End of Epoch 985, Training Loss: 0.24200291657749898, Training Accuracy: 93.35555555555555\n",
            "End of Epoch 985, Validation Loss: 0.2158830744463027, Validation Accuracy: 94.26666666666667\n",
            "End of Epoch 986, Training Loss: 0.24183957312660423, Training Accuracy: 93.35555555555555\n",
            "End of Epoch 986, Validation Loss: 0.2157455209795864, Validation Accuracy: 94.26666666666667\n",
            "End of Epoch 987, Training Loss: 0.24167652547807444, Training Accuracy: 93.35555555555555\n",
            "End of Epoch 987, Validation Loss: 0.2156082636301545, Validation Accuracy: 94.25\n",
            "End of Epoch 988, Training Loss: 0.24151377272962987, Training Accuracy: 93.35740740740741\n",
            "End of Epoch 988, Validation Loss: 0.21547130155646246, Validation Accuracy: 94.25\n",
            "End of Epoch 989, Training Loss: 0.241351313983931, Training Accuracy: 93.36296296296295\n",
            "End of Epoch 989, Validation Loss: 0.21533463391749097, Validation Accuracy: 94.26666666666667\n",
            "End of Epoch 990, Training Loss: 0.24118914834855196, Training Accuracy: 93.36851851851851\n",
            "End of Epoch 990, Validation Loss: 0.21519825987284708, Validation Accuracy: 94.28333333333333\n",
            "End of Epoch 991, Training Loss: 0.2410272749359548, Training Accuracy: 93.37222222222222\n",
            "End of Epoch 991, Validation Loss: 0.21506217858286497, Validation Accuracy: 94.3\n",
            "End of Epoch 992, Training Loss: 0.24086569286346365, Training Accuracy: 93.37592592592593\n",
            "End of Epoch 992, Validation Loss: 0.21492638920870547, Validation Accuracy: 94.3\n",
            "End of Epoch 993, Training Loss: 0.2407044012532388, Training Accuracy: 93.38148148148147\n",
            "End of Epoch 993, Validation Loss: 0.21479089091245424, Validation Accuracy: 94.3\n",
            "End of Epoch 994, Training Loss: 0.24054339923225285, Training Accuracy: 93.3962962962963\n",
            "End of Epoch 994, Validation Loss: 0.21465568285721906, Validation Accuracy: 94.31666666666668\n",
            "End of Epoch 995, Training Loss: 0.24038268593226528, Training Accuracy: 93.4037037037037\n",
            "End of Epoch 995, Validation Loss: 0.21452076420722552, Validation Accuracy: 94.33333333333334\n",
            "End of Epoch 996, Training Loss: 0.24022226048979883, Training Accuracy: 93.40555555555555\n",
            "End of Epoch 996, Validation Loss: 0.21438613412791108, Validation Accuracy: 94.33333333333334\n",
            "End of Epoch 997, Training Loss: 0.24006212204611532, Training Accuracy: 93.40555555555555\n",
            "End of Epoch 997, Validation Loss: 0.21425179178601797, Validation Accuracy: 94.33333333333334\n",
            "End of Epoch 998, Training Loss: 0.23990226974719223, Training Accuracy: 93.40925925925926\n",
            "End of Epoch 998, Validation Loss: 0.21411773634968378, Validation Accuracy: 94.33333333333334\n",
            "End of Epoch 999, Training Loss: 0.23974270274369894, Training Accuracy: 93.40925925925926\n",
            "End of Epoch 999, Validation Loss: 0.21398396698853075, Validation Accuracy: 94.33333333333334\n",
            "End of Epoch 1000, Training Loss: 0.2395834201909745, Training Accuracy: 93.41296296296296\n",
            "End of Epoch 1000, Validation Loss: 0.21385048287375286, Validation Accuracy: 94.35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Yes the larger network is giving much good performance than the smaller one.**\n",
        "\n",
        "    It is giving 94% nearly(Validation Accuracy) whereas the previous smaller network is giving 80% nearly(Validation Accuracy).\n",
        "\n",
        "\n",
        "    Hence the validation accuracy is increased after increasing one hidden layer"
      ],
      "metadata": {
        "id": "i1qVuxjCIOQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p5kpU5mDpu8h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}